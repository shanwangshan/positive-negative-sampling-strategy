/run/nvme/job_2987382/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 309, 'ckp_path': './checkpoint/', 'vgg_path': '/vgg-sound/', 'unwanted_files_path': '../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64}
use_cude True
all together the number of training files is 169683
total number of training files is 38007
total number of training files is 38007
Let's use 4 GPUs!
Directory  ./checkpoint/  Created 
this is epoch 0
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   0 |   100/  123 batches | ms/batch 7796.21 | loss  5.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   0 | time: 873.02s | training loss  5.75 |
[5.746218739486322]
this is epoch 1
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   1 |   100/  123 batches | ms/batch 5742.66 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 664.62s | training loss  5.73 |
[5.746218739486322, 5.732219634017324]
this is epoch 2
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   2 |   100/  123 batches | ms/batch 5679.13 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 647.36s | training loss  5.73 |
[5.746218739486322, 5.732219634017324, 5.730679589558423]
this is epoch 3
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   3 |   100/  123 batches | ms/batch 5777.71 | loss  5.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 663.16s | training loss  5.73 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337]
this is epoch 4
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   4 |   100/  123 batches | ms/batch 5685.39 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 677.31s | training loss  5.72 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115]
this is epoch 5
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   5 |   100/  123 batches | ms/batch 5546.50 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 657.44s | training loss  5.72 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408]
this is epoch 6
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   6 |   100/  123 batches | ms/batch 5442.62 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 625.02s | training loss  5.71 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882]
this is epoch 7
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   7 |   100/  123 batches | ms/batch 5528.08 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 647.60s | training loss  5.71 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905]
this is epoch 8
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   8 |   100/  123 batches | ms/batch 5521.91 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 650.64s | training loss  5.71 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088]
this is epoch 9
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   9 |   100/  123 batches | ms/batch 5519.29 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 641.29s | training loss  5.71 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355]
this is epoch 10
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  10 |   100/  123 batches | ms/batch 5559.47 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 635.80s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833]
this is epoch 11
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  11 |   100/  123 batches | ms/batch 5541.38 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 635.41s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411]
this is epoch 12
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  12 |   100/  123 batches | ms/batch 5455.01 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 637.32s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317]
this is epoch 13
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  13 |   100/  123 batches | ms/batch 5733.21 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 664.77s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066]
this is epoch 14
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  14 |   100/  123 batches | ms/batch 5589.24 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 643.95s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941]
this is epoch 15
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  15 |   100/  123 batches | ms/batch 5616.29 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 642.18s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235]
this is epoch 16
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  16 |   100/  123 batches | ms/batch 5612.09 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 638.70s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022]
this is epoch 17
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  17 |   100/  123 batches | ms/batch 5526.82 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 630.10s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415]
this is epoch 18
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  18 |   100/  123 batches | ms/batch 5493.08 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 640.72s | training loss  5.70 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019]
this is epoch 19
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  19 |   100/  123 batches | ms/batch 5576.66 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 647.05s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753]
this is epoch 20
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  20 |   100/  123 batches | ms/batch 5539.17 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 634.55s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913]
this is epoch 21
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  21 |   100/  123 batches | ms/batch 5663.65 | loss  5.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 646.83s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856]
this is epoch 22
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  22 |   100/  123 batches | ms/batch 5531.39 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 642.27s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085]
this is epoch 23
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  23 |   100/  123 batches | ms/batch 5499.88 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 639.18s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005]
this is epoch 24
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  24 |   100/  123 batches | ms/batch 5537.39 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 631.75s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297]
this is epoch 25
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  25 |   100/  123 batches | ms/batch 5598.21 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 647.64s | training loss  5.69 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403]
this is epoch 26
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  26 |   100/  123 batches | ms/batch 5576.87 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 639.84s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049]
this is epoch 27
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  27 |   100/  123 batches | ms/batch 5618.81 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 669.26s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662]
this is epoch 28
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  28 |   100/  123 batches | ms/batch 5629.51 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 641.44s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055]
this is epoch 29
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  29 |   100/  123 batches | ms/batch 5781.15 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 660.32s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425]
this is epoch 30
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  30 |   100/  123 batches | ms/batch 5668.43 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 643.14s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695]
this is epoch 31
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  31 |   100/  123 batches | ms/batch 5521.63 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 631.37s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741]
this is epoch 32
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  32 |   100/  123 batches | ms/batch 5595.53 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 638.89s | training loss  5.67 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234]
this is epoch 33
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  33 |   100/  123 batches | ms/batch 5539.36 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 630.32s | training loss  5.68 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024]
this is epoch 34
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  34 |   100/  123 batches | ms/batch 5693.73 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 643.65s | training loss  5.67 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735]
this is epoch 35
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  35 |   100/  123 batches | ms/batch 5440.98 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 639.29s | training loss  5.67 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409]
this is epoch 36
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  36 |   100/  123 batches | ms/batch 5614.70 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 642.99s | training loss  5.67 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654]
this is epoch 37
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  37 |   100/  123 batches | ms/batch 5555.34 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 636.94s | training loss  5.67 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669]
this is epoch 38
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  38 |   100/  123 batches | ms/batch 5515.97 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 630.67s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141]
this is epoch 39
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  39 |   100/  123 batches | ms/batch 5598.18 | loss  5.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 640.25s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935]
this is epoch 40
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  40 |   100/  123 batches | ms/batch 5573.26 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 641.11s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947]
this is epoch 41
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  41 |   100/  123 batches | ms/batch 5558.83 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 638.92s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779]
this is epoch 42
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  42 |   100/  123 batches | ms/batch 5652.93 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 644.65s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181]
this is epoch 43
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  43 |   100/  123 batches | ms/batch 5495.42 | loss  5.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 643.01s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725]
this is epoch 44
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  44 |   100/  123 batches | ms/batch 5689.80 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 643.27s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435]
this is epoch 45
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  45 |   100/  123 batches | ms/batch 5572.04 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 642.52s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291]
this is epoch 46
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  46 |   100/  123 batches | ms/batch 5469.97 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 637.58s | training loss  5.66 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118]
this is epoch 47
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  47 |   100/  123 batches | ms/batch 5551.13 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 629.67s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865]
this is epoch 48
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  48 |   100/  123 batches | ms/batch 5611.21 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 639.04s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188]
this is epoch 49
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  49 |   100/  123 batches | ms/batch 5583.24 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 638.03s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401]
this is epoch 50
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  50 |   100/  123 batches | ms/batch 5586.80 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 644.99s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348]
this is epoch 51
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  51 |   100/  123 batches | ms/batch 5651.23 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 641.47s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215]
this is epoch 52
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  52 |   100/  123 batches | ms/batch 5474.28 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 643.79s | training loss  5.65 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572]
this is epoch 53
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  53 |   100/  123 batches | ms/batch 5477.14 | loss  5.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 637.69s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485]
this is epoch 54
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  54 |   100/  123 batches | ms/batch 5545.46 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 630.69s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732]
this is epoch 55
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  55 |   100/  123 batches | ms/batch 5664.40 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 641.67s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635]
this is epoch 56
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  56 |   100/  123 batches | ms/batch 5634.58 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 639.84s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908]
this is epoch 57
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  57 |   100/  123 batches | ms/batch 5497.80 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 634.24s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842]
this is epoch 58
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  58 |   100/  123 batches | ms/batch 5449.33 | loss  5.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 632.76s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332]
this is epoch 59
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  59 |   100/  123 batches | ms/batch 5634.76 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 640.35s | training loss  5.64 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797]
this is epoch 60
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  60 |   100/  123 batches | ms/batch 5538.15 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 634.57s | training loss  5.63 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781]
this is epoch 61
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  61 |   100/  123 batches | ms/batch 5466.77 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 644.48s | training loss  5.63 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036]
this is epoch 62
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  62 |   100/  123 batches | ms/batch 5537.07 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 645.17s | training loss  5.63 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833]
this is epoch 63
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  63 |   100/  123 batches | ms/batch 5552.71 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 638.13s | training loss  5.63 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113]
this is epoch 64
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  64 |   100/  123 batches | ms/batch 5551.63 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 635.39s | training loss  5.63 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432]
this is epoch 65
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  65 |   100/  123 batches | ms/batch 5607.19 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 650.18s | training loss  5.62 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464]
this is epoch 66
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  66 |   100/  123 batches | ms/batch 5595.28 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 640.80s | training loss  5.62 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195]
this is epoch 67
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  67 |   100/  123 batches | ms/batch 5506.14 | loss  5.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 638.17s | training loss  5.61 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165]
this is epoch 68
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  68 |   100/  123 batches | ms/batch 5501.49 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 635.37s | training loss  5.62 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156]
this is epoch 69
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  69 |   100/  123 batches | ms/batch 5640.95 | loss  5.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 645.35s | training loss  5.62 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668]
this is epoch 70
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  70 |   100/  123 batches | ms/batch 5498.80 | loss  5.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 635.08s | training loss  5.62 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203]
this is epoch 71
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  71 |   100/  123 batches | ms/batch 5750.97 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 654.33s | training loss  5.61 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813]
this is epoch 72
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  72 |   100/  123 batches | ms/batch 5542.71 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 634.31s | training loss  5.61 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676]
this is epoch 73
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  73 |   100/  123 batches | ms/batch 5673.16 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 645.25s | training loss  5.61 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815]
this is epoch 74
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  74 |   100/  123 batches | ms/batch 5691.55 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 643.98s | training loss  5.61 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245]
this is epoch 75
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  75 |   100/  123 batches | ms/batch 5635.67 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 641.61s | training loss  5.60 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468]
this is epoch 76
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  76 |   100/  123 batches | ms/batch 5559.07 | loss  5.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 635.83s | training loss  5.60 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175]
this is epoch 77
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  77 |   100/  123 batches | ms/batch 5651.84 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 644.24s | training loss  5.60 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993]
this is epoch 78
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  78 |   100/  123 batches | ms/batch 5580.48 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 637.79s | training loss  5.60 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625]
this is epoch 79
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  79 |   100/  123 batches | ms/batch 5631.48 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 646.91s | training loss  5.60 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385]
this is epoch 80
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  80 |   100/  123 batches | ms/batch 5574.59 | loss  5.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 633.17s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606]
this is epoch 81
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  81 |   100/  123 batches | ms/batch 5585.71 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 641.38s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668]
this is epoch 82
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  82 |   100/  123 batches | ms/batch 5642.93 | loss  5.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 646.30s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033]
this is epoch 83
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  83 |   100/  123 batches | ms/batch 5488.65 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 642.50s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909]
this is epoch 84
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  84 |   100/  123 batches | ms/batch 5528.59 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 634.01s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257]
this is epoch 85
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  85 |   100/  123 batches | ms/batch 5637.21 | loss  5.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 640.06s | training loss  5.59 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803]
this is epoch 86
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  86 |   100/  123 batches | ms/batch 5694.42 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 650.46s | training loss  5.58 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758]
this is epoch 87
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  87 |   100/  123 batches | ms/batch 5513.32 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 642.60s | training loss  5.58 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274]
this is epoch 88
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  88 |   100/  123 batches | ms/batch 5566.16 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 634.54s | training loss  5.57 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314]
this is epoch 89
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  89 |   100/  123 batches | ms/batch 5646.66 | loss  5.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 650.89s | training loss  5.57 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185]
this is epoch 90
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  90 |   100/  123 batches | ms/batch 5594.67 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 636.92s | training loss  5.58 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223]
this is epoch 91
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  91 |   100/  123 batches | ms/batch 5616.79 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 635.58s | training loss  5.57 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252]
this is epoch 92
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  92 |   100/  123 batches | ms/batch 5578.45 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 643.15s | training loss  5.57 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383]
this is epoch 93
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  93 |   100/  123 batches | ms/batch 5628.73 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 635.50s | training loss  5.57 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069]
this is epoch 94
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  94 |   100/  123 batches | ms/batch 5592.86 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 640.88s | training loss  5.56 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595]
this is epoch 95
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  95 |   100/  123 batches | ms/batch 5557.02 | loss  5.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 647.66s | training loss  5.56 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595, 5.55991822917287]
this is epoch 96
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  96 |   100/  123 batches | ms/batch 5632.34 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 638.18s | training loss  5.56 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595, 5.55991822917287, 5.556543660357716]
this is epoch 97
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  97 |   100/  123 batches | ms/batch 5589.82 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 638.77s | training loss  5.56 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595, 5.55991822917287, 5.556543660357716, 5.556955825991746]
this is epoch 98
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  98 |   100/  123 batches | ms/batch 5647.48 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 644.43s | training loss  5.56 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595, 5.55991822917287, 5.556543660357716, 5.556955825991746, 5.555997801990044]
this is epoch 99
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  99 |   100/  123 batches | ms/batch 5530.99 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 637.82s | training loss  5.55 |
[5.746218739486322, 5.732219634017324, 5.730679589558423, 5.72803245327337, 5.7239433459150115, 5.716542569602408, 5.713346768200882, 5.7110655249618905, 5.71074504387088, 5.706547078078355, 5.702869279597833, 5.704801726147411, 5.701847762596317, 5.698809499663066, 5.701167141518941, 5.697919702142235, 5.698260729875022, 5.6963375603280415, 5.69557608240019, 5.692630422793753, 5.694232657672913, 5.6918161632568856, 5.691041876630085, 5.688218077993005, 5.686134586489297, 5.68621883547403, 5.683094858154049, 5.68311412547662, 5.681768239029055, 5.6805798251454425, 5.6783441760675695, 5.679151217142741, 5.672798086957234, 5.675975031969024, 5.67242927473735, 5.673496436297409, 5.671326276732654, 5.672279702938669, 5.664025849443141, 5.6640825659278935, 5.66162194662947, 5.663683406705779, 5.662122687673181, 5.658773034568725, 5.660624376157435, 5.660902391604291, 5.657627032055118, 5.653662677702865, 5.654854336405188, 5.650942038714401, 5.648170711548348, 5.65034591473215, 5.648671064919572, 5.6430139309022485, 5.640475056035732, 5.6371832320360635, 5.642589243446908, 5.638125628959842, 5.6361545198332, 5.637520103919797, 5.633283580221781, 5.628265780162036, 5.626630701669833, 5.628661051029113, 5.626289344415432, 5.621712936618464, 5.6197773925657195, 5.6140373276501165, 5.617357664961156, 5.618163574032668, 5.615395394767203, 5.61000437852813, 5.612726932618676, 5.6060228425312815, 5.6072352262047245, 5.602771332593468, 5.599417760120175, 5.596561358226993, 5.59771728515625, 5.5969724034875385, 5.590545503104606, 5.588837588705668, 5.586103326906033, 5.585412064218909, 5.591707962315257, 5.585776325163803, 5.580096361113758, 5.579131401651274, 5.573216961651314, 5.573477547343185, 5.577492713928223, 5.571496959624252, 5.566145548006383, 5.570187758624069, 5.563982025394595, 5.55991822917287, 5.556543660357716, 5.556955825991746, 5.555997801990044, 5.548824822030416]
