/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 3200.33 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 341.59s | training loss  1.89 |
    | end of validation epoch   1 | time: 130.87s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [1.89430460843954] validation loss is  [1.1320249023847282]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 3170.71 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 338.59s | training loss  1.27 |
    | end of validation epoch   2 | time: 130.89s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [1.89430460843954, 1.2662910308923807] validation loss is  [1.1320249023847282, 1.0501879301543038]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 3168.30 | loss  1.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 339.13s | training loss  1.08 |
    | end of validation epoch   3 | time: 124.52s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 3194.98 | loss  0.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 339.21s | training loss  0.98 |
    | end of validation epoch   4 | time: 129.80s | validation loss  1.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 3165.85 | loss  1.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 338.55s | training loss  0.88 |
    | end of validation epoch   5 | time: 125.67s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 3153.67 | loss  0.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 339.02s | training loss  0.78 |
    | end of validation epoch   6 | time: 127.32s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 3215.34 | loss  0.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 341.91s | training loss  0.72 |
    | end of validation epoch   7 | time: 129.20s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 3217.50 | loss  0.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 343.36s | training loss  0.66 |
    | end of validation epoch   8 | time: 127.65s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 3201.12 | loss  0.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 340.79s | training loss  0.60 |
    | end of validation epoch   9 | time: 130.06s | validation loss  1.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 3171.24 | loss  0.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 339.96s | training loss  0.57 |
    | end of validation epoch  10 | time: 124.14s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 3209.54 | loss  0.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 341.29s | training loss  0.53 |
    | end of validation epoch  11 | time: 127.64s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 3179.99 | loss  0.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 339.39s | training loss  0.49 |
    | end of validation epoch  12 | time: 127.49s | validation loss  1.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 3206.40 | loss  0.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 341.15s | training loss  0.45 |
    | end of validation epoch  13 | time: 127.48s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 3208.47 | loss  0.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 341.58s | training loss  0.43 |
    | end of validation epoch  14 | time: 128.73s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 3232.71 | loss  0.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 343.81s | training loss  0.40 |
    | end of validation epoch  15 | time: 126.08s | validation loss  1.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 3185.07 | loss  0.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 342.08s | training loss  0.40 |
    | end of validation epoch  16 | time: 130.08s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 3198.08 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 340.64s | training loss  0.37 |
    | end of validation epoch  17 | time: 123.84s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 3183.54 | loss  0.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 341.43s | training loss  0.36 |
    | end of validation epoch  18 | time: 128.80s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 3183.60 | loss  0.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 338.96s | training loss  0.34 |
    | end of validation epoch  19 | time: 127.73s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 3199.01 | loss  0.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 341.68s | training loss  0.30 |
    | end of validation epoch  20 | time: 125.42s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 3195.10 | loss  0.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 340.72s | training loss  0.31 |
    | end of validation epoch  21 | time: 127.11s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216]
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 3207.23 | loss  0.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 343.43s | training loss  0.29 |
    | end of validation epoch  22 | time: 126.36s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 3219.07 | loss  0.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 341.98s | training loss  0.28 |
    | end of validation epoch  23 | time: 129.30s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 3196.83 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 340.45s | training loss  0.26 |
    | end of validation epoch  24 | time: 125.15s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 3202.62 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 341.66s | training loss  0.25 |
    | end of validation epoch  25 | time: 128.36s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 3182.51 | loss  0.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 339.09s | training loss  0.24 |
    | end of validation epoch  26 | time: 127.16s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 3190.89 | loss  0.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 341.27s | training loss  0.26 |
    | end of validation epoch  27 | time: 127.24s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198]
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 3197.53 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 339.63s | training loss  0.22 |
    | end of validation epoch  28 | time: 128.49s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 3210.15 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 340.43s | training loss  0.23 |
    | end of validation epoch  29 | time: 123.20s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481]
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 3189.57 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 341.54s | training loss  0.20 |
    | end of validation epoch  30 | time: 130.74s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 3170.11 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 339.21s | training loss  0.21 |
    | end of validation epoch  31 | time: 124.57s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045]
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 3220.32 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 342.75s | training loss  0.20 |
    | end of validation epoch  32 | time: 127.11s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 3215.53 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 340.26s | training loss  0.21 |
    | end of validation epoch  33 | time: 127.07s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 3198.24 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 341.78s | training loss  0.20 |
    | end of validation epoch  34 | time: 127.61s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 3189.82 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 339.65s | training loss  0.20 |
    | end of validation epoch  35 | time: 128.96s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427]
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 3216.16 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 341.10s | training loss  0.17 |
    | end of validation epoch  36 | time: 124.43s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 3216.58 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 342.35s | training loss  0.18 |
    | end of validation epoch  37 | time: 128.63s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717]
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 3182.08 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 338.97s | training loss  0.15 |
    | end of validation epoch  38 | time: 126.95s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 3215.70 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 341.75s | training loss  0.17 |
    | end of validation epoch  39 | time: 127.52s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724]
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 3176.46 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 338.90s | training loss  0.15 |
    | end of validation epoch  40 | time: 128.28s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 3202.22 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 341.55s | training loss  0.15 |
    | end of validation epoch  41 | time: 126.44s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 3188.38 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 340.36s | training loss  0.15 |
    | end of validation epoch  42 | time: 129.53s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 3160.36 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 340.44s | training loss  0.16 |
    | end of validation epoch  43 | time: 125.36s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 3217.07 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 343.79s | training loss  0.14 |
    | end of validation epoch  44 | time: 129.58s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 3191.50 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 339.59s | training loss  0.14 |
    | end of validation epoch  45 | time: 126.72s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 3216.95 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 342.78s | training loss  0.14 |
    | end of validation epoch  46 | time: 128.20s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 3186.38 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 340.46s | training loss  0.14 |
    | end of validation epoch  47 | time: 127.73s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 3216.01 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 343.27s | training loss  0.13 |
    | end of validation epoch  48 | time: 128.74s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 3187.37 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 342.05s | training loss  0.12 |
    | end of validation epoch  49 | time: 130.70s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 3214.49 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 341.84s | training loss  0.12 |
    | end of validation epoch  50 | time: 126.28s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 3228.94 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 343.64s | training loss  0.12 |
    | end of validation epoch  51 | time: 128.78s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 3196.22 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 340.92s | training loss  0.11 |
    | end of validation epoch  52 | time: 128.58s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 3204.03 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 342.34s | training loss  0.11 |
    | end of validation epoch  53 | time: 126.35s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 3158.43 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 340.04s | training loss  0.10 |
    | end of validation epoch  54 | time: 128.98s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 3186.55 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 340.35s | training loss  0.11 |
    | end of validation epoch  55 | time: 125.17s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 3205.47 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 341.05s | training loss  0.12 |
    | end of validation epoch  56 | time: 128.99s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 3175.30 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 338.97s | training loss  0.11 |
    | end of validation epoch  57 | time: 125.67s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 3194.31 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 341.44s | training loss  0.11 |
    | end of validation epoch  58 | time: 127.64s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 3168.62 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 339.41s | training loss  0.10 |
    | end of validation epoch  59 | time: 128.71s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 3200.46 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 341.32s | training loss  0.10 |
    | end of validation epoch  60 | time: 125.71s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 3199.19 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 341.22s | training loss  0.10 |
    | end of validation epoch  61 | time: 130.23s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 3200.83 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 339.88s | training loss  0.10 |
    | end of validation epoch  62 | time: 124.72s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 3215.15 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 340.70s | training loss  0.09 |
    | end of validation epoch  63 | time: 129.78s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 3171.39 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 339.03s | training loss  0.09 |
    | end of validation epoch  64 | time: 126.03s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 3208.64 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 341.33s | training loss  0.10 |
    | end of validation epoch  65 | time: 128.12s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 3196.95 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 339.45s | training loss  0.10 |
    | end of validation epoch  66 | time: 128.15s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 3206.76 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 341.68s | training loss  0.10 |
    | end of validation epoch  67 | time: 126.33s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 3199.55 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 340.60s | training loss  0.09 |
    | end of validation epoch  68 | time: 129.54s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 3198.45 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 340.36s | training loss  0.09 |
    | end of validation epoch  69 | time: 124.58s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 3200.35 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 341.87s | training loss  0.08 |
    | end of validation epoch  70 | time: 128.82s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 3186.49 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 338.88s | training loss  0.08 |
    | end of validation epoch  71 | time: 126.34s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 3218.20 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 340.83s | training loss  0.09 |
    | end of validation epoch  72 | time: 127.59s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 3199.82 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 339.45s | training loss  0.10 |
    | end of validation epoch  73 | time: 127.36s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 3193.53 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 339.51s | training loss  0.08 |
    | end of validation epoch  74 | time: 126.16s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 3202.43 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 340.46s | training loss  0.08 |
    | end of validation epoch  75 | time: 128.66s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 3171.19 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 339.89s | training loss  0.08 |
    | end of validation epoch  76 | time: 126.29s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 3198.71 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 341.55s | training loss  0.07 |
    | end of validation epoch  77 | time: 128.72s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 3181.66 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 339.44s | training loss  0.08 |
    | end of validation epoch  78 | time: 126.66s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 3187.18 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 341.09s | training loss  0.08 |
    | end of validation epoch  79 | time: 126.64s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 3165.34 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 338.31s | training loss  0.07 |
    | end of validation epoch  80 | time: 127.84s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 3176.74 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 340.78s | training loss  0.07 |
    | end of validation epoch  81 | time: 125.68s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 3191.36 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 340.08s | training loss  0.07 |
    | end of validation epoch  82 | time: 129.51s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 3164.58 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 339.91s | training loss  0.07 |
    | end of validation epoch  83 | time: 125.87s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 3198.37 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 341.74s | training loss  0.07 |
    | end of validation epoch  84 | time: 128.58s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 3189.25 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 339.16s | training loss  0.07 |
    | end of validation epoch  85 | time: 127.30s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 3194.89 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 341.04s | training loss  0.06 |
    | end of validation epoch  86 | time: 126.46s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 3163.93 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 337.78s | training loss  0.07 |
    | end of validation epoch  87 | time: 127.57s | validation loss  1.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 3204.00 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 340.66s | training loss  0.07 |
    | end of validation epoch  88 | time: 125.35s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 3179.26 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 339.79s | training loss  0.07 |
    | end of validation epoch  89 | time: 130.81s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 3174.45 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 339.88s | training loss  0.07 |
    | end of validation epoch  90 | time: 125.80s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 3237.99 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 342.37s | training loss  0.06 |
    | end of validation epoch  91 | time: 128.23s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 3176.31 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 338.95s | training loss  0.06 |
    | end of validation epoch  92 | time: 127.62s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 3187.95 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 341.31s | training loss  0.06 |
    | end of validation epoch  93 | time: 127.40s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 3187.91 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 340.35s | training loss  0.05 |
    | end of validation epoch  94 | time: 129.87s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 3193.17 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 340.62s | training loss  0.06 |
    | end of validation epoch  95 | time: 125.42s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924, 0.05858845612755767] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954, 1.4018284201450417]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 3204.22 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 342.21s | training loss  0.07 |
    | end of validation epoch  96 | time: 130.37s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924, 0.05858845612755767, 0.07195970282663365] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954, 1.4018284201450417, 1.540624952496728]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 3173.61 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 340.35s | training loss  0.07 |
    | end of validation epoch  97 | time: 126.37s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924, 0.05858845612755767, 0.07195970282663365, 0.06532932335074555] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954, 1.4018284201450417, 1.540624952496728, 1.4364255946402409]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 3203.53 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 340.24s | training loss  0.06 |
    | end of validation epoch  98 | time: 128.50s | validation loss  1.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924, 0.05858845612755767, 0.07195970282663365, 0.06532932335074555, 0.059763864471434475] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954, 1.4018284201450417, 1.540624952496728, 1.4364255946402409, 1.7237915808573234]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 3168.85 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 338.40s | training loss  0.06 |
    | end of validation epoch  99 | time: 127.81s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [1.89430460843954, 1.2662910308923807, 1.0840992594624426, 0.9814035237372458, 0.8762254441106642, 0.7830626164470706, 0.7213302998392431, 0.6603547850170651, 0.5968255465095108, 0.5651305833378354, 0.5296530927623715, 0.4896219010288651, 0.4516942390987465, 0.43052008466140645, 0.4043363440144169, 0.40092610198635237, 0.36681879962886776, 0.35870488994830363, 0.33756611529771274, 0.29914246459265015, 0.3080845495333543, 0.28762484241176295, 0.28240566895351754, 0.26458530054167584, 0.2535006844245636, 0.24293308671530303, 0.255664863632069, 0.21694813715713518, 0.22659100978089883, 0.20462115917791118, 0.20924738130053958, 0.20132786713473433, 0.20863205187761033, 0.19695649987405484, 0.19809383268023398, 0.17445155421087333, 0.18327034952806998, 0.1546959746461194, 0.16604148093107585, 0.1539115464096671, 0.15432907748329747, 0.15033019887837204, 0.15639526266101245, 0.14435733483919688, 0.13541257176716048, 0.14185555611510534, 0.14396024748750097, 0.13177422189094998, 0.12144081536177043, 0.12347736134118326, 0.12372072302811854, 0.11228415037731866, 0.10950787608755065, 0.10156038893504185, 0.11177132879425813, 0.11556182805079597, 0.10822948288206044, 0.10805995300882035, 0.09514648975754106, 0.09919544161708506, 0.09684783317618542, 0.10235036431333504, 0.08869624966839412, 0.09135646317657586, 0.0980316909147544, 0.10031712706293072, 0.09728182634120588, 0.09259746571046275, 0.09095461139673586, 0.08106024908153592, 0.07579156634741807, 0.08659265270909748, 0.09647043931155323, 0.08056014025298593, 0.0803762891304654, 0.07755444479921648, 0.06836417098403783, 0.08087545013273353, 0.07685438332122725, 0.06944221345002989, 0.0671639370119518, 0.06747008208185434, 0.0737670979521296, 0.06541180527592833, 0.06744719444362966, 0.06471989905411327, 0.06806076331450059, 0.0694934447790991, 0.06680052873451968, 0.06759866707966672, 0.0643684966154061, 0.0639921678104312, 0.06318692271471829, 0.054453231394290924, 0.05858845612755767, 0.07195970282663365, 0.06532932335074555, 0.059763864471434475, 0.06353758555744682] validation loss is  [1.1320249023847282, 1.0501879301543038, 1.0420289406708132, 1.0314502807256456, 1.0704901262264077, 1.0521884667299066, 1.1645998917131994, 1.0409045626486961, 0.9971774549533924, 1.1466716118544962, 1.045548025382838, 1.0097167519464467, 1.1248185765968326, 1.2056872472167015, 0.9995888702008718, 1.1369931189110503, 1.175168605831762, 1.0759750497721445, 1.200598698516842, 1.2249727765253435, 1.0713156047762216, 1.1300211275714294, 1.0657022074835065, 1.2336957597872242, 1.1880534393712878, 1.299018410073283, 1.1494532493622198, 1.1909455075704802, 1.1384356757965481, 1.459821129469977, 1.161500779679045, 1.1960332141898107, 1.3218580751660436, 1.107349025318399, 1.2339818970067427, 1.2819801081592839, 1.1816322388670717, 1.2013386626106997, 1.2109051474835724, 1.3534951870048342, 1.400080901493008, 1.18193155581442, 1.3046015956788324, 1.4691206839246054, 1.385953313826273, 1.245069313098308, 1.3980647780699655, 1.393914608381844, 1.3012615251160848, 1.3062274402279097, 1.3254590640814665, 1.3835049783810973, 1.3551171562382176, 1.3161698724337232, 1.4550267854865524, 1.5787919755093753, 1.3201445116052735, 1.434734841187795, 1.5282142497696138, 1.465699891244488, 1.2476098562280338, 1.5933599521946842, 1.438578903404353, 1.4520206917841278, 1.3455379845969826, 1.538237371489231, 1.204223299685206, 1.4101755271403817, 1.432744355678248, 1.4437928458016056, 1.3077631415953874, 1.370138583210064, 1.4382664389098256, 1.3601523943289067, 1.573951592768329, 1.443187160276769, 1.348050267425909, 1.4273077675898094, 1.5531430200013954, 1.4957348321622703, 1.5881821211120648, 1.570295207357655, 1.5240068837811123, 1.5254370771387282, 1.582428964000428, 1.6305967373524861, 1.613675330033099, 1.5762888932285932, 1.3133957163960683, 1.4660777251798816, 1.553567676347522, 1.4658944577046593, 1.3716413586953422, 1.6846011791712954, 1.4018284201450417, 1.540624952496728, 1.4364255946402409, 1.7237915808573234, 1.4853008420711074]
