/run/nvme/job_2995875/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/scratch/asignal/shanshan/Audio-video-ACL/real_ssl_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 540.40 | loss  6.65 |
| epoch   1 |   200/  475 batches | ms/batch 307.86 | loss  6.65 |
| epoch   1 |   300/  475 batches | ms/batch 230.19 | loss  6.73 |
| epoch   1 |   400/  475 batches | ms/batch 191.37 | loss  6.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 82.82s | training loss  6.75 |
    | end of validation epoch   1 | time: 32.30s | validation loss  5.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.7473609532808005] validation loss is  [5.507659815940537]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 90.72 | loss  6.33 |
| epoch   2 |   200/  475 batches | ms/batch 81.12 | loss  5.98 |
| epoch   2 |   300/  475 batches | ms/batch 77.31 | loss  6.20 |
| epoch   2 |   400/  475 batches | ms/batch 75.51 | loss  5.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 35.84s | training loss  6.32 |
    | end of validation epoch   2 | time: 33.25s | validation loss  5.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.7473609532808005, 6.323365870024029] validation loss is  [5.507659815940537, 5.211971703697653]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 151.89 | loss  5.78 |
| epoch   3 |   200/  475 batches | ms/batch 110.42 | loss  6.20 |
| epoch   3 |   300/  475 batches | ms/batch 96.87 | loss  6.12 |
| epoch   3 |   400/  475 batches | ms/batch 90.14 | loss  5.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 41.68s | training loss  6.02 |
    | end of validation epoch   3 | time: 30.93s | validation loss  5.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 90.42 | loss  5.89 |
| epoch   4 |   200/  475 batches | ms/batch 80.25 | loss  5.73 |
| epoch   4 |   300/  475 batches | ms/batch 76.60 | loss  6.00 |
| epoch   4 |   400/  475 batches | ms/batch 75.07 | loss  5.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 35.69s | training loss  5.80 |
    | end of validation epoch   4 | time: 30.91s | validation loss  4.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 90.42 | loss  5.79 |
| epoch   5 |   200/  475 batches | ms/batch 79.72 | loss  5.34 |
| epoch   5 |   300/  475 batches | ms/batch 75.70 | loss  5.65 |
| epoch   5 |   400/  475 batches | ms/batch 74.10 | loss  5.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 35.46s | training loss  5.65 |
    | end of validation epoch   5 | time: 30.91s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 90.29 | loss  5.71 |
| epoch   6 |   200/  475 batches | ms/batch 79.61 | loss  5.61 |
| epoch   6 |   300/  475 batches | ms/batch 75.93 | loss  5.62 |
| epoch   6 |   400/  475 batches | ms/batch 73.96 | loss  5.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 35.22s | training loss  5.49 |
    | end of validation epoch   6 | time: 30.96s | validation loss  4.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 88.97 | loss  5.65 |
| epoch   7 |   200/  475 batches | ms/batch 79.06 | loss  4.98 |
| epoch   7 |   300/  475 batches | ms/batch 75.88 | loss  5.53 |
| epoch   7 |   400/  475 batches | ms/batch 73.93 | loss  5.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 35.41s | training loss  5.40 |
    | end of validation epoch   7 | time: 30.87s | validation loss  4.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 88.38 | loss  5.55 |
| epoch   8 |   200/  475 batches | ms/batch 79.27 | loss  5.28 |
| epoch   8 |   300/  475 batches | ms/batch 75.95 | loss  5.83 |
| epoch   8 |   400/  475 batches | ms/batch 74.52 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 35.48s | training loss  5.29 |
    | end of validation epoch   8 | time: 31.02s | validation loss  4.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 90.75 | loss  5.23 |
| epoch   9 |   200/  475 batches | ms/batch 80.04 | loss  5.66 |
| epoch   9 |   300/  475 batches | ms/batch 76.50 | loss  5.27 |
| epoch   9 |   400/  475 batches | ms/batch 74.68 | loss  5.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 35.86s | training loss  5.21 |
    | end of validation epoch   9 | time: 30.91s | validation loss  4.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 89.23 | loss  5.58 |
| epoch  10 |   200/  475 batches | ms/batch 79.58 | loss  5.28 |
| epoch  10 |   300/  475 batches | ms/batch 75.58 | loss  4.98 |
| epoch  10 |   400/  475 batches | ms/batch 74.00 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 35.38s | training loss  5.14 |
    | end of validation epoch  10 | time: 30.87s | validation loss  4.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 89.44 | loss  4.88 |
| epoch  11 |   200/  475 batches | ms/batch 79.47 | loss  4.78 |
| epoch  11 |   300/  475 batches | ms/batch 75.79 | loss  5.12 |
| epoch  11 |   400/  475 batches | ms/batch 74.13 | loss  5.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 35.32s | training loss  5.07 |
    | end of validation epoch  11 | time: 30.88s | validation loss  4.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 88.83 | loss  4.78 |
| epoch  12 |   200/  475 batches | ms/batch 78.84 | loss  5.12 |
| epoch  12 |   300/  475 batches | ms/batch 75.12 | loss  4.72 |
| epoch  12 |   400/  475 batches | ms/batch 73.61 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 35.07s | training loss  5.02 |
    | end of validation epoch  12 | time: 31.00s | validation loss  4.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 88.95 | loss  5.01 |
| epoch  13 |   200/  475 batches | ms/batch 78.90 | loss  5.09 |
| epoch  13 |   300/  475 batches | ms/batch 75.61 | loss  5.02 |
| epoch  13 |   400/  475 batches | ms/batch 73.97 | loss  5.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 35.23s | training loss  4.96 |
    | end of validation epoch  13 | time: 30.86s | validation loss  4.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 88.02 | loss  4.75 |
| epoch  14 |   200/  475 batches | ms/batch 79.15 | loss  4.99 |
| epoch  14 |   300/  475 batches | ms/batch 75.10 | loss  4.94 |
| epoch  14 |   400/  475 batches | ms/batch 73.62 | loss  5.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 35.18s | training loss  4.91 |
    | end of validation epoch  14 | time: 30.82s | validation loss  4.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 88.93 | loss  4.64 |
| epoch  15 |   200/  475 batches | ms/batch 79.45 | loss  4.91 |
| epoch  15 |   300/  475 batches | ms/batch 76.27 | loss  4.83 |
| epoch  15 |   400/  475 batches | ms/batch 74.19 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 35.32s | training loss  4.89 |
    | end of validation epoch  15 | time: 30.90s | validation loss  4.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 89.04 | loss  4.84 |
| epoch  16 |   200/  475 batches | ms/batch 78.58 | loss  5.22 |
| epoch  16 |   300/  475 batches | ms/batch 75.42 | loss  4.70 |
| epoch  16 |   400/  475 batches | ms/batch 73.96 | loss  4.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 35.28s | training loss  4.84 |
    | end of validation epoch  16 | time: 30.92s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 89.39 | loss  5.31 |
| epoch  17 |   200/  475 batches | ms/batch 79.06 | loss  4.77 |
| epoch  17 |   300/  475 batches | ms/batch 76.39 | loss  4.65 |
| epoch  17 |   400/  475 batches | ms/batch 73.69 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 35.20s | training loss  4.80 |
    | end of validation epoch  17 | time: 30.92s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 88.93 | loss  4.82 |
| epoch  18 |   200/  475 batches | ms/batch 78.83 | loss  4.64 |
| epoch  18 |   300/  475 batches | ms/batch 75.63 | loss  4.56 |
| epoch  18 |   400/  475 batches | ms/batch 73.72 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 35.13s | training loss  4.78 |
    | end of validation epoch  18 | time: 30.88s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 89.79 | loss  4.54 |
| epoch  19 |   200/  475 batches | ms/batch 79.07 | loss  4.80 |
| epoch  19 |   300/  475 batches | ms/batch 75.70 | loss  5.05 |
| epoch  19 |   400/  475 batches | ms/batch 74.39 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 35.20s | training loss  4.75 |
    | end of validation epoch  19 | time: 30.94s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 89.52 | loss  4.66 |
| epoch  20 |   200/  475 batches | ms/batch 79.48 | loss  4.76 |
| epoch  20 |   300/  475 batches | ms/batch 76.18 | loss  4.80 |
| epoch  20 |   400/  475 batches | ms/batch 74.75 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 35.50s | training loss  4.72 |
    | end of validation epoch  20 | time: 30.97s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 89.31 | loss  4.79 |
| epoch  21 |   200/  475 batches | ms/batch 79.20 | loss  4.92 |
| epoch  21 |   300/  475 batches | ms/batch 76.46 | loss  4.86 |
| epoch  21 |   400/  475 batches | ms/batch 74.69 | loss  4.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 35.60s | training loss  4.72 |
    | end of validation epoch  21 | time: 30.94s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 89.32 | loss  4.48 |
| epoch  22 |   200/  475 batches | ms/batch 79.70 | loss  4.43 |
| epoch  22 |   300/  475 batches | ms/batch 75.85 | loss  4.82 |
| epoch  22 |   400/  475 batches | ms/batch 74.00 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 35.42s | training loss  4.69 |
    | end of validation epoch  22 | time: 30.93s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 88.53 | loss  4.94 |
| epoch  23 |   200/  475 batches | ms/batch 80.27 | loss  4.95 |
| epoch  23 |   300/  475 batches | ms/batch 76.52 | loss  4.78 |
| epoch  23 |   400/  475 batches | ms/batch 74.14 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 35.38s | training loss  4.66 |
    | end of validation epoch  23 | time: 31.00s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 86.59 | loss  4.58 |
| epoch  24 |   200/  475 batches | ms/batch 79.28 | loss  4.55 |
| epoch  24 |   300/  475 batches | ms/batch 75.98 | loss  4.66 |
| epoch  24 |   400/  475 batches | ms/batch 73.89 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 35.24s | training loss  4.65 |
    | end of validation epoch  24 | time: 30.94s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 89.79 | loss  4.48 |
| epoch  25 |   200/  475 batches | ms/batch 79.21 | loss  5.02 |
| epoch  25 |   300/  475 batches | ms/batch 75.42 | loss  4.95 |
| epoch  25 |   400/  475 batches | ms/batch 74.41 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 35.24s | training loss  4.64 |
    | end of validation epoch  25 | time: 30.97s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 90.79 | loss  4.52 |
| epoch  26 |   200/  475 batches | ms/batch 80.56 | loss  4.89 |
| epoch  26 |   300/  475 batches | ms/batch 76.86 | loss  4.50 |
| epoch  26 |   400/  475 batches | ms/batch 74.65 | loss  4.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 35.67s | training loss  4.63 |
    | end of validation epoch  26 | time: 30.95s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 91.86 | loss  4.58 |
| epoch  27 |   200/  475 batches | ms/batch 80.00 | loss  4.69 |
| epoch  27 |   300/  475 batches | ms/batch 76.03 | loss  5.17 |
| epoch  27 |   400/  475 batches | ms/batch 74.61 | loss  4.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 35.55s | training loss  4.61 |
    | end of validation epoch  27 | time: 30.92s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 88.97 | loss  4.55 |
| epoch  28 |   200/  475 batches | ms/batch 78.60 | loss  4.45 |
| epoch  28 |   300/  475 batches | ms/batch 75.30 | loss  4.62 |
| epoch  28 |   400/  475 batches | ms/batch 73.89 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 35.06s | training loss  4.59 |
    | end of validation epoch  28 | time: 30.85s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 90.08 | loss  4.63 |
| epoch  29 |   200/  475 batches | ms/batch 79.75 | loss  4.74 |
| epoch  29 |   300/  475 batches | ms/batch 76.19 | loss  4.31 |
| epoch  29 |   400/  475 batches | ms/batch 74.06 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 35.34s | training loss  4.58 |
    | end of validation epoch  29 | time: 30.92s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 90.17 | loss  4.41 |
| epoch  30 |   200/  475 batches | ms/batch 81.22 | loss  4.42 |
| epoch  30 |   300/  475 batches | ms/batch 76.76 | loss  4.40 |
| epoch  30 |   400/  475 batches | ms/batch 74.25 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 35.29s | training loss  4.57 |
    | end of validation epoch  30 | time: 30.90s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 90.96 | loss  4.46 |
| epoch  31 |   200/  475 batches | ms/batch 80.88 | loss  4.63 |
| epoch  31 |   300/  475 batches | ms/batch 76.52 | loss  4.71 |
| epoch  31 |   400/  475 batches | ms/batch 74.34 | loss  4.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 35.42s | training loss  4.56 |
    | end of validation epoch  31 | time: 31.00s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 89.43 | loss  4.27 |
| epoch  32 |   200/  475 batches | ms/batch 80.39 | loss  4.60 |
| epoch  32 |   300/  475 batches | ms/batch 76.17 | loss  4.98 |
| epoch  32 |   400/  475 batches | ms/batch 74.00 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 35.35s | training loss  4.54 |
    | end of validation epoch  32 | time: 32.38s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 90.49 | loss  4.48 |
| epoch  33 |   200/  475 batches | ms/batch 80.12 | loss  4.45 |
| epoch  33 |   300/  475 batches | ms/batch 76.18 | loss  4.27 |
| epoch  33 |   400/  475 batches | ms/batch 74.25 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 35.60s | training loss  4.55 |
    | end of validation epoch  33 | time: 31.04s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 89.10 | loss  4.34 |
| epoch  34 |   200/  475 batches | ms/batch 79.78 | loss  4.57 |
| epoch  34 |   300/  475 batches | ms/batch 76.04 | loss  4.48 |
| epoch  34 |   400/  475 batches | ms/batch 73.98 | loss  4.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 35.28s | training loss  4.53 |
    | end of validation epoch  34 | time: 31.11s | validation loss  3.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 89.21 | loss  4.68 |
| epoch  35 |   200/  475 batches | ms/batch 79.78 | loss  4.52 |
| epoch  35 |   300/  475 batches | ms/batch 75.93 | loss  4.88 |
| epoch  35 |   400/  475 batches | ms/batch 73.99 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 35.38s | training loss  4.53 |
    | end of validation epoch  35 | time: 31.13s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 88.97 | loss  4.10 |
| epoch  36 |   200/  475 batches | ms/batch 78.76 | loss  4.25 |
| epoch  36 |   300/  475 batches | ms/batch 75.83 | loss  4.58 |
| epoch  36 |   400/  475 batches | ms/batch 74.10 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 35.38s | training loss  4.52 |
    | end of validation epoch  36 | time: 31.00s | validation loss  3.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 88.71 | loss  4.62 |
| epoch  37 |   200/  475 batches | ms/batch 79.23 | loss  4.77 |
| epoch  37 |   300/  475 batches | ms/batch 75.53 | loss  4.61 |
| epoch  37 |   400/  475 batches | ms/batch 73.62 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 35.10s | training loss  4.53 |
    | end of validation epoch  37 | time: 31.02s | validation loss  3.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 90.11 | loss  4.53 |
| epoch  38 |   200/  475 batches | ms/batch 78.81 | loss  4.68 |
| epoch  38 |   300/  475 batches | ms/batch 75.67 | loss  4.04 |
| epoch  38 |   400/  475 batches | ms/batch 74.28 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 35.26s | training loss  4.51 |
    | end of validation epoch  38 | time: 31.11s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 89.62 | loss  4.68 |
| epoch  39 |   200/  475 batches | ms/batch 79.85 | loss  4.49 |
| epoch  39 |   300/  475 batches | ms/batch 76.65 | loss  4.59 |
| epoch  39 |   400/  475 batches | ms/batch 74.88 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 35.56s | training loss  4.52 |
    | end of validation epoch  39 | time: 31.03s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627]
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 89.62 | loss  4.41 |
| epoch  40 |   200/  475 batches | ms/batch 79.46 | loss  4.13 |
| epoch  40 |   300/  475 batches | ms/batch 76.27 | loss  4.42 |
| epoch  40 |   400/  475 batches | ms/batch 74.17 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 35.32s | training loss  4.51 |
    | end of validation epoch  40 | time: 31.07s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 90.29 | loss  4.23 |
| epoch  41 |   200/  475 batches | ms/batch 79.29 | loss  4.42 |
| epoch  41 |   300/  475 batches | ms/batch 75.43 | loss  4.59 |
| epoch  41 |   400/  475 batches | ms/batch 73.91 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 35.39s | training loss  4.50 |
    | end of validation epoch  41 | time: 31.14s | validation loss  3.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 90.02 | loss  4.48 |
| epoch  42 |   200/  475 batches | ms/batch 80.25 | loss  4.46 |
| epoch  42 |   300/  475 batches | ms/batch 76.22 | loss  4.41 |
| epoch  42 |   400/  475 batches | ms/batch 74.25 | loss  4.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 35.51s | training loss  4.51 |
    | end of validation epoch  42 | time: 31.09s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 89.98 | loss  4.74 |
| epoch  43 |   200/  475 batches | ms/batch 80.14 | loss  4.67 |
| epoch  43 |   300/  475 batches | ms/batch 76.53 | loss  4.64 |
| epoch  43 |   400/  475 batches | ms/batch 74.46 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 35.48s | training loss  4.50 |
    | end of validation epoch  43 | time: 31.10s | validation loss  3.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 90.14 | loss  4.28 |
| epoch  44 |   200/  475 batches | ms/batch 79.85 | loss  4.52 |
| epoch  44 |   300/  475 batches | ms/batch 76.22 | loss  4.53 |
| epoch  44 |   400/  475 batches | ms/batch 74.59 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 35.45s | training loss  4.50 |
    | end of validation epoch  44 | time: 31.06s | validation loss  3.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147]
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 88.21 | loss  4.78 |
| epoch  45 |   200/  475 batches | ms/batch 78.72 | loss  4.45 |
| epoch  45 |   300/  475 batches | ms/batch 75.44 | loss  4.18 |
| epoch  45 |   400/  475 batches | ms/batch 73.93 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 35.47s | training loss  4.49 |
    | end of validation epoch  45 | time: 31.01s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 91.98 | loss  4.48 |
| epoch  46 |   200/  475 batches | ms/batch 80.12 | loss  4.30 |
| epoch  46 |   300/  475 batches | ms/batch 76.15 | loss  4.27 |
| epoch  46 |   400/  475 batches | ms/batch 74.40 | loss  4.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 35.37s | training loss  4.49 |
    | end of validation epoch  46 | time: 31.00s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 88.57 | loss  4.63 |
| epoch  47 |   200/  475 batches | ms/batch 79.23 | loss  4.53 |
| epoch  47 |   300/  475 batches | ms/batch 76.05 | loss  4.45 |
| epoch  47 |   400/  475 batches | ms/batch 75.23 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 35.36s | training loss  4.49 |
    | end of validation epoch  47 | time: 31.06s | validation loss  3.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994]
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 90.20 | loss  4.50 |
| epoch  48 |   200/  475 batches | ms/batch 79.60 | loss  4.57 |
| epoch  48 |   300/  475 batches | ms/batch 76.62 | loss  4.62 |
| epoch  48 |   400/  475 batches | ms/batch 74.89 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 35.73s | training loss  4.48 |
    | end of validation epoch  48 | time: 31.00s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 89.44 | loss  4.47 |
| epoch  49 |   200/  475 batches | ms/batch 78.59 | loss  4.30 |
| epoch  49 |   300/  475 batches | ms/batch 75.35 | loss  4.44 |
| epoch  49 |   400/  475 batches | ms/batch 74.36 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 35.46s | training loss  4.48 |
    | end of validation epoch  49 | time: 31.07s | validation loss  3.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 89.50 | loss  4.88 |
| epoch  50 |   200/  475 batches | ms/batch 79.18 | loss  4.76 |
| epoch  50 |   300/  475 batches | ms/batch 76.19 | loss  4.49 |
| epoch  50 |   400/  475 batches | ms/batch 74.50 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 35.50s | training loss  4.48 |
    | end of validation epoch  50 | time: 31.06s | validation loss  3.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 88.22 | loss  4.33 |
| epoch  51 |   200/  475 batches | ms/batch 79.13 | loss  4.49 |
| epoch  51 |   300/  475 batches | ms/batch 75.48 | loss  4.47 |
| epoch  51 |   400/  475 batches | ms/batch 73.84 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 35.18s | training loss  4.47 |
    | end of validation epoch  51 | time: 31.10s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 89.62 | loss  4.33 |
| epoch  52 |   200/  475 batches | ms/batch 78.58 | loss  4.43 |
| epoch  52 |   300/  475 batches | ms/batch 75.57 | loss  4.43 |
| epoch  52 |   400/  475 batches | ms/batch 73.70 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 35.18s | training loss  4.48 |
    | end of validation epoch  52 | time: 31.11s | validation loss  3.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036]
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 90.34 | loss  4.31 |
| epoch  53 |   200/  475 batches | ms/batch 79.72 | loss  4.39 |
| epoch  53 |   300/  475 batches | ms/batch 76.32 | loss  4.55 |
| epoch  53 |   400/  475 batches | ms/batch 74.57 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 35.51s | training loss  4.48 |
    | end of validation epoch  53 | time: 30.92s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394]
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 89.14 | loss  4.40 |
| epoch  54 |   200/  475 batches | ms/batch 78.79 | loss  4.36 |
| epoch  54 |   300/  475 batches | ms/batch 75.75 | loss  4.33 |
| epoch  54 |   400/  475 batches | ms/batch 74.01 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 35.39s | training loss  4.47 |
    | end of validation epoch  54 | time: 31.14s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 90.81 | loss  4.46 |
| epoch  55 |   200/  475 batches | ms/batch 79.24 | loss  4.59 |
| epoch  55 |   300/  475 batches | ms/batch 75.89 | loss  4.39 |
| epoch  55 |   400/  475 batches | ms/batch 74.13 | loss  4.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 35.29s | training loss  4.47 |
    | end of validation epoch  55 | time: 31.07s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567]
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 89.48 | loss  4.21 |
| epoch  56 |   200/  475 batches | ms/batch 79.47 | loss  4.37 |
| epoch  56 |   300/  475 batches | ms/batch 76.03 | loss  4.44 |
| epoch  56 |   400/  475 batches | ms/batch 74.45 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 35.48s | training loss  4.48 |
    | end of validation epoch  56 | time: 31.00s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 90.35 | loss  4.07 |
| epoch  57 |   200/  475 batches | ms/batch 79.07 | loss  4.20 |
| epoch  57 |   300/  475 batches | ms/batch 76.07 | loss  4.68 |
| epoch  57 |   400/  475 batches | ms/batch 74.51 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 35.52s | training loss  4.47 |
    | end of validation epoch  57 | time: 31.02s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352]
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 89.66 | loss  4.62 |
| epoch  58 |   200/  475 batches | ms/batch 79.09 | loss  4.56 |
| epoch  58 |   300/  475 batches | ms/batch 75.61 | loss  4.30 |
| epoch  58 |   400/  475 batches | ms/batch 74.12 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 35.23s | training loss  4.46 |
    | end of validation epoch  58 | time: 31.07s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 89.65 | loss  4.22 |
| epoch  59 |   200/  475 batches | ms/batch 80.88 | loss  4.48 |
| epoch  59 |   300/  475 batches | ms/batch 76.56 | loss  4.40 |
| epoch  59 |   400/  475 batches | ms/batch 74.44 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 35.50s | training loss  4.46 |
    | end of validation epoch  59 | time: 31.04s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 90.18 | loss  4.68 |
| epoch  60 |   200/  475 batches | ms/batch 79.24 | loss  4.39 |
| epoch  60 |   300/  475 batches | ms/batch 75.81 | loss  4.62 |
| epoch  60 |   400/  475 batches | ms/batch 74.37 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 35.52s | training loss  4.48 |
    | end of validation epoch  60 | time: 31.04s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877]
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 91.25 | loss  4.83 |
| epoch  61 |   200/  475 batches | ms/batch 79.85 | loss  4.51 |
| epoch  61 |   300/  475 batches | ms/batch 75.93 | loss  4.39 |
| epoch  61 |   400/  475 batches | ms/batch 74.19 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 37.01s | training loss  4.47 |
    | end of validation epoch  61 | time: 31.02s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425]
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 88.75 | loss  4.30 |
| epoch  62 |   200/  475 batches | ms/batch 79.13 | loss  4.32 |
| epoch  62 |   300/  475 batches | ms/batch 76.08 | loss  4.32 |
| epoch  62 |   400/  475 batches | ms/batch 74.47 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 35.62s | training loss  4.48 |
    | end of validation epoch  62 | time: 31.06s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 89.96 | loss  4.17 |
| epoch  63 |   200/  475 batches | ms/batch 80.40 | loss  4.45 |
| epoch  63 |   300/  475 batches | ms/batch 76.37 | loss  4.25 |
| epoch  63 |   400/  475 batches | ms/batch 74.71 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 35.74s | training loss  4.46 |
    | end of validation epoch  63 | time: 31.01s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 88.98 | loss  4.34 |
| epoch  64 |   200/  475 batches | ms/batch 80.37 | loss  4.41 |
| epoch  64 |   300/  475 batches | ms/batch 76.27 | loss  4.41 |
| epoch  64 |   400/  475 batches | ms/batch 74.36 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 35.47s | training loss  4.47 |
    | end of validation epoch  64 | time: 31.11s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462]
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 91.52 | loss  4.30 |
| epoch  65 |   200/  475 batches | ms/batch 80.40 | loss  4.71 |
| epoch  65 |   300/  475 batches | ms/batch 76.42 | loss  4.16 |
| epoch  65 |   400/  475 batches | ms/batch 74.39 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 35.49s | training loss  4.47 |
    | end of validation epoch  65 | time: 31.01s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 90.11 | loss  4.54 |
| epoch  66 |   200/  475 batches | ms/batch 78.93 | loss  4.35 |
| epoch  66 |   300/  475 batches | ms/batch 75.79 | loss  4.62 |
| epoch  66 |   400/  475 batches | ms/batch 74.20 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 35.43s | training loss  4.46 |
    | end of validation epoch  66 | time: 31.07s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 89.58 | loss  4.54 |
| epoch  67 |   200/  475 batches | ms/batch 79.35 | loss  4.22 |
| epoch  67 |   300/  475 batches | ms/batch 76.21 | loss  4.34 |
| epoch  67 |   400/  475 batches | ms/batch 74.47 | loss  4.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 35.46s | training loss  4.46 |
    | end of validation epoch  67 | time: 31.13s | validation loss  3.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 89.17 | loss  4.02 |
| epoch  68 |   200/  475 batches | ms/batch 79.73 | loss  4.67 |
| epoch  68 |   300/  475 batches | ms/batch 75.51 | loss  4.57 |
| epoch  68 |   400/  475 batches | ms/batch 73.85 | loss  4.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 35.35s | training loss  4.45 |
    | end of validation epoch  68 | time: 31.06s | validation loss  3.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 89.84 | loss  4.38 |
| epoch  69 |   200/  475 batches | ms/batch 79.10 | loss  4.60 |
| epoch  69 |   300/  475 batches | ms/batch 75.63 | loss  4.65 |
| epoch  69 |   400/  475 batches | ms/batch 74.03 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 35.32s | training loss  4.47 |
    | end of validation epoch  69 | time: 31.10s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 89.18 | loss  4.33 |
| epoch  70 |   200/  475 batches | ms/batch 78.70 | loss  4.40 |
| epoch  70 |   300/  475 batches | ms/batch 75.44 | loss  4.62 |
| epoch  70 |   400/  475 batches | ms/batch 73.79 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 35.23s | training loss  4.46 |
    | end of validation epoch  70 | time: 30.99s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029]
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 88.34 | loss  4.54 |
| epoch  71 |   200/  475 batches | ms/batch 79.01 | loss  4.67 |
| epoch  71 |   300/  475 batches | ms/batch 75.59 | loss  4.48 |
| epoch  71 |   400/  475 batches | ms/batch 74.15 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 35.44s | training loss  4.46 |
    | end of validation epoch  71 | time: 31.02s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 89.83 | loss  4.70 |
| epoch  72 |   200/  475 batches | ms/batch 79.24 | loss  4.47 |
| epoch  72 |   300/  475 batches | ms/batch 75.52 | loss  4.36 |
| epoch  72 |   400/  475 batches | ms/batch 73.92 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 35.29s | training loss  4.46 |
    | end of validation epoch  72 | time: 31.19s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882]
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 89.92 | loss  4.43 |
| epoch  73 |   200/  475 batches | ms/batch 79.59 | loss  4.46 |
| epoch  73 |   300/  475 batches | ms/batch 75.34 | loss  4.41 |
| epoch  73 |   400/  475 batches | ms/batch 73.94 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 35.25s | training loss  4.46 |
    | end of validation epoch  73 | time: 31.05s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 89.33 | loss  4.44 |
| epoch  74 |   200/  475 batches | ms/batch 80.34 | loss  4.57 |
| epoch  74 |   300/  475 batches | ms/batch 77.13 | loss  4.07 |
| epoch  74 |   400/  475 batches | ms/batch 75.30 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 35.92s | training loss  4.45 |
    | end of validation epoch  74 | time: 31.04s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 88.96 | loss  4.71 |
| epoch  75 |   200/  475 batches | ms/batch 78.78 | loss  4.79 |
| epoch  75 |   300/  475 batches | ms/batch 75.74 | loss  4.35 |
| epoch  75 |   400/  475 batches | ms/batch 74.00 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 35.29s | training loss  4.46 |
    | end of validation epoch  75 | time: 31.06s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 89.16 | loss  4.22 |
| epoch  76 |   200/  475 batches | ms/batch 79.14 | loss  4.47 |
| epoch  76 |   300/  475 batches | ms/batch 75.37 | loss  4.60 |
| epoch  76 |   400/  475 batches | ms/batch 73.58 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 35.18s | training loss  4.46 |
    | end of validation epoch  76 | time: 31.01s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 88.54 | loss  4.52 |
| epoch  77 |   200/  475 batches | ms/batch 79.01 | loss  4.93 |
| epoch  77 |   300/  475 batches | ms/batch 75.66 | loss  4.24 |
| epoch  77 |   400/  475 batches | ms/batch 73.55 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 35.31s | training loss  4.46 |
    | end of validation epoch  77 | time: 31.21s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156]
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 89.59 | loss  4.31 |
| epoch  78 |   200/  475 batches | ms/batch 79.35 | loss  4.90 |
| epoch  78 |   300/  475 batches | ms/batch 76.31 | loss  4.64 |
| epoch  78 |   400/  475 batches | ms/batch 74.51 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 35.52s | training loss  4.46 |
    | end of validation epoch  78 | time: 30.99s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 89.71 | loss  4.44 |
| epoch  79 |   200/  475 batches | ms/batch 78.98 | loss  4.45 |
| epoch  79 |   300/  475 batches | ms/batch 75.60 | loss  4.64 |
| epoch  79 |   400/  475 batches | ms/batch 74.37 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 35.37s | training loss  4.44 |
    | end of validation epoch  79 | time: 31.04s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 90.57 | loss  4.48 |
| epoch  80 |   200/  475 batches | ms/batch 80.10 | loss  5.08 |
| epoch  80 |   300/  475 batches | ms/batch 76.09 | loss  4.39 |
| epoch  80 |   400/  475 batches | ms/batch 74.34 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 35.42s | training loss  4.48 |
    | end of validation epoch  80 | time: 31.06s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 88.92 | loss  4.55 |
| epoch  81 |   200/  475 batches | ms/batch 79.59 | loss  4.52 |
| epoch  81 |   300/  475 batches | ms/batch 76.01 | loss  4.44 |
| epoch  81 |   400/  475 batches | ms/batch 74.45 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 35.40s | training loss  4.46 |
    | end of validation epoch  81 | time: 31.15s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 90.33 | loss  4.59 |
| epoch  82 |   200/  475 batches | ms/batch 79.94 | loss  4.57 |
| epoch  82 |   300/  475 batches | ms/batch 76.58 | loss  4.26 |
| epoch  82 |   400/  475 batches | ms/batch 74.64 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 35.66s | training loss  4.45 |
    | end of validation epoch  82 | time: 31.06s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 93.22 | loss  4.40 |
| epoch  83 |   200/  475 batches | ms/batch 81.37 | loss  4.42 |
| epoch  83 |   300/  475 batches | ms/batch 77.49 | loss  4.67 |
| epoch  83 |   400/  475 batches | ms/batch 75.58 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 35.87s | training loss  4.45 |
    | end of validation epoch  83 | time: 30.96s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 90.08 | loss  4.89 |
| epoch  84 |   200/  475 batches | ms/batch 80.19 | loss  4.22 |
| epoch  84 |   300/  475 batches | ms/batch 76.28 | loss  4.30 |
| epoch  84 |   400/  475 batches | ms/batch 74.77 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 35.55s | training loss  4.45 |
    | end of validation epoch  84 | time: 30.93s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 89.93 | loss  4.44 |
| epoch  85 |   200/  475 batches | ms/batch 79.25 | loss  4.33 |
| epoch  85 |   300/  475 batches | ms/batch 75.57 | loss  4.66 |
| epoch  85 |   400/  475 batches | ms/batch 73.61 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 35.13s | training loss  4.44 |
    | end of validation epoch  85 | time: 30.92s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 90.13 | loss  4.77 |
| epoch  86 |   200/  475 batches | ms/batch 79.78 | loss  4.91 |
| epoch  86 |   300/  475 batches | ms/batch 76.04 | loss  4.39 |
| epoch  86 |   400/  475 batches | ms/batch 74.72 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 35.62s | training loss  4.44 |
    | end of validation epoch  86 | time: 30.92s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 89.82 | loss  4.41 |
| epoch  87 |   200/  475 batches | ms/batch 78.21 | loss  3.97 |
| epoch  87 |   300/  475 batches | ms/batch 75.37 | loss  4.10 |
| epoch  87 |   400/  475 batches | ms/batch 73.89 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 35.11s | training loss  4.45 |
    | end of validation epoch  87 | time: 30.95s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 89.32 | loss  4.51 |
| epoch  88 |   200/  475 batches | ms/batch 79.15 | loss  4.68 |
| epoch  88 |   300/  475 batches | ms/batch 75.81 | loss  4.44 |
| epoch  88 |   400/  475 batches | ms/batch 73.80 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 35.19s | training loss  4.46 |
    | end of validation epoch  88 | time: 30.95s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 89.57 | loss  4.68 |
| epoch  89 |   200/  475 batches | ms/batch 79.25 | loss  4.65 |
| epoch  89 |   300/  475 batches | ms/batch 75.29 | loss  4.52 |
| epoch  89 |   400/  475 batches | ms/batch 73.66 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 35.11s | training loss  4.45 |
    | end of validation epoch  89 | time: 30.82s | validation loss  3.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 90.98 | loss  4.67 |
| epoch  90 |   200/  475 batches | ms/batch 80.69 | loss  4.63 |
| epoch  90 |   300/  475 batches | ms/batch 76.27 | loss  4.48 |
| epoch  90 |   400/  475 batches | ms/batch 74.50 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 35.59s | training loss  4.45 |
    | end of validation epoch  90 | time: 30.83s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 89.68 | loss  4.75 |
| epoch  91 |   200/  475 batches | ms/batch 78.65 | loss  4.39 |
| epoch  91 |   300/  475 batches | ms/batch 75.34 | loss  4.43 |
| epoch  91 |   400/  475 batches | ms/batch 73.96 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 35.22s | training loss  4.45 |
    | end of validation epoch  91 | time: 30.86s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 88.57 | loss  4.90 |
| epoch  92 |   200/  475 batches | ms/batch 79.20 | loss  4.34 |
| epoch  92 |   300/  475 batches | ms/batch 76.07 | loss  3.89 |
| epoch  92 |   400/  475 batches | ms/batch 74.00 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 35.32s | training loss  4.45 |
    | end of validation epoch  92 | time: 30.93s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 89.76 | loss  4.20 |
| epoch  93 |   200/  475 batches | ms/batch 79.12 | loss  4.70 |
| epoch  93 |   300/  475 batches | ms/batch 76.74 | loss  4.77 |
| epoch  93 |   400/  475 batches | ms/batch 74.53 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 35.40s | training loss  4.45 |
    | end of validation epoch  93 | time: 30.88s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 92.40 | loss  4.74 |
| epoch  94 |   200/  475 batches | ms/batch 81.16 | loss  4.36 |
| epoch  94 |   300/  475 batches | ms/batch 76.37 | loss  4.24 |
| epoch  94 |   400/  475 batches | ms/batch 74.61 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 35.67s | training loss  4.46 |
    | end of validation epoch  94 | time: 30.87s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 89.03 | loss  4.31 |
| epoch  95 |   200/  475 batches | ms/batch 79.79 | loss  4.61 |
| epoch  95 |   300/  475 batches | ms/batch 76.10 | loss  4.44 |
| epoch  95 |   400/  475 batches | ms/batch 74.27 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 35.38s | training loss  4.45 |
    | end of validation epoch  95 | time: 30.85s | validation loss  3.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243, 4.453350426523309] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481, 3.870450300328872]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 90.85 | loss  4.61 |
| epoch  96 |   200/  475 batches | ms/batch 79.41 | loss  4.79 |
| epoch  96 |   300/  475 batches | ms/batch 76.05 | loss  4.46 |
| epoch  96 |   400/  475 batches | ms/batch 74.46 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 35.44s | training loss  4.44 |
    | end of validation epoch  96 | time: 30.91s | validation loss  3.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243, 4.453350426523309, 4.442015206186395] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481, 3.870450300328872, 3.8504754334938625]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 88.32 | loss  4.63 |
| epoch  97 |   200/  475 batches | ms/batch 79.30 | loss  4.33 |
| epoch  97 |   300/  475 batches | ms/batch 76.20 | loss  3.97 |
| epoch  97 |   400/  475 batches | ms/batch 74.59 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 35.47s | training loss  4.45 |
    | end of validation epoch  97 | time: 30.92s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243, 4.453350426523309, 4.442015206186395, 4.4478389258133735] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481, 3.870450300328872, 3.8504754334938625, 3.880299896753135]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 89.77 | loss  4.55 |
| epoch  98 |   200/  475 batches | ms/batch 79.49 | loss  4.61 |
| epoch  98 |   300/  475 batches | ms/batch 76.06 | loss  4.23 |
| epoch  98 |   400/  475 batches | ms/batch 74.41 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 35.46s | training loss  4.46 |
    | end of validation epoch  98 | time: 30.89s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243, 4.453350426523309, 4.442015206186395, 4.4478389258133735, 4.456038322950664] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481, 3.870450300328872, 3.8504754334938625, 3.880299896753135, 3.880001669170476]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 89.61 | loss  4.55 |
| epoch  99 |   200/  475 batches | ms/batch 79.10 | loss  4.69 |
| epoch  99 |   300/  475 batches | ms/batch 75.50 | loss  4.49 |
| epoch  99 |   400/  475 batches | ms/batch 73.85 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 35.38s | training loss  4.44 |
    | end of validation epoch  99 | time: 30.96s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.7473609532808005, 6.323365870024029, 6.023972091674804, 5.802454280853271, 5.65429058576885, 5.492277057045384, 5.395449099289744, 5.291256851397063, 5.207574872468647, 5.138324616080836, 5.074426779496043, 5.016426352450722, 4.96279444945486, 4.9122828272769326, 4.88595744183189, 4.837948343377365, 4.798171143782766, 4.780776712517989, 4.754566381856015, 4.721094724755538, 4.71879353473061, 4.691310868514211, 4.659170026277241, 4.650131659256784, 4.636819167890047, 4.626807908509907, 4.606234388853374, 4.5905208221234775, 4.57844023855109, 4.566079893614116, 4.562497052644429, 4.539286187322516, 4.54558410744918, 4.532463673039486, 4.529051542784038, 4.523052371175665, 4.525523637470446, 4.50552538721185, 4.5243648785039, 4.513776798248291, 4.502493918067531, 4.513776950334248, 4.4956809560876145, 4.4981733246853475, 4.488056091007434, 4.48732948353416, 4.491091386393497, 4.481262642709832, 4.477701269952875, 4.475683274520071, 4.469661090750443, 4.484749131955598, 4.4777067284835015, 4.473646301470305, 4.474852530328851, 4.476036963211863, 4.471457239954095, 4.463174988596063, 4.458104044261732, 4.477483884911788, 4.46781401333056, 4.475963055460077, 4.4593095945057115, 4.467478163869758, 4.467946426993922, 4.457890796661377, 4.455796872691105, 4.450856569189774, 4.469837510460302, 4.462620870690596, 4.464601101624338, 4.463875507555509, 4.45690780589455, 4.454028007105777, 4.464425026743036, 4.459659915221365, 4.46097991893166, 4.460699635053936, 4.443656185049759, 4.47721758290341, 4.45847060303939, 4.45416862487793, 4.452155999635395, 4.450469046140972, 4.441073864384701, 4.442695884202656, 4.4508898202996505, 4.456320544794986, 4.448721187491166, 4.453800892578928, 4.452700696242483, 4.452786028008712, 4.446023465708683, 4.461547990096243, 4.453350426523309, 4.442015206186395, 4.4478389258133735, 4.456038322950664, 4.442834866674323] validation loss is  [5.507659815940537, 5.211971703697653, 5.0295376176593685, 4.860652699190028, 4.716711348846179, 4.622404976051395, 4.534185609897645, 4.473730047209924, 4.410625674143559, 4.3422285889377115, 4.328616611096037, 4.268033165891631, 4.260088457780726, 4.22625277623409, 4.225996630532401, 4.172643863854288, 4.125107915461564, 4.128590118985216, 4.113113283109264, 4.103086048815431, 4.069460904898763, 4.052587404972365, 4.061564946374974, 4.038292480116131, 4.027718642178704, 4.0022312893587, 4.002435912605093, 4.020118374784453, 3.990291737708725, 3.986541717994113, 3.99285906102477, 3.9792073313929452, 3.977107871480349, 3.9686181825750015, 3.9453444981775365, 3.9650568441182625, 3.9330567151558498, 3.9495165808861996, 3.9482237551392627, 3.921052125321717, 3.926417114353981, 3.916544279130567, 3.9267708473846694, 3.9409160553908147, 3.922894013028185, 3.945167743859171, 3.9362091737634994, 3.9229736308089826, 3.926663885597421, 3.91407304651597, 3.895536022025998, 3.911179374246036, 3.9033459695447394, 3.8979725977953743, 3.8956804375688567, 3.8938844244019326, 3.894397982028352, 3.883421130540992, 3.8947954778911686, 3.8973618234906877, 3.8848764736111425, 3.8882886361675104, 3.884489237761297, 3.901555592272462, 3.8847857883998325, 3.892537726073706, 3.8749656717316445, 3.8732918410741983, 3.8924559625256965, 3.889692532916029, 3.8859257998586703, 3.8945811776553882, 3.886579970351788, 3.895076649529593, 3.894127266747611, 3.891553922861564, 3.8785903253475156, 3.8635742524090935, 3.9016749237765787, 3.8898790223257884, 3.8812611463691007, 3.891065258939727, 3.899566620337863, 3.8796709926188493, 3.882183415549142, 3.894925089443431, 3.8641525216463233, 3.8825127797968246, 3.873894234665302, 3.8771921386237906, 3.8776075098694873, 3.89498286086972, 3.875701173012998, 3.876490356541481, 3.870450300328872, 3.8504754334938625, 3.880299896753135, 3.880001669170476, 3.879772148212465]
