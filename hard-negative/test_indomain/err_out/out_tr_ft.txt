/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/super_hard_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 172.29 | loss  5.93 |
| epoch   1 |   200/  475 batches | ms/batch 155.94 | loss  5.83 |
| epoch   1 |   300/  475 batches | ms/batch 150.04 | loss  5.62 |
| epoch   1 |   400/  475 batches | ms/batch 147.44 | loss  5.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 69.71s | training loss  5.85 |
    | end of validation epoch   1 | time: 55.24s | validation loss  5.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.852484442058363] validation loss is  [5.449494638362853]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 170.70 | loss  5.52 |
| epoch   2 |   200/  475 batches | ms/batch 151.96 | loss  5.46 |
| epoch   2 |   300/  475 batches | ms/batch 146.37 | loss  5.37 |
| epoch   2 |   400/  475 batches | ms/batch 144.92 | loss  5.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 68.48s | training loss  5.43 |
    | end of validation epoch   2 | time: 53.98s | validation loss  5.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.852484442058363, 5.433862408085873] validation loss is  [5.449494638362853, 5.066381009686895]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 165.00 | loss  5.23 |
| epoch   3 |   200/  475 batches | ms/batch 149.04 | loss  5.41 |
| epoch   3 |   300/  475 batches | ms/batch 145.58 | loss  5.15 |
| epoch   3 |   400/  475 batches | ms/batch 142.59 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 68.07s | training loss  5.20 |
    | end of validation epoch   3 | time: 54.14s | validation loss  4.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 164.67 | loss  5.27 |
| epoch   4 |   200/  475 batches | ms/batch 151.30 | loss  5.10 |
| epoch   4 |   300/  475 batches | ms/batch 145.66 | loss  4.94 |
| epoch   4 |   400/  475 batches | ms/batch 143.34 | loss  5.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 67.95s | training loss  4.98 |
    | end of validation epoch   4 | time: 55.11s | validation loss  4.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 162.81 | loss  4.78 |
| epoch   5 |   200/  475 batches | ms/batch 149.93 | loss  4.62 |
| epoch   5 |   300/  475 batches | ms/batch 146.55 | loss  4.70 |
| epoch   5 |   400/  475 batches | ms/batch 144.00 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 68.21s | training loss  4.82 |
    | end of validation epoch   5 | time: 53.99s | validation loss  4.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 163.30 | loss  4.84 |
| epoch   6 |   200/  475 batches | ms/batch 150.44 | loss  4.40 |
| epoch   6 |   300/  475 batches | ms/batch 146.50 | loss  4.72 |
| epoch   6 |   400/  475 batches | ms/batch 144.23 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 68.46s | training loss  4.69 |
    | end of validation epoch   6 | time: 53.14s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 162.96 | loss  4.47 |
| epoch   7 |   200/  475 batches | ms/batch 150.36 | loss  4.56 |
| epoch   7 |   300/  475 batches | ms/batch 145.75 | loss  4.68 |
| epoch   7 |   400/  475 batches | ms/batch 143.35 | loss  4.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 68.16s | training loss  4.56 |
    | end of validation epoch   7 | time: 53.03s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 168.74 | loss  4.51 |
| epoch   8 |   200/  475 batches | ms/batch 151.45 | loss  4.30 |
| epoch   8 |   300/  475 batches | ms/batch 146.66 | loss  4.31 |
| epoch   8 |   400/  475 batches | ms/batch 144.30 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 68.35s | training loss  4.46 |
    | end of validation epoch   8 | time: 54.09s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 166.67 | loss  4.66 |
| epoch   9 |   200/  475 batches | ms/batch 152.14 | loss  4.19 |
| epoch   9 |   300/  475 batches | ms/batch 146.40 | loss  4.20 |
| epoch   9 |   400/  475 batches | ms/batch 143.97 | loss  4.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 68.14s | training loss  4.37 |
    | end of validation epoch   9 | time: 53.37s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 163.40 | loss  4.16 |
| epoch  10 |   200/  475 batches | ms/batch 150.01 | loss  4.19 |
| epoch  10 |   300/  475 batches | ms/batch 145.47 | loss  4.64 |
| epoch  10 |   400/  475 batches | ms/batch 142.75 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 67.73s | training loss  4.30 |
    | end of validation epoch  10 | time: 54.09s | validation loss  3.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 151.83 | loss  4.63 |
| epoch  11 |   200/  475 batches | ms/batch 150.06 | loss  4.28 |
| epoch  11 |   300/  475 batches | ms/batch 145.60 | loss  4.14 |
| epoch  11 |   400/  475 batches | ms/batch 143.37 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 68.06s | training loss  4.23 |
    | end of validation epoch  11 | time: 55.26s | validation loss  3.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 167.51 | loss  4.09 |
| epoch  12 |   200/  475 batches | ms/batch 153.18 | loss  3.88 |
| epoch  12 |   300/  475 batches | ms/batch 148.26 | loss  4.11 |
| epoch  12 |   400/  475 batches | ms/batch 144.49 | loss  4.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 69.06s | training loss  4.18 |
    | end of validation epoch  12 | time: 54.20s | validation loss  3.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 155.44 | loss  4.27 |
| epoch  13 |   200/  475 batches | ms/batch 146.54 | loss  4.11 |
| epoch  13 |   300/  475 batches | ms/batch 144.06 | loss  4.03 |
| epoch  13 |   400/  475 batches | ms/batch 142.00 | loss  3.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 67.92s | training loss  4.13 |
    | end of validation epoch  13 | time: 54.50s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 161.55 | loss  4.09 |
| epoch  14 |   200/  475 batches | ms/batch 148.68 | loss  4.01 |
| epoch  14 |   300/  475 batches | ms/batch 145.40 | loss  3.63 |
| epoch  14 |   400/  475 batches | ms/batch 143.13 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 68.04s | training loss  4.07 |
    | end of validation epoch  14 | time: 53.67s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 164.90 | loss  4.04 |
| epoch  15 |   200/  475 batches | ms/batch 150.89 | loss  4.11 |
| epoch  15 |   300/  475 batches | ms/batch 144.71 | loss  4.05 |
| epoch  15 |   400/  475 batches | ms/batch 143.16 | loss  4.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 67.89s | training loss  4.03 |
    | end of validation epoch  15 | time: 54.34s | validation loss  3.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 162.64 | loss  4.33 |
| epoch  16 |   200/  475 batches | ms/batch 149.64 | loss  3.91 |
| epoch  16 |   300/  475 batches | ms/batch 146.55 | loss  3.90 |
| epoch  16 |   400/  475 batches | ms/batch 144.71 | loss  3.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 68.69s | training loss  3.98 |
    | end of validation epoch  16 | time: 53.79s | validation loss  3.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 166.98 | loss  4.13 |
| epoch  17 |   200/  475 batches | ms/batch 150.98 | loss  4.10 |
| epoch  17 |   300/  475 batches | ms/batch 147.44 | loss  4.01 |
| epoch  17 |   400/  475 batches | ms/batch 144.83 | loss  3.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 68.89s | training loss  3.94 |
    | end of validation epoch  17 | time: 53.21s | validation loss  3.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 167.97 | loss  4.19 |
| epoch  18 |   200/  475 batches | ms/batch 151.72 | loss  3.65 |
| epoch  18 |   300/  475 batches | ms/batch 145.78 | loss  3.58 |
| epoch  18 |   400/  475 batches | ms/batch 143.42 | loss  3.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 67.84s | training loss  3.92 |
    | end of validation epoch  18 | time: 53.63s | validation loss  3.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 165.65 | loss  3.59 |
| epoch  19 |   200/  475 batches | ms/batch 150.60 | loss  3.96 |
| epoch  19 |   300/  475 batches | ms/batch 146.40 | loss  3.68 |
| epoch  19 |   400/  475 batches | ms/batch 144.18 | loss  4.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 68.73s | training loss  3.88 |
    | end of validation epoch  19 | time: 52.62s | validation loss  3.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 165.81 | loss  4.02 |
| epoch  20 |   200/  475 batches | ms/batch 152.71 | loss  3.81 |
| epoch  20 |   300/  475 batches | ms/batch 148.19 | loss  3.79 |
| epoch  20 |   400/  475 batches | ms/batch 145.45 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 68.55s | training loss  3.85 |
    | end of validation epoch  20 | time: 53.98s | validation loss  3.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 165.47 | loss  3.72 |
| epoch  21 |   200/  475 batches | ms/batch 152.49 | loss  3.67 |
| epoch  21 |   300/  475 batches | ms/batch 148.15 | loss  3.18 |
| epoch  21 |   400/  475 batches | ms/batch 145.57 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 68.74s | training loss  3.81 |
    | end of validation epoch  21 | time: 52.75s | validation loss  3.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 165.72 | loss  4.01 |
| epoch  22 |   200/  475 batches | ms/batch 151.79 | loss  3.60 |
| epoch  22 |   300/  475 batches | ms/batch 147.16 | loss  4.08 |
| epoch  22 |   400/  475 batches | ms/batch 144.36 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 68.66s | training loss  3.80 |
    | end of validation epoch  22 | time: 56.02s | validation loss  3.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 166.12 | loss  4.14 |
| epoch  23 |   200/  475 batches | ms/batch 152.22 | loss  3.78 |
| epoch  23 |   300/  475 batches | ms/batch 148.06 | loss  3.77 |
| epoch  23 |   400/  475 batches | ms/batch 145.72 | loss  3.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 68.80s | training loss  3.76 |
    | end of validation epoch  23 | time: 54.21s | validation loss  3.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 166.47 | loss  3.45 |
| epoch  24 |   200/  475 batches | ms/batch 152.28 | loss  3.73 |
| epoch  24 |   300/  475 batches | ms/batch 145.94 | loss  3.33 |
| epoch  24 |   400/  475 batches | ms/batch 144.61 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 68.54s | training loss  3.75 |
    | end of validation epoch  24 | time: 54.79s | validation loss  3.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 155.00 | loss  3.56 |
| epoch  25 |   200/  475 batches | ms/batch 152.02 | loss  3.47 |
| epoch  25 |   300/  475 batches | ms/batch 147.94 | loss  3.35 |
| epoch  25 |   400/  475 batches | ms/batch 144.60 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 68.53s | training loss  3.71 |
    | end of validation epoch  25 | time: 53.77s | validation loss  3.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 162.01 | loss  3.28 |
| epoch  26 |   200/  475 batches | ms/batch 152.38 | loss  3.49 |
| epoch  26 |   300/  475 batches | ms/batch 147.71 | loss  4.10 |
| epoch  26 |   400/  475 batches | ms/batch 144.66 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 68.72s | training loss  3.69 |
    | end of validation epoch  26 | time: 54.59s | validation loss  3.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 169.93 | loss  3.73 |
| epoch  27 |   200/  475 batches | ms/batch 151.18 | loss  3.57 |
| epoch  27 |   300/  475 batches | ms/batch 145.62 | loss  3.81 |
| epoch  27 |   400/  475 batches | ms/batch 143.75 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 68.15s | training loss  3.67 |
    | end of validation epoch  27 | time: 54.32s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 157.88 | loss  3.65 |
| epoch  28 |   200/  475 batches | ms/batch 150.32 | loss  3.47 |
| epoch  28 |   300/  475 batches | ms/batch 145.63 | loss  3.43 |
| epoch  28 |   400/  475 batches | ms/batch 143.01 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 67.78s | training loss  3.65 |
    | end of validation epoch  28 | time: 53.03s | validation loss  2.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 162.80 | loss  3.46 |
| epoch  29 |   200/  475 batches | ms/batch 150.02 | loss  3.72 |
| epoch  29 |   300/  475 batches | ms/batch 145.02 | loss  4.07 |
| epoch  29 |   400/  475 batches | ms/batch 143.46 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 68.04s | training loss  3.63 |
    | end of validation epoch  29 | time: 53.81s | validation loss  2.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 164.50 | loss  3.89 |
| epoch  30 |   200/  475 batches | ms/batch 150.61 | loss  3.43 |
| epoch  30 |   300/  475 batches | ms/batch 146.80 | loss  3.33 |
| epoch  30 |   400/  475 batches | ms/batch 144.20 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 68.34s | training loss  3.60 |
    | end of validation epoch  30 | time: 54.53s | validation loss  2.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 162.94 | loss  3.66 |
| epoch  31 |   200/  475 batches | ms/batch 149.85 | loss  3.65 |
| epoch  31 |   300/  475 batches | ms/batch 145.38 | loss  3.75 |
| epoch  31 |   400/  475 batches | ms/batch 142.11 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 67.47s | training loss  3.60 |
    | end of validation epoch  31 | time: 54.23s | validation loss  3.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 166.02 | loss  3.16 |
| epoch  32 |   200/  475 batches | ms/batch 152.19 | loss  3.46 |
| epoch  32 |   300/  475 batches | ms/batch 147.72 | loss  3.63 |
| epoch  32 |   400/  475 batches | ms/batch 144.82 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 68.72s | training loss  3.57 |
    | end of validation epoch  32 | time: 55.06s | validation loss  2.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 163.95 | loss  3.44 |
| epoch  33 |   200/  475 batches | ms/batch 151.43 | loss  3.70 |
| epoch  33 |   300/  475 batches | ms/batch 145.69 | loss  3.66 |
| epoch  33 |   400/  475 batches | ms/batch 143.28 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 67.94s | training loss  3.57 |
    | end of validation epoch  33 | time: 54.87s | validation loss  2.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 163.92 | loss  3.83 |
| epoch  34 |   200/  475 batches | ms/batch 150.41 | loss  3.69 |
| epoch  34 |   300/  475 batches | ms/batch 145.72 | loss  3.38 |
| epoch  34 |   400/  475 batches | ms/batch 143.17 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 67.99s | training loss  3.54 |
    | end of validation epoch  34 | time: 54.83s | validation loss  2.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 165.68 | loss  3.67 |
| epoch  35 |   200/  475 batches | ms/batch 151.99 | loss  3.73 |
| epoch  35 |   300/  475 batches | ms/batch 146.68 | loss  3.66 |
| epoch  35 |   400/  475 batches | ms/batch 144.47 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 68.30s | training loss  3.52 |
    | end of validation epoch  35 | time: 54.83s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 168.89 | loss  3.76 |
| epoch  36 |   200/  475 batches | ms/batch 152.95 | loss  3.53 |
| epoch  36 |   300/  475 batches | ms/batch 147.07 | loss  3.94 |
| epoch  36 |   400/  475 batches | ms/batch 144.41 | loss  3.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 68.27s | training loss  3.52 |
    | end of validation epoch  36 | time: 53.81s | validation loss  2.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 166.79 | loss  3.42 |
| epoch  37 |   200/  475 batches | ms/batch 151.30 | loss  3.49 |
| epoch  37 |   300/  475 batches | ms/batch 145.67 | loss  3.51 |
| epoch  37 |   400/  475 batches | ms/batch 144.15 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 68.53s | training loss  3.51 |
    | end of validation epoch  37 | time: 54.48s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 168.60 | loss  3.14 |
| epoch  38 |   200/  475 batches | ms/batch 150.81 | loss  3.54 |
| epoch  38 |   300/  475 batches | ms/batch 146.44 | loss  3.25 |
| epoch  38 |   400/  475 batches | ms/batch 143.04 | loss  3.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 68.02s | training loss  3.49 |
    | end of validation epoch  38 | time: 55.56s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 167.76 | loss  3.42 |
| epoch  39 |   200/  475 batches | ms/batch 151.33 | loss  3.34 |
| epoch  39 |   300/  475 batches | ms/batch 146.11 | loss  3.78 |
| epoch  39 |   400/  475 batches | ms/batch 144.26 | loss  3.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 68.55s | training loss  3.46 |
    | end of validation epoch  39 | time: 54.74s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 163.32 | loss  3.15 |
| epoch  40 |   200/  475 batches | ms/batch 150.94 | loss  3.24 |
| epoch  40 |   300/  475 batches | ms/batch 146.88 | loss  3.42 |
| epoch  40 |   400/  475 batches | ms/batch 144.22 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 68.31s | training loss  3.46 |
    | end of validation epoch  40 | time: 55.03s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 161.10 | loss  3.90 |
| epoch  41 |   200/  475 batches | ms/batch 149.58 | loss  3.42 |
| epoch  41 |   300/  475 batches | ms/batch 145.63 | loss  3.21 |
| epoch  41 |   400/  475 batches | ms/batch 142.84 | loss  3.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 68.09s | training loss  3.45 |
    | end of validation epoch  41 | time: 54.82s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 163.49 | loss  3.40 |
| epoch  42 |   200/  475 batches | ms/batch 149.89 | loss  3.37 |
| epoch  42 |   300/  475 batches | ms/batch 145.60 | loss  3.48 |
| epoch  42 |   400/  475 batches | ms/batch 143.43 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 68.04s | training loss  3.44 |
    | end of validation epoch  42 | time: 54.45s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 166.60 | loss  3.24 |
| epoch  43 |   200/  475 batches | ms/batch 151.92 | loss  3.53 |
| epoch  43 |   300/  475 batches | ms/batch 146.63 | loss  3.22 |
| epoch  43 |   400/  475 batches | ms/batch 144.52 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 68.40s | training loss  3.42 |
    | end of validation epoch  43 | time: 54.96s | validation loss  2.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 164.35 | loss  3.07 |
| epoch  44 |   200/  475 batches | ms/batch 149.27 | loss  3.51 |
| epoch  44 |   300/  475 batches | ms/batch 146.50 | loss  3.70 |
| epoch  44 |   400/  475 batches | ms/batch 144.83 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 68.11s | training loss  3.41 |
    | end of validation epoch  44 | time: 54.17s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 161.40 | loss  3.65 |
| epoch  45 |   200/  475 batches | ms/batch 149.88 | loss  3.26 |
| epoch  45 |   300/  475 batches | ms/batch 145.51 | loss  3.47 |
| epoch  45 |   400/  475 batches | ms/batch 143.28 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 68.12s | training loss  3.40 |
    | end of validation epoch  45 | time: 52.71s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 164.82 | loss  3.49 |
| epoch  46 |   200/  475 batches | ms/batch 151.70 | loss  3.49 |
| epoch  46 |   300/  475 batches | ms/batch 146.64 | loss  3.51 |
| epoch  46 |   400/  475 batches | ms/batch 144.41 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 68.61s | training loss  3.39 |
    | end of validation epoch  46 | time: 54.28s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 164.43 | loss  3.52 |
| epoch  47 |   200/  475 batches | ms/batch 151.82 | loss  3.44 |
| epoch  47 |   300/  475 batches | ms/batch 147.57 | loss  3.44 |
| epoch  47 |   400/  475 batches | ms/batch 145.44 | loss  3.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 68.88s | training loss  3.39 |
    | end of validation epoch  47 | time: 53.00s | validation loss  2.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 160.86 | loss  3.56 |
| epoch  48 |   200/  475 batches | ms/batch 151.02 | loss  3.58 |
| epoch  48 |   300/  475 batches | ms/batch 146.26 | loss  3.39 |
| epoch  48 |   400/  475 batches | ms/batch 144.17 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 68.45s | training loss  3.37 |
    | end of validation epoch  48 | time: 54.03s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 164.30 | loss  3.90 |
| epoch  49 |   200/  475 batches | ms/batch 151.11 | loss  3.41 |
| epoch  49 |   300/  475 batches | ms/batch 146.93 | loss  3.33 |
| epoch  49 |   400/  475 batches | ms/batch 144.60 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 68.66s | training loss  3.35 |
    | end of validation epoch  49 | time: 54.89s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 162.47 | loss  3.69 |
| epoch  50 |   200/  475 batches | ms/batch 149.35 | loss  3.29 |
| epoch  50 |   300/  475 batches | ms/batch 145.58 | loss  3.23 |
| epoch  50 |   400/  475 batches | ms/batch 143.79 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 68.28s | training loss  3.33 |
    | end of validation epoch  50 | time: 54.49s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 164.61 | loss  3.60 |
| epoch  51 |   200/  475 batches | ms/batch 151.41 | loss  3.18 |
| epoch  51 |   300/  475 batches | ms/batch 146.11 | loss  3.39 |
| epoch  51 |   400/  475 batches | ms/batch 144.36 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 68.32s | training loss  3.32 |
    | end of validation epoch  51 | time: 53.67s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 169.38 | loss  3.38 |
| epoch  52 |   200/  475 batches | ms/batch 153.13 | loss  3.50 |
| epoch  52 |   300/  475 batches | ms/batch 147.86 | loss  3.04 |
| epoch  52 |   400/  475 batches | ms/batch 145.10 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 69.00s | training loss  3.32 |
    | end of validation epoch  52 | time: 53.79s | validation loss  2.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 163.56 | loss  3.21 |
| epoch  53 |   200/  475 batches | ms/batch 150.36 | loss  3.19 |
| epoch  53 |   300/  475 batches | ms/batch 146.38 | loss  3.79 |
| epoch  53 |   400/  475 batches | ms/batch 144.81 | loss  3.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 68.25s | training loss  3.31 |
    | end of validation epoch  53 | time: 53.38s | validation loss  2.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 165.69 | loss  3.38 |
| epoch  54 |   200/  475 batches | ms/batch 151.52 | loss  3.30 |
| epoch  54 |   300/  475 batches | ms/batch 146.16 | loss  3.33 |
| epoch  54 |   400/  475 batches | ms/batch 144.35 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 68.53s | training loss  3.32 |
    | end of validation epoch  54 | time: 53.36s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 164.19 | loss  3.43 |
| epoch  55 |   200/  475 batches | ms/batch 150.46 | loss  2.98 |
| epoch  55 |   300/  475 batches | ms/batch 145.98 | loss  3.10 |
| epoch  55 |   400/  475 batches | ms/batch 144.16 | loss  3.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 68.29s | training loss  3.29 |
    | end of validation epoch  55 | time: 53.49s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 167.31 | loss  3.28 |
| epoch  56 |   200/  475 batches | ms/batch 152.08 | loss  3.29 |
| epoch  56 |   300/  475 batches | ms/batch 147.73 | loss  3.20 |
| epoch  56 |   400/  475 batches | ms/batch 144.56 | loss  3.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 68.28s | training loss  3.30 |
    | end of validation epoch  56 | time: 55.27s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 164.19 | loss  3.47 |
| epoch  57 |   200/  475 batches | ms/batch 150.63 | loss  3.57 |
| epoch  57 |   300/  475 batches | ms/batch 146.46 | loss  3.30 |
| epoch  57 |   400/  475 batches | ms/batch 144.78 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 69.03s | training loss  3.29 |
    | end of validation epoch  57 | time: 55.36s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417]
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 162.35 | loss  3.41 |
| epoch  58 |   200/  475 batches | ms/batch 149.37 | loss  3.21 |
| epoch  58 |   300/  475 batches | ms/batch 147.27 | loss  3.06 |
| epoch  58 |   400/  475 batches | ms/batch 144.89 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 69.10s | training loss  3.28 |
    | end of validation epoch  58 | time: 55.05s | validation loss  2.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 168.39 | loss  2.90 |
| epoch  59 |   200/  475 batches | ms/batch 153.97 | loss  3.05 |
| epoch  59 |   300/  475 batches | ms/batch 147.54 | loss  3.11 |
| epoch  59 |   400/  475 batches | ms/batch 145.27 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 68.80s | training loss  3.26 |
    | end of validation epoch  59 | time: 54.41s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 166.20 | loss  3.12 |
| epoch  60 |   200/  475 batches | ms/batch 151.00 | loss  3.30 |
| epoch  60 |   300/  475 batches | ms/batch 146.63 | loss  3.25 |
| epoch  60 |   400/  475 batches | ms/batch 144.24 | loss  3.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 68.45s | training loss  3.24 |
    | end of validation epoch  60 | time: 54.46s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 155.19 | loss  2.95 |
| epoch  61 |   200/  475 batches | ms/batch 152.67 | loss  3.20 |
| epoch  61 |   300/  475 batches | ms/batch 147.47 | loss  2.87 |
| epoch  61 |   400/  475 batches | ms/batch 144.27 | loss  3.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 68.63s | training loss  3.24 |
    | end of validation epoch  61 | time: 54.59s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 165.01 | loss  3.33 |
| epoch  62 |   200/  475 batches | ms/batch 151.10 | loss  3.35 |
| epoch  62 |   300/  475 batches | ms/batch 147.09 | loss  3.04 |
| epoch  62 |   400/  475 batches | ms/batch 143.78 | loss  3.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 68.43s | training loss  3.23 |
    | end of validation epoch  62 | time: 55.06s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 163.60 | loss  3.12 |
| epoch  63 |   200/  475 batches | ms/batch 151.56 | loss  3.42 |
| epoch  63 |   300/  475 batches | ms/batch 147.24 | loss  3.26 |
| epoch  63 |   400/  475 batches | ms/batch 145.40 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 69.02s | training loss  3.22 |
    | end of validation epoch  63 | time: 55.82s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 160.57 | loss  2.91 |
| epoch  64 |   200/  475 batches | ms/batch 150.07 | loss  3.08 |
| epoch  64 |   300/  475 batches | ms/batch 147.19 | loss  3.64 |
| epoch  64 |   400/  475 batches | ms/batch 144.68 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 68.42s | training loss  3.22 |
    | end of validation epoch  64 | time: 55.07s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 161.71 | loss  3.13 |
| epoch  65 |   200/  475 batches | ms/batch 150.92 | loss  2.94 |
| epoch  65 |   300/  475 batches | ms/batch 147.11 | loss  3.24 |
| epoch  65 |   400/  475 batches | ms/batch 144.65 | loss  2.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 68.58s | training loss  3.21 |
    | end of validation epoch  65 | time: 54.46s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 169.63 | loss  3.44 |
| epoch  66 |   200/  475 batches | ms/batch 151.15 | loss  3.35 |
| epoch  66 |   300/  475 batches | ms/batch 147.51 | loss  2.96 |
| epoch  66 |   400/  475 batches | ms/batch 144.14 | loss  2.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 68.57s | training loss  3.21 |
    | end of validation epoch  66 | time: 53.01s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 162.89 | loss  3.02 |
| epoch  67 |   200/  475 batches | ms/batch 149.29 | loss  3.04 |
| epoch  67 |   300/  475 batches | ms/batch 145.09 | loss  3.00 |
| epoch  67 |   400/  475 batches | ms/batch 143.50 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 67.95s | training loss  3.20 |
    | end of validation epoch  67 | time: 52.29s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 166.07 | loss  3.44 |
| epoch  68 |   200/  475 batches | ms/batch 152.39 | loss  3.35 |
| epoch  68 |   300/  475 batches | ms/batch 147.33 | loss  3.59 |
| epoch  68 |   400/  475 batches | ms/batch 144.98 | loss  3.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 68.49s | training loss  3.17 |
    | end of validation epoch  68 | time: 54.56s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 164.85 | loss  3.08 |
| epoch  69 |   200/  475 batches | ms/batch 150.88 | loss  2.92 |
| epoch  69 |   300/  475 batches | ms/batch 146.37 | loss  2.90 |
| epoch  69 |   400/  475 batches | ms/batch 144.57 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 68.89s | training loss  3.19 |
    | end of validation epoch  69 | time: 54.81s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 162.51 | loss  3.24 |
| epoch  70 |   200/  475 batches | ms/batch 149.25 | loss  2.87 |
| epoch  70 |   300/  475 batches | ms/batch 146.54 | loss  3.06 |
| epoch  70 |   400/  475 batches | ms/batch 144.36 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 68.41s | training loss  3.18 |
    | end of validation epoch  70 | time: 55.46s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 166.64 | loss  3.09 |
| epoch  71 |   200/  475 batches | ms/batch 150.23 | loss  3.06 |
| epoch  71 |   300/  475 batches | ms/batch 146.09 | loss  3.07 |
| epoch  71 |   400/  475 batches | ms/batch 143.67 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 68.24s | training loss  3.17 |
    | end of validation epoch  71 | time: 54.91s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 165.49 | loss  3.18 |
| epoch  72 |   200/  475 batches | ms/batch 151.77 | loss  3.28 |
| epoch  72 |   300/  475 batches | ms/batch 146.76 | loss  3.26 |
| epoch  72 |   400/  475 batches | ms/batch 144.07 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 68.51s | training loss  3.15 |
    | end of validation epoch  72 | time: 53.72s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 169.14 | loss  3.19 |
| epoch  73 |   200/  475 batches | ms/batch 152.24 | loss  3.03 |
| epoch  73 |   300/  475 batches | ms/batch 146.70 | loss  3.30 |
| epoch  73 |   400/  475 batches | ms/batch 144.24 | loss  2.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 68.35s | training loss  3.16 |
    | end of validation epoch  73 | time: 55.24s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 163.00 | loss  3.56 |
| epoch  74 |   200/  475 batches | ms/batch 152.82 | loss  2.94 |
| epoch  74 |   300/  475 batches | ms/batch 146.71 | loss  3.58 |
| epoch  74 |   400/  475 batches | ms/batch 144.37 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 68.67s | training loss  3.16 |
    | end of validation epoch  74 | time: 55.86s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 162.41 | loss  3.27 |
| epoch  75 |   200/  475 batches | ms/batch 150.83 | loss  3.22 |
| epoch  75 |   300/  475 batches | ms/batch 145.57 | loss  2.80 |
| epoch  75 |   400/  475 batches | ms/batch 142.86 | loss  2.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 67.77s | training loss  3.12 |
    | end of validation epoch  75 | time: 55.24s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 162.98 | loss  3.15 |
| epoch  76 |   200/  475 batches | ms/batch 147.86 | loss  2.89 |
| epoch  76 |   300/  475 batches | ms/batch 144.87 | loss  3.65 |
| epoch  76 |   400/  475 batches | ms/batch 142.97 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 67.62s | training loss  3.13 |
    | end of validation epoch  76 | time: 55.48s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 159.07 | loss  3.44 |
| epoch  77 |   200/  475 batches | ms/batch 149.06 | loss  3.11 |
| epoch  77 |   300/  475 batches | ms/batch 145.55 | loss  2.61 |
| epoch  77 |   400/  475 batches | ms/batch 144.20 | loss  3.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 68.21s | training loss  3.12 |
    | end of validation epoch  77 | time: 53.92s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 167.45 | loss  3.13 |
| epoch  78 |   200/  475 batches | ms/batch 152.65 | loss  3.20 |
| epoch  78 |   300/  475 batches | ms/batch 147.66 | loss  2.79 |
| epoch  78 |   400/  475 batches | ms/batch 144.94 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 68.51s | training loss  3.11 |
    | end of validation epoch  78 | time: 55.01s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 161.67 | loss  2.87 |
| epoch  79 |   200/  475 batches | ms/batch 149.05 | loss  3.18 |
| epoch  79 |   300/  475 batches | ms/batch 144.87 | loss  2.94 |
| epoch  79 |   400/  475 batches | ms/batch 144.03 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 68.24s | training loss  3.09 |
    | end of validation epoch  79 | time: 55.40s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 177.49 | loss  3.22 |
| epoch  80 |   200/  475 batches | ms/batch 157.78 | loss  3.32 |
| epoch  80 |   300/  475 batches | ms/batch 153.18 | loss  3.13 |
| epoch  80 |   400/  475 batches | ms/batch 149.57 | loss  2.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 70.68s | training loss  3.10 |
    | end of validation epoch  80 | time: 56.17s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 169.76 | loss  2.85 |
| epoch  81 |   200/  475 batches | ms/batch 153.41 | loss  2.58 |
| epoch  81 |   300/  475 batches | ms/batch 147.61 | loss  3.21 |
| epoch  81 |   400/  475 batches | ms/batch 144.77 | loss  2.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 68.50s | training loss  3.09 |
    | end of validation epoch  81 | time: 53.66s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 162.87 | loss  2.83 |
| epoch  82 |   200/  475 batches | ms/batch 151.69 | loss  2.97 |
| epoch  82 |   300/  475 batches | ms/batch 147.06 | loss  3.33 |
| epoch  82 |   400/  475 batches | ms/batch 145.49 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 68.72s | training loss  3.09 |
    | end of validation epoch  82 | time: 54.73s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 163.98 | loss  2.94 |
| epoch  83 |   200/  475 batches | ms/batch 150.78 | loss  3.27 |
| epoch  83 |   300/  475 batches | ms/batch 145.93 | loss  2.89 |
| epoch  83 |   400/  475 batches | ms/batch 143.79 | loss  2.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 68.44s | training loss  3.08 |
    | end of validation epoch  83 | time: 55.37s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 164.82 | loss  3.04 |
| epoch  84 |   200/  475 batches | ms/batch 151.49 | loss  2.92 |
| epoch  84 |   300/  475 batches | ms/batch 146.36 | loss  3.68 |
| epoch  84 |   400/  475 batches | ms/batch 143.95 | loss  2.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 68.60s | training loss  3.06 |
    | end of validation epoch  84 | time: 55.31s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 159.76 | loss  2.79 |
| epoch  85 |   200/  475 batches | ms/batch 150.43 | loss  3.04 |
| epoch  85 |   300/  475 batches | ms/batch 146.40 | loss  3.25 |
| epoch  85 |   400/  475 batches | ms/batch 144.12 | loss  3.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 68.62s | training loss  3.09 |
    | end of validation epoch  85 | time: 54.44s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 165.85 | loss  2.79 |
| epoch  86 |   200/  475 batches | ms/batch 152.71 | loss  3.02 |
| epoch  86 |   300/  475 batches | ms/batch 146.77 | loss  3.63 |
| epoch  86 |   400/  475 batches | ms/batch 144.13 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 68.32s | training loss  3.08 |
    | end of validation epoch  86 | time: 55.07s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 161.08 | loss  3.38 |
| epoch  87 |   200/  475 batches | ms/batch 150.87 | loss  3.13 |
| epoch  87 |   300/  475 batches | ms/batch 146.21 | loss  3.13 |
| epoch  87 |   400/  475 batches | ms/batch 144.25 | loss  2.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 68.22s | training loss  3.04 |
    | end of validation epoch  87 | time: 54.89s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 166.86 | loss  2.86 |
| epoch  88 |   200/  475 batches | ms/batch 151.60 | loss  3.18 |
| epoch  88 |   300/  475 batches | ms/batch 147.55 | loss  3.03 |
| epoch  88 |   400/  475 batches | ms/batch 145.02 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 68.69s | training loss  3.04 |
    | end of validation epoch  88 | time: 54.45s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 162.45 | loss  3.05 |
| epoch  89 |   200/  475 batches | ms/batch 149.37 | loss  3.39 |
| epoch  89 |   300/  475 batches | ms/batch 146.27 | loss  3.20 |
| epoch  89 |   400/  475 batches | ms/batch 144.60 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 68.55s | training loss  3.06 |
    | end of validation epoch  89 | time: 54.88s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 166.32 | loss  3.21 |
| epoch  90 |   200/  475 batches | ms/batch 152.38 | loss  3.07 |
| epoch  90 |   300/  475 batches | ms/batch 147.17 | loss  2.84 |
| epoch  90 |   400/  475 batches | ms/batch 145.20 | loss  2.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 68.75s | training loss  3.04 |
    | end of validation epoch  90 | time: 55.32s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 166.14 | loss  3.10 |
| epoch  91 |   200/  475 batches | ms/batch 152.67 | loss  2.88 |
| epoch  91 |   300/  475 batches | ms/batch 147.24 | loss  3.36 |
| epoch  91 |   400/  475 batches | ms/batch 144.77 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 68.39s | training loss  3.03 |
    | end of validation epoch  91 | time: 54.83s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 165.34 | loss  3.02 |
| epoch  92 |   200/  475 batches | ms/batch 150.17 | loss  2.72 |
| epoch  92 |   300/  475 batches | ms/batch 145.32 | loss  3.27 |
| epoch  92 |   400/  475 batches | ms/batch 143.18 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 67.84s | training loss  3.03 |
    | end of validation epoch  92 | time: 55.11s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 160.36 | loss  2.94 |
| epoch  93 |   200/  475 batches | ms/batch 148.16 | loss  3.03 |
| epoch  93 |   300/  475 batches | ms/batch 145.82 | loss  3.11 |
| epoch  93 |   400/  475 batches | ms/batch 144.34 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 68.40s | training loss  3.00 |
    | end of validation epoch  93 | time: 55.21s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 163.06 | loss  2.85 |
| epoch  94 |   200/  475 batches | ms/batch 149.60 | loss  3.37 |
| epoch  94 |   300/  475 batches | ms/batch 145.27 | loss  2.71 |
| epoch  94 |   400/  475 batches | ms/batch 143.74 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 68.49s | training loss  3.01 |
    | end of validation epoch  94 | time: 54.09s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 160.45 | loss  2.59 |
| epoch  95 |   200/  475 batches | ms/batch 151.58 | loss  2.96 |
| epoch  95 |   300/  475 batches | ms/batch 145.96 | loss  2.84 |
| epoch  95 |   400/  475 batches | ms/batch 143.72 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 68.23s | training loss  3.01 |
    | end of validation epoch  95 | time: 53.29s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204, 3.014015201769377] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896, 2.5697182086335513]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 163.57 | loss  2.71 |
| epoch  96 |   200/  475 batches | ms/batch 150.99 | loss  2.94 |
| epoch  96 |   300/  475 batches | ms/batch 146.06 | loss  3.12 |
| epoch  96 |   400/  475 batches | ms/batch 144.21 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 68.30s | training loss  3.00 |
    | end of validation epoch  96 | time: 54.82s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204, 3.014015201769377, 2.9951614676023786] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896, 2.5697182086335513, 2.5491188263692774]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 168.24 | loss  3.01 |
| epoch  97 |   200/  475 batches | ms/batch 151.15 | loss  3.05 |
| epoch  97 |   300/  475 batches | ms/batch 146.23 | loss  3.15 |
| epoch  97 |   400/  475 batches | ms/batch 144.68 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 68.49s | training loss  3.00 |
    | end of validation epoch  97 | time: 54.33s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204, 3.014015201769377, 2.9951614676023786, 3.0021752884513453] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896, 2.5697182086335513, 2.5491188263692774, 2.495121450985179]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 171.64 | loss  2.97 |
| epoch  98 |   200/  475 batches | ms/batch 153.98 | loss  2.63 |
| epoch  98 |   300/  475 batches | ms/batch 148.04 | loss  3.31 |
| epoch  98 |   400/  475 batches | ms/batch 145.76 | loss  2.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 68.99s | training loss  3.00 |
    | end of validation epoch  98 | time: 52.68s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204, 3.014015201769377, 2.9951614676023786, 3.0021752884513453, 2.9975025202098644] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896, 2.5697182086335513, 2.5491188263692774, 2.495121450985179, 2.56770946899382]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 159.82 | loss  2.15 |
| epoch  99 |   200/  475 batches | ms/batch 149.74 | loss  2.65 |
| epoch  99 |   300/  475 batches | ms/batch 145.19 | loss  2.80 |
| epoch  99 |   400/  475 batches | ms/batch 143.75 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 68.17s | training loss  2.98 |
    | end of validation epoch  99 | time: 53.31s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.852484442058363, 5.433862408085873, 5.198899095434892, 4.98144156506187, 4.818589479546798, 4.685723504518208, 4.561504779112966, 4.460172325937371, 4.373681862981696, 4.304370521746184, 4.23243482238368, 4.175439002890336, 4.132651391280325, 4.067358006427162, 4.031447807111238, 3.9791028323926425, 3.935112587276258, 3.92039245505082, 3.875527286529541, 3.8498047778480933, 3.8069731737438, 3.8031004609559713, 3.7625253305937116, 3.74699643034684, 3.711488782983077, 3.692312953346654, 3.665537090301514, 3.6475347905409965, 3.6272565906926206, 3.6017699628127247, 3.600781897996601, 3.5700193746466384, 3.5660061680643182, 3.538404631865652, 3.5214553993626643, 3.5198878248114336, 3.512617943914313, 3.491143788287514, 3.4617214880491556, 3.4558360536474932, 3.451132159985994, 3.4381721561833434, 3.421862661462081, 3.4126688214352257, 3.4037129939229867, 3.393004254290932, 3.3888408957029643, 3.366829585025185, 3.345239415921663, 3.329647522976524, 3.324188007053576, 3.3154054119712426, 3.305271491502461, 3.3167758243962338, 3.285377305683337, 3.3012376464040654, 3.2868594937575493, 3.277104747169896, 3.261409630022551, 3.2358324773688065, 3.2440189105586, 3.226838384929456, 3.2246533725136204, 3.2163254973762916, 3.209664787493254, 3.211072070473119, 3.1974088046425266, 3.1717687722256307, 3.1900030517578126, 3.176048984025654, 3.165615977739033, 3.1519914170315393, 3.1611364193966516, 3.1563167105223005, 3.120303159011038, 3.128993542821784, 3.120622137973183, 3.1077416921916763, 3.091920596674869, 3.09809390168441, 3.085536057321649, 3.0856158010583177, 3.0782114636270625, 3.0618490334560997, 3.0868813840966474, 3.077708476719103, 3.0361051750183106, 3.0391200768320186, 3.0558333080693294, 3.040338975504825, 3.029414620650442, 3.0312508191560443, 3.0023653150859633, 3.0057345114256204, 3.014015201769377, 2.9951614676023786, 3.0021752884513453, 2.9975025202098644, 2.9815500530443693] validation loss is  [5.449494638362853, 5.066381009686895, 4.755702323272448, 4.512546683559899, 4.279979409289961, 4.1277865081274205, 3.9854962445106827, 3.890720513688416, 3.8182809553226504, 3.6780543948421958, 3.574906986300685, 3.6504000535532204, 3.4831619202589787, 3.421963509391336, 3.3758029997849666, 3.3243627648393645, 3.337538102093865, 3.262475426457509, 3.209409980212941, 3.1710875074402627, 3.2041588510785783, 3.1329949984029564, 3.081602240810875, 3.0917634222687793, 3.068841803975466, 3.055380817220992, 3.01782664731771, 2.988032112602426, 2.9889069024254296, 2.953298963418528, 2.9967683723994663, 2.966843783354559, 2.9189497603087866, 2.903197923628222, 2.871675317026988, 2.8801030772072926, 2.8478854664233553, 2.867898376048112, 2.8406442774444067, 2.840232268101027, 2.844786403559837, 2.821022288138125, 2.8280021242734765, 2.8088826472017945, 2.8079464435577393, 2.767728725401293, 2.7938507404647956, 2.7499467785618887, 2.7475890592366707, 2.7495841038327256, 2.7192114180877427, 2.7640430486502767, 2.7924306633091773, 2.7231349584435214, 2.6946673633671607, 2.6907776423863004, 2.7254105085084417, 2.706429028711399, 2.6888424789204315, 2.682159996834122, 2.673841703839663, 2.664790696456653, 2.6749756466440795, 2.6727255522703923, 2.644738732265825, 2.6262106795270905, 2.6504885980061124, 2.650224091626015, 2.629314444646114, 2.6111063466352573, 2.6350818611994513, 2.6250692515814005, 2.6305441175188338, 2.619095211269475, 2.6996225689639566, 2.6235205345794936, 2.568394960475569, 2.612659113747733, 2.5931349652154103, 2.584467478159095, 2.6071426658069385, 2.5826244464441506, 2.61958760073205, 2.602611330376954, 2.6131177920253337, 2.594138445974398, 2.5745369506483318, 2.5574295661028694, 2.5962205003289616, 2.557100283999403, 2.5926838632391283, 2.550539978412019, 2.582320766288693, 2.5731654006893896, 2.5697182086335513, 2.5491188263692774, 2.495121450985179, 2.56770946899382, 2.5161761676563934]
 Best training model found.
---------------------------------------------------------------------------------------------------
