/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/super_hard_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 160.09 | loss  7.04 |
| epoch   1 |   200/  475 batches | ms/batch 149.74 | loss  6.69 |
| epoch   1 |   300/  475 batches | ms/batch 145.82 | loss  6.96 |
| epoch   1 |   400/  475 batches | ms/batch 144.55 | loss  6.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 68.56s | training loss  6.91 |
    | end of validation epoch   1 | time: 53.80s | validation loss  5.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.910239618201005] validation loss is  [5.733729566846575]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 161.41 | loss  6.86 |
| epoch   2 |   200/  475 batches | ms/batch 149.56 | loss  6.46 |
| epoch   2 |   300/  475 batches | ms/batch 145.31 | loss  6.44 |
| epoch   2 |   400/  475 batches | ms/batch 143.28 | loss  6.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 68.02s | training loss  6.60 |
    | end of validation epoch   2 | time: 52.43s | validation loss  5.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.910239618201005, 6.596164766612806] validation loss is  [5.733729566846575, 5.607950939851649]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 155.00 | loss  6.07 |
| epoch   3 |   200/  475 batches | ms/batch 147.72 | loss  6.43 |
| epoch   3 |   300/  475 batches | ms/batch 143.67 | loss  6.11 |
| epoch   3 |   400/  475 batches | ms/batch 141.16 | loss  6.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 66.78s | training loss  6.40 |
    | end of validation epoch   3 | time: 52.39s | validation loss  5.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 159.53 | loss  6.84 |
| epoch   4 |   200/  475 batches | ms/batch 147.26 | loss  6.27 |
| epoch   4 |   300/  475 batches | ms/batch 143.36 | loss  6.07 |
| epoch   4 |   400/  475 batches | ms/batch 141.21 | loss  6.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 67.13s | training loss  6.23 |
    | end of validation epoch   4 | time: 52.42s | validation loss  5.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 160.18 | loss  5.85 |
| epoch   5 |   200/  475 batches | ms/batch 148.77 | loss  6.19 |
| epoch   5 |   300/  475 batches | ms/batch 143.26 | loss  6.29 |
| epoch   5 |   400/  475 batches | ms/batch 141.09 | loss  5.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 66.52s | training loss  6.09 |
    | end of validation epoch   5 | time: 53.86s | validation loss  5.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 149.19 | loss  5.91 |
| epoch   6 |   200/  475 batches | ms/batch 144.50 | loss  6.02 |
| epoch   6 |   300/  475 batches | ms/batch 142.14 | loss  6.24 |
| epoch   6 |   400/  475 batches | ms/batch 140.79 | loss  5.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 66.67s | training loss  5.98 |
    | end of validation epoch   6 | time: 53.11s | validation loss  5.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 162.66 | loss  5.89 |
| epoch   7 |   200/  475 batches | ms/batch 147.52 | loss  5.93 |
| epoch   7 |   300/  475 batches | ms/batch 142.04 | loss  5.86 |
| epoch   7 |   400/  475 batches | ms/batch 140.32 | loss  5.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 66.47s | training loss  5.88 |
    | end of validation epoch   7 | time: 52.15s | validation loss  5.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 160.77 | loss  5.46 |
| epoch   8 |   200/  475 batches | ms/batch 145.42 | loss  5.76 |
| epoch   8 |   300/  475 batches | ms/batch 141.83 | loss  5.80 |
| epoch   8 |   400/  475 batches | ms/batch 140.47 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 66.47s | training loss  5.79 |
    | end of validation epoch   8 | time: 52.59s | validation loss  5.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 161.30 | loss  5.57 |
| epoch   9 |   200/  475 batches | ms/batch 146.98 | loss  5.72 |
| epoch   9 |   300/  475 batches | ms/batch 143.10 | loss  5.60 |
| epoch   9 |   400/  475 batches | ms/batch 141.19 | loss  5.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 66.87s | training loss  5.71 |
    | end of validation epoch   9 | time: 53.17s | validation loss  5.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 155.33 | loss  5.34 |
| epoch  10 |   200/  475 batches | ms/batch 144.27 | loss  5.61 |
| epoch  10 |   300/  475 batches | ms/batch 140.57 | loss  5.62 |
| epoch  10 |   400/  475 batches | ms/batch 139.28 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 66.08s | training loss  5.64 |
    | end of validation epoch  10 | time: 52.52s | validation loss  5.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 161.84 | loss  5.61 |
| epoch  11 |   200/  475 batches | ms/batch 147.38 | loss  5.41 |
| epoch  11 |   300/  475 batches | ms/batch 142.48 | loss  5.25 |
| epoch  11 |   400/  475 batches | ms/batch 139.48 | loss  5.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 66.33s | training loss  5.58 |
    | end of validation epoch  11 | time: 52.86s | validation loss  5.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 163.11 | loss  5.27 |
| epoch  12 |   200/  475 batches | ms/batch 147.74 | loss  5.65 |
| epoch  12 |   300/  475 batches | ms/batch 142.93 | loss  5.73 |
| epoch  12 |   400/  475 batches | ms/batch 140.84 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 66.84s | training loss  5.52 |
    | end of validation epoch  12 | time: 52.43s | validation loss  5.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 162.40 | loss  5.73 |
| epoch  13 |   200/  475 batches | ms/batch 147.22 | loss  5.44 |
| epoch  13 |   300/  475 batches | ms/batch 142.78 | loss  5.70 |
| epoch  13 |   400/  475 batches | ms/batch 139.58 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 66.34s | training loss  5.47 |
    | end of validation epoch  13 | time: 53.12s | validation loss  5.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 159.46 | loss  5.41 |
| epoch  14 |   200/  475 batches | ms/batch 146.66 | loss  5.17 |
| epoch  14 |   300/  475 batches | ms/batch 141.41 | loss  5.52 |
| epoch  14 |   400/  475 batches | ms/batch 139.23 | loss  5.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 66.00s | training loss  5.44 |
    | end of validation epoch  14 | time: 53.44s | validation loss  4.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 159.06 | loss  5.27 |
| epoch  15 |   200/  475 batches | ms/batch 147.11 | loss  5.43 |
| epoch  15 |   300/  475 batches | ms/batch 141.42 | loss  5.59 |
| epoch  15 |   400/  475 batches | ms/batch 138.55 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 65.78s | training loss  5.39 |
    | end of validation epoch  15 | time: 53.27s | validation loss  4.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 162.72 | loss  5.35 |
| epoch  16 |   200/  475 batches | ms/batch 148.59 | loss  5.31 |
| epoch  16 |   300/  475 batches | ms/batch 142.29 | loss  5.40 |
| epoch  16 |   400/  475 batches | ms/batch 139.99 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 66.31s | training loss  5.36 |
    | end of validation epoch  16 | time: 53.92s | validation loss  4.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 161.29 | loss  5.61 |
| epoch  17 |   200/  475 batches | ms/batch 147.56 | loss  5.41 |
| epoch  17 |   300/  475 batches | ms/batch 143.07 | loss  5.05 |
| epoch  17 |   400/  475 batches | ms/batch 141.20 | loss  5.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 67.06s | training loss  5.32 |
    | end of validation epoch  17 | time: 53.44s | validation loss  4.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 163.29 | loss  5.44 |
| epoch  18 |   200/  475 batches | ms/batch 147.28 | loss  5.40 |
| epoch  18 |   300/  475 batches | ms/batch 142.87 | loss  5.38 |
| epoch  18 |   400/  475 batches | ms/batch 141.90 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 67.43s | training loss  5.29 |
    | end of validation epoch  18 | time: 53.27s | validation loss  4.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 159.64 | loss  5.07 |
| epoch  19 |   200/  475 batches | ms/batch 147.64 | loss  5.25 |
| epoch  19 |   300/  475 batches | ms/batch 143.10 | loss  5.30 |
| epoch  19 |   400/  475 batches | ms/batch 140.53 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 66.38s | training loss  5.27 |
    | end of validation epoch  19 | time: 53.46s | validation loss  4.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 157.49 | loss  5.20 |
| epoch  20 |   200/  475 batches | ms/batch 147.06 | loss  5.30 |
| epoch  20 |   300/  475 batches | ms/batch 143.25 | loss  5.40 |
| epoch  20 |   400/  475 batches | ms/batch 140.93 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 67.03s | training loss  5.24 |
    | end of validation epoch  20 | time: 52.68s | validation loss  4.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 162.00 | loss  5.01 |
| epoch  21 |   200/  475 batches | ms/batch 148.65 | loss  5.31 |
| epoch  21 |   300/  475 batches | ms/batch 144.49 | loss  5.18 |
| epoch  21 |   400/  475 batches | ms/batch 141.85 | loss  5.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 67.44s | training loss  5.22 |
    | end of validation epoch  21 | time: 52.98s | validation loss  4.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 162.46 | loss  5.40 |
| epoch  22 |   200/  475 batches | ms/batch 146.48 | loss  5.14 |
| epoch  22 |   300/  475 batches | ms/batch 140.51 | loss  4.89 |
| epoch  22 |   400/  475 batches | ms/batch 138.73 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 65.99s | training loss  5.21 |
    | end of validation epoch  22 | time: 52.68s | validation loss  4.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 155.28 | loss  5.30 |
| epoch  23 |   200/  475 batches | ms/batch 144.93 | loss  5.10 |
| epoch  23 |   300/  475 batches | ms/batch 143.17 | loss  5.07 |
| epoch  23 |   400/  475 batches | ms/batch 140.78 | loss  4.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 66.71s | training loss  5.19 |
    | end of validation epoch  23 | time: 53.13s | validation loss  4.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 162.26 | loss  4.92 |
| epoch  24 |   200/  475 batches | ms/batch 146.13 | loss  5.60 |
| epoch  24 |   300/  475 batches | ms/batch 141.76 | loss  5.38 |
| epoch  24 |   400/  475 batches | ms/batch 140.10 | loss  5.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 66.75s | training loss  5.18 |
    | end of validation epoch  24 | time: 55.94s | validation loss  4.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 158.40 | loss  4.86 |
| epoch  25 |   200/  475 batches | ms/batch 148.04 | loss  5.20 |
| epoch  25 |   300/  475 batches | ms/batch 144.80 | loss  4.95 |
| epoch  25 |   400/  475 batches | ms/batch 142.37 | loss  5.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 67.93s | training loss  5.14 |
    | end of validation epoch  25 | time: 54.15s | validation loss  4.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 158.43 | loss  5.33 |
| epoch  26 |   200/  475 batches | ms/batch 147.93 | loss  5.51 |
| epoch  26 |   300/  475 batches | ms/batch 142.60 | loss  5.33 |
| epoch  26 |   400/  475 batches | ms/batch 140.25 | loss  5.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 66.51s | training loss  5.14 |
    | end of validation epoch  26 | time: 51.79s | validation loss  4.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 153.02 | loss  5.21 |
| epoch  27 |   200/  475 batches | ms/batch 142.58 | loss  5.03 |
| epoch  27 |   300/  475 batches | ms/batch 141.87 | loss  5.00 |
| epoch  27 |   400/  475 batches | ms/batch 140.63 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 66.41s | training loss  5.11 |
    | end of validation epoch  27 | time: 52.88s | validation loss  4.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 161.98 | loss  5.11 |
| epoch  28 |   200/  475 batches | ms/batch 147.96 | loss  5.47 |
| epoch  28 |   300/  475 batches | ms/batch 143.44 | loss  4.88 |
| epoch  28 |   400/  475 batches | ms/batch 141.74 | loss  5.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 67.12s | training loss  5.11 |
    | end of validation epoch  28 | time: 52.35s | validation loss  4.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 160.21 | loss  5.18 |
| epoch  29 |   200/  475 batches | ms/batch 147.07 | loss  5.25 |
| epoch  29 |   300/  475 batches | ms/batch 142.59 | loss  5.06 |
| epoch  29 |   400/  475 batches | ms/batch 140.97 | loss  5.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 66.90s | training loss  5.11 |
    | end of validation epoch  29 | time: 52.43s | validation loss  4.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707]
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 159.88 | loss  4.93 |
| epoch  30 |   200/  475 batches | ms/batch 149.48 | loss  5.14 |
| epoch  30 |   300/  475 batches | ms/batch 143.59 | loss  5.13 |
| epoch  30 |   400/  475 batches | ms/batch 142.44 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 67.52s | training loss  5.10 |
    | end of validation epoch  30 | time: 52.83s | validation loss  4.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 160.10 | loss  4.98 |
| epoch  31 |   200/  475 batches | ms/batch 148.19 | loss  5.08 |
| epoch  31 |   300/  475 batches | ms/batch 143.13 | loss  4.73 |
| epoch  31 |   400/  475 batches | ms/batch 141.48 | loss  4.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 67.11s | training loss  5.09 |
    | end of validation epoch  31 | time: 53.39s | validation loss  4.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 164.59 | loss  5.02 |
| epoch  32 |   200/  475 batches | ms/batch 149.27 | loss  5.18 |
| epoch  32 |   300/  475 batches | ms/batch 144.18 | loss  4.99 |
| epoch  32 |   400/  475 batches | ms/batch 141.53 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 67.37s | training loss  5.08 |
    | end of validation epoch  32 | time: 52.90s | validation loss  4.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 160.86 | loss  4.97 |
| epoch  33 |   200/  475 batches | ms/batch 148.32 | loss  5.25 |
| epoch  33 |   300/  475 batches | ms/batch 143.70 | loss  5.14 |
| epoch  33 |   400/  475 batches | ms/batch 141.96 | loss  5.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 67.24s | training loss  5.06 |
    | end of validation epoch  33 | time: 52.25s | validation loss  4.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 157.69 | loss  4.87 |
| epoch  34 |   200/  475 batches | ms/batch 145.74 | loss  4.90 |
| epoch  34 |   300/  475 batches | ms/batch 141.69 | loss  5.03 |
| epoch  34 |   400/  475 batches | ms/batch 139.63 | loss  5.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 66.13s | training loss  5.06 |
    | end of validation epoch  34 | time: 52.20s | validation loss  4.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 165.85 | loss  5.22 |
| epoch  35 |   200/  475 batches | ms/batch 148.56 | loss  5.26 |
| epoch  35 |   300/  475 batches | ms/batch 143.59 | loss  5.01 |
| epoch  35 |   400/  475 batches | ms/batch 141.07 | loss  5.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 66.94s | training loss  5.06 |
    | end of validation epoch  35 | time: 53.38s | validation loss  4.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443]
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 161.29 | loss  5.04 |
| epoch  36 |   200/  475 batches | ms/batch 147.69 | loss  5.02 |
| epoch  36 |   300/  475 batches | ms/batch 144.05 | loss  5.11 |
| epoch  36 |   400/  475 batches | ms/batch 141.43 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 67.18s | training loss  5.05 |
    | end of validation epoch  36 | time: 54.06s | validation loss  4.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 163.46 | loss  4.94 |
| epoch  37 |   200/  475 batches | ms/batch 149.53 | loss  5.08 |
| epoch  37 |   300/  475 batches | ms/batch 144.06 | loss  4.80 |
| epoch  37 |   400/  475 batches | ms/batch 141.45 | loss  4.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 67.20s | training loss  5.04 |
    | end of validation epoch  37 | time: 52.18s | validation loss  4.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 156.78 | loss  5.10 |
| epoch  38 |   200/  475 batches | ms/batch 148.31 | loss  5.13 |
| epoch  38 |   300/  475 batches | ms/batch 143.14 | loss  5.22 |
| epoch  38 |   400/  475 batches | ms/batch 140.74 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 66.47s | training loss  5.03 |
    | end of validation epoch  38 | time: 53.89s | validation loss  4.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 155.66 | loss  4.80 |
| epoch  39 |   200/  475 batches | ms/batch 147.05 | loss  4.93 |
| epoch  39 |   300/  475 batches | ms/batch 141.58 | loss  5.01 |
| epoch  39 |   400/  475 batches | ms/batch 139.94 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 66.37s | training loss  5.03 |
    | end of validation epoch  39 | time: 53.41s | validation loss  4.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 164.00 | loss  5.09 |
| epoch  40 |   200/  475 batches | ms/batch 146.32 | loss  5.08 |
| epoch  40 |   300/  475 batches | ms/batch 141.47 | loss  5.08 |
| epoch  40 |   400/  475 batches | ms/batch 139.52 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 66.49s | training loss  5.04 |
    | end of validation epoch  40 | time: 50.55s | validation loss  4.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445]
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 158.44 | loss  5.01 |
| epoch  41 |   200/  475 batches | ms/batch 147.28 | loss  5.03 |
| epoch  41 |   300/  475 batches | ms/batch 142.43 | loss  4.74 |
| epoch  41 |   400/  475 batches | ms/batch 140.38 | loss  5.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 66.67s | training loss  5.02 |
    | end of validation epoch  41 | time: 52.45s | validation loss  4.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 156.89 | loss  5.09 |
| epoch  42 |   200/  475 batches | ms/batch 149.23 | loss  4.83 |
| epoch  42 |   300/  475 batches | ms/batch 143.16 | loss  5.08 |
| epoch  42 |   400/  475 batches | ms/batch 140.78 | loss  4.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 67.00s | training loss  5.01 |
    | end of validation epoch  42 | time: 54.21s | validation loss  4.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 162.07 | loss  4.87 |
| epoch  43 |   200/  475 batches | ms/batch 147.30 | loss  4.94 |
| epoch  43 |   300/  475 batches | ms/batch 142.37 | loss  5.11 |
| epoch  43 |   400/  475 batches | ms/batch 140.58 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 66.65s | training loss  5.01 |
    | end of validation epoch  43 | time: 52.69s | validation loss  4.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 160.44 | loss  5.07 |
| epoch  44 |   200/  475 batches | ms/batch 148.21 | loss  5.04 |
| epoch  44 |   300/  475 batches | ms/batch 143.73 | loss  5.10 |
| epoch  44 |   400/  475 batches | ms/batch 141.54 | loss  5.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 66.98s | training loss  5.01 |
    | end of validation epoch  44 | time: 53.68s | validation loss  4.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 161.77 | loss  4.99 |
| epoch  45 |   200/  475 batches | ms/batch 147.11 | loss  4.76 |
| epoch  45 |   300/  475 batches | ms/batch 142.27 | loss  5.02 |
| epoch  45 |   400/  475 batches | ms/batch 140.55 | loss  4.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 66.71s | training loss  5.01 |
    | end of validation epoch  45 | time: 54.24s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 157.85 | loss  4.81 |
| epoch  46 |   200/  475 batches | ms/batch 146.48 | loss  5.19 |
| epoch  46 |   300/  475 batches | ms/batch 143.23 | loss  5.03 |
| epoch  46 |   400/  475 batches | ms/batch 140.07 | loss  5.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 66.98s | training loss  5.01 |
    | end of validation epoch  46 | time: 52.69s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 163.15 | loss  4.99 |
| epoch  47 |   200/  475 batches | ms/batch 149.03 | loss  5.04 |
| epoch  47 |   300/  475 batches | ms/batch 143.40 | loss  5.40 |
| epoch  47 |   400/  475 batches | ms/batch 141.15 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 67.07s | training loss  5.00 |
    | end of validation epoch  47 | time: 54.52s | validation loss  4.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 158.95 | loss  5.22 |
| epoch  48 |   200/  475 batches | ms/batch 146.63 | loss  4.83 |
| epoch  48 |   300/  475 batches | ms/batch 143.93 | loss  5.07 |
| epoch  48 |   400/  475 batches | ms/batch 140.97 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 67.11s | training loss  4.99 |
    | end of validation epoch  48 | time: 53.09s | validation loss  4.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 163.63 | loss  4.93 |
| epoch  49 |   200/  475 batches | ms/batch 148.71 | loss  4.80 |
| epoch  49 |   300/  475 batches | ms/batch 143.83 | loss  4.87 |
| epoch  49 |   400/  475 batches | ms/batch 141.08 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 67.23s | training loss  5.01 |
    | end of validation epoch  49 | time: 52.33s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363]
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 159.42 | loss  5.09 |
| epoch  50 |   200/  475 batches | ms/batch 145.83 | loss  5.01 |
| epoch  50 |   300/  475 batches | ms/batch 141.16 | loss  5.08 |
| epoch  50 |   400/  475 batches | ms/batch 140.34 | loss  4.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 66.76s | training loss  5.00 |
    | end of validation epoch  50 | time: 54.48s | validation loss  4.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194]
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 166.35 | loss  4.79 |
| epoch  51 |   200/  475 batches | ms/batch 147.74 | loss  4.87 |
| epoch  51 |   300/  475 batches | ms/batch 142.49 | loss  4.84 |
| epoch  51 |   400/  475 batches | ms/batch 140.71 | loss  5.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 66.78s | training loss  5.00 |
    | end of validation epoch  51 | time: 53.81s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 161.04 | loss  4.68 |
| epoch  52 |   200/  475 batches | ms/batch 149.05 | loss  5.10 |
| epoch  52 |   300/  475 batches | ms/batch 143.34 | loss  5.23 |
| epoch  52 |   400/  475 batches | ms/batch 140.72 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 66.94s | training loss  4.99 |
    | end of validation epoch  52 | time: 52.53s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 155.01 | loss  4.70 |
| epoch  53 |   200/  475 batches | ms/batch 149.77 | loss  4.95 |
| epoch  53 |   300/  475 batches | ms/batch 143.45 | loss  4.81 |
| epoch  53 |   400/  475 batches | ms/batch 140.14 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 66.63s | training loss  4.99 |
    | end of validation epoch  53 | time: 54.36s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 164.52 | loss  5.15 |
| epoch  54 |   200/  475 batches | ms/batch 148.91 | loss  4.69 |
| epoch  54 |   300/  475 batches | ms/batch 145.40 | loss  5.28 |
| epoch  54 |   400/  475 batches | ms/batch 143.02 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 67.69s | training loss  4.99 |
    | end of validation epoch  54 | time: 54.30s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 164.08 | loss  4.86 |
| epoch  55 |   200/  475 batches | ms/batch 149.75 | loss  5.04 |
| epoch  55 |   300/  475 batches | ms/batch 144.49 | loss  4.78 |
| epoch  55 |   400/  475 batches | ms/batch 142.50 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 67.37s | training loss  4.98 |
    | end of validation epoch  55 | time: 53.98s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 161.03 | loss  4.64 |
| epoch  56 |   200/  475 batches | ms/batch 148.21 | loss  5.00 |
| epoch  56 |   300/  475 batches | ms/batch 143.69 | loss  4.84 |
| epoch  56 |   400/  475 batches | ms/batch 141.42 | loss  5.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 67.20s | training loss  4.98 |
    | end of validation epoch  56 | time: 52.88s | validation loss  4.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 162.71 | loss  4.84 |
| epoch  57 |   200/  475 batches | ms/batch 148.20 | loss  4.94 |
| epoch  57 |   300/  475 batches | ms/batch 142.29 | loss  5.01 |
| epoch  57 |   400/  475 batches | ms/batch 140.11 | loss  5.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 66.68s | training loss  5.00 |
    | end of validation epoch  57 | time: 53.23s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005]
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 163.54 | loss  5.07 |
| epoch  58 |   200/  475 batches | ms/batch 148.09 | loss  5.00 |
| epoch  58 |   300/  475 batches | ms/batch 142.74 | loss  5.03 |
| epoch  58 |   400/  475 batches | ms/batch 140.55 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 66.60s | training loss  4.98 |
    | end of validation epoch  58 | time: 52.86s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 162.12 | loss  5.14 |
| epoch  59 |   200/  475 batches | ms/batch 149.45 | loss  4.55 |
| epoch  59 |   300/  475 batches | ms/batch 144.82 | loss  5.17 |
| epoch  59 |   400/  475 batches | ms/batch 141.70 | loss  4.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 67.08s | training loss  4.99 |
    | end of validation epoch  59 | time: 53.91s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791]
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 163.93 | loss  5.02 |
| epoch  60 |   200/  475 batches | ms/batch 148.40 | loss  4.92 |
| epoch  60 |   300/  475 batches | ms/batch 143.31 | loss  5.10 |
| epoch  60 |   400/  475 batches | ms/batch 140.97 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 66.69s | training loss  4.98 |
    | end of validation epoch  60 | time: 53.89s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 161.79 | loss  5.14 |
| epoch  61 |   200/  475 batches | ms/batch 147.49 | loss  4.73 |
| epoch  61 |   300/  475 batches | ms/batch 141.73 | loss  5.21 |
| epoch  61 |   400/  475 batches | ms/batch 140.32 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 66.39s | training loss  4.98 |
    | end of validation epoch  61 | time: 53.66s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 162.89 | loss  4.60 |
| epoch  62 |   200/  475 batches | ms/batch 145.96 | loss  5.09 |
| epoch  62 |   300/  475 batches | ms/batch 141.87 | loss  5.35 |
| epoch  62 |   400/  475 batches | ms/batch 139.34 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 66.26s | training loss  4.98 |
    | end of validation epoch  62 | time: 53.49s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 158.05 | loss  4.98 |
| epoch  63 |   200/  475 batches | ms/batch 146.92 | loss  4.98 |
| epoch  63 |   300/  475 batches | ms/batch 142.35 | loss  4.99 |
| epoch  63 |   400/  475 batches | ms/batch 140.10 | loss  4.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 66.85s | training loss  4.97 |
    | end of validation epoch  63 | time: 53.39s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 161.31 | loss  4.78 |
| epoch  64 |   200/  475 batches | ms/batch 147.55 | loss  4.70 |
| epoch  64 |   300/  475 batches | ms/batch 141.62 | loss  5.02 |
| epoch  64 |   400/  475 batches | ms/batch 140.14 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 66.52s | training loss  4.99 |
    | end of validation epoch  64 | time: 53.36s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 160.73 | loss  5.14 |
| epoch  65 |   200/  475 batches | ms/batch 149.11 | loss  5.04 |
| epoch  65 |   300/  475 batches | ms/batch 144.12 | loss  5.03 |
| epoch  65 |   400/  475 batches | ms/batch 140.98 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 66.88s | training loss  4.97 |
    | end of validation epoch  65 | time: 53.27s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 161.17 | loss  4.74 |
| epoch  66 |   200/  475 batches | ms/batch 148.24 | loss  5.18 |
| epoch  66 |   300/  475 batches | ms/batch 141.90 | loss  5.09 |
| epoch  66 |   400/  475 batches | ms/batch 139.79 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 66.51s | training loss  4.98 |
    | end of validation epoch  66 | time: 54.47s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 164.66 | loss  4.96 |
| epoch  67 |   200/  475 batches | ms/batch 149.05 | loss  5.11 |
| epoch  67 |   300/  475 batches | ms/batch 143.67 | loss  5.16 |
| epoch  67 |   400/  475 batches | ms/batch 140.45 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 66.81s | training loss  4.98 |
    | end of validation epoch  67 | time: 53.48s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455]
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 160.98 | loss  4.89 |
| epoch  68 |   200/  475 batches | ms/batch 148.41 | loss  4.80 |
| epoch  68 |   300/  475 batches | ms/batch 142.21 | loss  5.22 |
| epoch  68 |   400/  475 batches | ms/batch 139.76 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 66.43s | training loss  4.98 |
    | end of validation epoch  68 | time: 53.90s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 159.25 | loss  5.18 |
| epoch  69 |   200/  475 batches | ms/batch 147.44 | loss  5.07 |
| epoch  69 |   300/  475 batches | ms/batch 142.53 | loss  5.10 |
| epoch  69 |   400/  475 batches | ms/batch 139.69 | loss  5.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 66.83s | training loss  4.98 |
    | end of validation epoch  69 | time: 53.96s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 164.03 | loss  5.15 |
| epoch  70 |   200/  475 batches | ms/batch 148.66 | loss  4.82 |
| epoch  70 |   300/  475 batches | ms/batch 143.75 | loss  5.27 |
| epoch  70 |   400/  475 batches | ms/batch 140.30 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 66.54s | training loss  4.98 |
    | end of validation epoch  70 | time: 54.08s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633]
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 161.95 | loss  4.95 |
| epoch  71 |   200/  475 batches | ms/batch 148.13 | loss  5.28 |
| epoch  71 |   300/  475 batches | ms/batch 143.03 | loss  4.93 |
| epoch  71 |   400/  475 batches | ms/batch 140.31 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 66.59s | training loss  4.98 |
    | end of validation epoch  71 | time: 53.23s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 164.51 | loss  4.82 |
| epoch  72 |   200/  475 batches | ms/batch 148.69 | loss  4.97 |
| epoch  72 |   300/  475 batches | ms/batch 143.15 | loss  4.93 |
| epoch  72 |   400/  475 batches | ms/batch 141.49 | loss  5.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 67.36s | training loss  4.98 |
    | end of validation epoch  72 | time: 53.25s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095]
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 161.65 | loss  4.95 |
| epoch  73 |   200/  475 batches | ms/batch 146.96 | loss  4.78 |
| epoch  73 |   300/  475 batches | ms/batch 142.39 | loss  5.14 |
| epoch  73 |   400/  475 batches | ms/batch 140.80 | loss  4.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 66.79s | training loss  4.98 |
    | end of validation epoch  73 | time: 51.73s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 160.86 | loss  4.86 |
| epoch  74 |   200/  475 batches | ms/batch 148.00 | loss  4.96 |
| epoch  74 |   300/  475 batches | ms/batch 143.74 | loss  5.05 |
| epoch  74 |   400/  475 batches | ms/batch 140.75 | loss  4.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 66.63s | training loss  4.97 |
    | end of validation epoch  74 | time: 55.33s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 158.56 | loss  4.91 |
| epoch  75 |   200/  475 batches | ms/batch 146.39 | loss  4.76 |
| epoch  75 |   300/  475 batches | ms/batch 141.63 | loss  4.55 |
| epoch  75 |   400/  475 batches | ms/batch 139.87 | loss  5.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 66.84s | training loss  4.98 |
    | end of validation epoch  75 | time: 53.78s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 157.12 | loss  5.05 |
| epoch  76 |   200/  475 batches | ms/batch 145.39 | loss  4.82 |
| epoch  76 |   300/  475 batches | ms/batch 140.95 | loss  4.92 |
| epoch  76 |   400/  475 batches | ms/batch 138.93 | loss  4.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 65.98s | training loss  4.98 |
    | end of validation epoch  76 | time: 53.19s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 155.23 | loss  5.14 |
| epoch  77 |   200/  475 batches | ms/batch 146.93 | loss  5.19 |
| epoch  77 |   300/  475 batches | ms/batch 142.00 | loss  4.79 |
| epoch  77 |   400/  475 batches | ms/batch 140.76 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 66.83s | training loss  4.97 |
    | end of validation epoch  77 | time: 54.57s | validation loss  4.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665]
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 159.39 | loss  4.97 |
| epoch  78 |   200/  475 batches | ms/batch 145.97 | loss  4.80 |
| epoch  78 |   300/  475 batches | ms/batch 141.34 | loss  5.00 |
| epoch  78 |   400/  475 batches | ms/batch 138.89 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 66.11s | training loss  4.97 |
    | end of validation epoch  78 | time: 53.94s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 157.85 | loss  4.89 |
| epoch  79 |   200/  475 batches | ms/batch 147.28 | loss  5.23 |
| epoch  79 |   300/  475 batches | ms/batch 143.03 | loss  5.38 |
| epoch  79 |   400/  475 batches | ms/batch 140.54 | loss  4.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 66.91s | training loss  4.97 |
    | end of validation epoch  79 | time: 52.83s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 163.21 | loss  5.37 |
| epoch  80 |   200/  475 batches | ms/batch 147.18 | loss  4.96 |
| epoch  80 |   300/  475 batches | ms/batch 141.84 | loss  4.74 |
| epoch  80 |   400/  475 batches | ms/batch 139.27 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 66.52s | training loss  4.98 |
    | end of validation epoch  80 | time: 53.27s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 163.21 | loss  4.64 |
| epoch  81 |   200/  475 batches | ms/batch 148.20 | loss  4.99 |
| epoch  81 |   300/  475 batches | ms/batch 144.07 | loss  5.11 |
| epoch  81 |   400/  475 batches | ms/batch 142.96 | loss  4.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 67.85s | training loss  4.96 |
    | end of validation epoch  81 | time: 55.80s | validation loss  4.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 163.05 | loss  5.08 |
| epoch  82 |   200/  475 batches | ms/batch 150.74 | loss  5.11 |
| epoch  82 |   300/  475 batches | ms/batch 144.65 | loss  5.24 |
| epoch  82 |   400/  475 batches | ms/batch 141.27 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 67.31s | training loss  4.97 |
    | end of validation epoch  82 | time: 54.34s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 162.59 | loss  4.92 |
| epoch  83 |   200/  475 batches | ms/batch 148.55 | loss  5.12 |
| epoch  83 |   300/  475 batches | ms/batch 143.69 | loss  4.47 |
| epoch  83 |   400/  475 batches | ms/batch 141.76 | loss  4.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 66.85s | training loss  4.98 |
    | end of validation epoch  83 | time: 54.04s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 155.63 | loss  4.83 |
| epoch  84 |   200/  475 batches | ms/batch 148.09 | loss  5.15 |
| epoch  84 |   300/  475 batches | ms/batch 142.84 | loss  4.83 |
| epoch  84 |   400/  475 batches | ms/batch 140.28 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 66.53s | training loss  4.98 |
    | end of validation epoch  84 | time: 51.61s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 153.11 | loss  5.04 |
| epoch  85 |   200/  475 batches | ms/batch 145.15 | loss  4.55 |
| epoch  85 |   300/  475 batches | ms/batch 142.53 | loss  4.58 |
| epoch  85 |   400/  475 batches | ms/batch 140.11 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 66.71s | training loss  4.98 |
    | end of validation epoch  85 | time: 53.65s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 164.06 | loss  4.93 |
| epoch  86 |   200/  475 batches | ms/batch 147.42 | loss  5.05 |
| epoch  86 |   300/  475 batches | ms/batch 142.19 | loss  5.02 |
| epoch  86 |   400/  475 batches | ms/batch 139.66 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 66.42s | training loss  4.97 |
    | end of validation epoch  86 | time: 53.70s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 160.44 | loss  4.68 |
| epoch  87 |   200/  475 batches | ms/batch 149.00 | loss  4.84 |
| epoch  87 |   300/  475 batches | ms/batch 143.78 | loss  5.21 |
| epoch  87 |   400/  475 batches | ms/batch 141.14 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 67.00s | training loss  4.97 |
    | end of validation epoch  87 | time: 54.26s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 162.16 | loss  5.40 |
| epoch  88 |   200/  475 batches | ms/batch 147.38 | loss  4.71 |
| epoch  88 |   300/  475 batches | ms/batch 142.14 | loss  4.98 |
| epoch  88 |   400/  475 batches | ms/batch 140.07 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 66.44s | training loss  4.97 |
    | end of validation epoch  88 | time: 53.93s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 152.14 | loss  5.18 |
| epoch  89 |   200/  475 batches | ms/batch 144.99 | loss  4.90 |
| epoch  89 |   300/  475 batches | ms/batch 143.01 | loss  4.78 |
| epoch  89 |   400/  475 batches | ms/batch 141.08 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 66.67s | training loss  4.98 |
    | end of validation epoch  89 | time: 53.43s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 160.83 | loss  4.88 |
| epoch  90 |   200/  475 batches | ms/batch 148.50 | loss  5.11 |
| epoch  90 |   300/  475 batches | ms/batch 143.24 | loss  4.98 |
| epoch  90 |   400/  475 batches | ms/batch 140.90 | loss  5.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 66.88s | training loss  4.98 |
    | end of validation epoch  90 | time: 52.90s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 162.85 | loss  4.83 |
| epoch  91 |   200/  475 batches | ms/batch 150.06 | loss  4.44 |
| epoch  91 |   300/  475 batches | ms/batch 144.37 | loss  5.00 |
| epoch  91 |   400/  475 batches | ms/batch 140.90 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 66.68s | training loss  4.97 |
    | end of validation epoch  91 | time: 54.40s | validation loss  4.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 156.83 | loss  4.91 |
| epoch  92 |   200/  475 batches | ms/batch 147.23 | loss  5.04 |
| epoch  92 |   300/  475 batches | ms/batch 142.84 | loss  5.08 |
| epoch  92 |   400/  475 batches | ms/batch 140.50 | loss  5.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 66.84s | training loss  4.97 |
    | end of validation epoch  92 | time: 53.65s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 162.59 | loss  4.75 |
| epoch  93 |   200/  475 batches | ms/batch 147.56 | loss  4.79 |
| epoch  93 |   300/  475 batches | ms/batch 142.99 | loss  5.10 |
| epoch  93 |   400/  475 batches | ms/batch 141.14 | loss  5.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 67.29s | training loss  4.97 |
    | end of validation epoch  93 | time: 54.53s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 162.24 | loss  5.17 |
| epoch  94 |   200/  475 batches | ms/batch 149.38 | loss  5.05 |
| epoch  94 |   300/  475 batches | ms/batch 142.78 | loss  4.89 |
| epoch  94 |   400/  475 batches | ms/batch 141.01 | loss  4.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 66.94s | training loss  4.97 |
    | end of validation epoch  94 | time: 54.57s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 154.42 | loss  4.92 |
| epoch  95 |   200/  475 batches | ms/batch 144.11 | loss  5.09 |
| epoch  95 |   300/  475 batches | ms/batch 143.95 | loss  5.16 |
| epoch  95 |   400/  475 batches | ms/batch 141.00 | loss  5.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 67.28s | training loss  4.97 |
    | end of validation epoch  95 | time: 53.92s | validation loss  4.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115, 4.969081173946983] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169, 4.706996392803032]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 159.52 | loss  5.30 |
| epoch  96 |   200/  475 batches | ms/batch 147.60 | loss  5.27 |
| epoch  96 |   300/  475 batches | ms/batch 143.41 | loss  5.13 |
| epoch  96 |   400/  475 batches | ms/batch 141.62 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 67.25s | training loss  4.96 |
    | end of validation epoch  96 | time: 54.21s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115, 4.969081173946983, 4.96432207107544] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169, 4.706996392803032, 4.7170826126547425]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 155.51 | loss  4.75 |
| epoch  97 |   200/  475 batches | ms/batch 147.72 | loss  5.21 |
| epoch  97 |   300/  475 batches | ms/batch 143.21 | loss  4.76 |
| epoch  97 |   400/  475 batches | ms/batch 141.05 | loss  4.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 67.06s | training loss  4.98 |
    | end of validation epoch  97 | time: 53.28s | validation loss  4.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115, 4.969081173946983, 4.96432207107544, 4.979024275729531] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169, 4.706996392803032, 4.7170826126547425, 4.724845012696851]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 161.77 | loss  4.83 |
| epoch  98 |   200/  475 batches | ms/batch 149.62 | loss  4.90 |
| epoch  98 |   300/  475 batches | ms/batch 144.22 | loss  4.98 |
| epoch  98 |   400/  475 batches | ms/batch 141.69 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 67.18s | training loss  4.97 |
    | end of validation epoch  98 | time: 53.18s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115, 4.969081173946983, 4.96432207107544, 4.979024275729531, 4.969217027363024] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169, 4.706996392803032, 4.7170826126547425, 4.724845012696851, 4.700976199462634]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 161.78 | loss  5.21 |
| epoch  99 |   200/  475 batches | ms/batch 148.93 | loss  4.87 |
| epoch  99 |   300/  475 batches | ms/batch 144.56 | loss  4.88 |
| epoch  99 |   400/  475 batches | ms/batch 142.29 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 67.50s | training loss  4.98 |
    | end of validation epoch  99 | time: 54.33s | validation loss  4.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.910239618201005, 6.596164766612806, 6.3987746700487635, 6.228687276338276, 6.091297186801308, 5.978769800286544, 5.875603368658768, 5.790623620685778, 5.705875551324142, 5.63725661026804, 5.577879264229223, 5.515801701796682, 5.472630495774118, 5.436069192384418, 5.3929167275679735, 5.364328445635344, 5.317848417382491, 5.292982908550061, 5.2680519314816125, 5.2368104432758535, 5.222142905185097, 5.207125066456042, 5.191807893451892, 5.177104988098145, 5.1446460844341075, 5.139801441995721, 5.1138879414608605, 5.1058032146253085, 5.107437336570338, 5.100356927168996, 5.087973015433864, 5.076289332540411, 5.05984968085038, 5.058326825593647, 5.059109824331183, 5.048056567342658, 5.040702584919177, 5.028072012851113, 5.025122032165528, 5.035517349243164, 5.020182353571841, 5.012728855735377, 5.007011314191316, 5.006719282049882, 5.012385453675923, 5.012680794565301, 5.004389032062731, 4.992348105781956, 5.006890302959241, 4.995837449525532, 5.000032826473839, 4.9935714290016575, 4.990718907808002, 4.988068565569426, 4.984725072760331, 4.983776263186806, 4.998050569233142, 4.979069984837582, 4.9923165180808615, 4.98083524904753, 4.978422404841373, 4.982865425912958, 4.97221848738821, 4.986166558516653, 4.9691269222058745, 4.981779317353901, 4.983442436017488, 4.9787085442793995, 4.978243300789281, 4.975448872415643, 4.983005224529066, 4.982487451653731, 4.984643562718442, 4.973097121590063, 4.9764161772477, 4.9823839207699425, 4.971718948765805, 4.973839107312654, 4.967728727240312, 4.976073608398438, 4.963767638959383, 4.965282410069515, 4.975747234946803, 4.980799366800409, 4.976305502841347, 4.974417201594303, 4.968875440296374, 4.973087679210462, 4.977249732770418, 4.978615795938592, 4.9693622167486895, 4.972757331446598, 4.972769480253521, 4.965696833761115, 4.969081173946983, 4.96432207107544, 4.979024275729531, 4.969217027363024, 4.983013451224879] validation loss is  [5.733729566846575, 5.607950939851649, 5.47966868536813, 5.392962143200786, 5.320643925867161, 5.2551661299056365, 5.210647783359559, 5.162831001922864, 5.12826540890862, 5.097552596020098, 5.0467000007629395, 5.036013759484812, 5.016400076761967, 4.982978123576701, 4.9786447677291745, 4.956049474347539, 4.942700089526777, 4.926229817526681, 4.912407398223877, 4.913237780082126, 4.901863522890236, 4.883093148720365, 4.881408362829385, 4.846088177015801, 4.855675589136717, 4.851847608550256, 4.8422750224586295, 4.838460361256319, 4.842873284796707, 4.81625437135456, 4.818181935478659, 4.816757242218787, 4.787220446001582, 4.7967603587302845, 4.795250351689443, 4.794467970102775, 4.78917194815243, 4.777798011523335, 4.78432418919411, 4.7821816957297445, 4.774365052455614, 4.769506791058709, 4.766179000630098, 4.77597364057012, 4.751291435305812, 4.750581613107889, 4.756538883978579, 4.758269466271921, 4.752180908908363, 4.764436313084194, 4.749352579357243, 4.739608688514774, 4.739438541797029, 4.74898978642055, 4.748382612436759, 4.76725399594347, 4.743871600688005, 4.744571108777984, 4.743969736980791, 4.732143281888561, 4.7418782009797935, 4.750251737963252, 4.743178980691092, 4.731339887410653, 4.723163196018764, 4.72968860433883, 4.723272215418455, 4.725183615163595, 4.731367459818094, 4.736323725275633, 4.7311950330974675, 4.7381402825107095, 4.7213326181684225, 4.715110334027715, 4.73389164740298, 4.73440974499999, 4.7383726985514665, 4.734787900908654, 4.731402228860294, 4.724586967660599, 4.710218165101123, 4.725230136839282, 4.7253521390321875, 4.717378247685793, 4.723235346689946, 4.719226740989365, 4.718284338462253, 4.715783015018752, 4.700093718136058, 4.732091510997099, 4.734340539499491, 4.7034494055419405, 4.704049118426668, 4.71554494505169, 4.706996392803032, 4.7170826126547425, 4.724845012696851, 4.700976199462634, 4.705120118726201]
