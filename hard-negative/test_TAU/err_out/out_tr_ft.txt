/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1065.17 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 114.38s | training loss  1.73 |
    | end of validation epoch   1 | time: 72.85s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [1.7344622010583277] validation loss is  [1.1347191488991182]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 969.08 | loss  0.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 105.50s | training loss  1.19 |
    | end of validation epoch   2 | time: 68.14s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [1.7344622010583277, 1.1871470094801069] validation loss is  [1.1347191488991182, 1.2026442454662174]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 994.94 | loss  0.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 107.38s | training loss  1.02 |
    | end of validation epoch   3 | time: 66.22s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 990.42 | loss  0.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 106.40s | training loss  0.89 |
    | end of validation epoch   4 | time: 71.42s | validation loss  1.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 1006.95 | loss  0.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 108.52s | training loss  0.81 |
    | end of validation epoch   5 | time: 67.48s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 998.21 | loss  1.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 107.28s | training loss  0.75 |
    | end of validation epoch   6 | time: 66.99s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1006.01 | loss  0.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 108.38s | training loss  0.66 |
    | end of validation epoch   7 | time: 66.96s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 998.05 | loss  0.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 107.33s | training loss  0.62 |
    | end of validation epoch   8 | time: 70.12s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1000.02 | loss  0.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 108.13s | training loss  0.56 |
    | end of validation epoch   9 | time: 69.45s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1007.84 | loss  0.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 108.30s | training loss  0.53 |
    | end of validation epoch  10 | time: 68.97s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1000.02 | loss  0.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 107.58s | training loss  0.50 |
    | end of validation epoch  11 | time: 71.38s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1023.24 | loss  0.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 110.38s | training loss  0.47 |
    | end of validation epoch  12 | time: 73.26s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1030.20 | loss  0.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 111.90s | training loss  0.43 |
    | end of validation epoch  13 | time: 70.20s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1045.77 | loss  0.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 112.80s | training loss  0.41 |
    | end of validation epoch  14 | time: 69.75s | validation loss  1.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1021.29 | loss  0.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 110.79s | training loss  0.38 |
    | end of validation epoch  15 | time: 73.20s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1121.95 | loss  0.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 120.11s | training loss  0.38 |
    | end of validation epoch  16 | time: 71.49s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1021.79 | loss  0.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 109.64s | training loss  0.35 |
    | end of validation epoch  17 | time: 70.69s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1035.31 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 111.36s | training loss  0.31 |
    | end of validation epoch  18 | time: 71.34s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1029.89 | loss  0.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 111.66s | training loss  0.34 |
    | end of validation epoch  19 | time: 71.63s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028]
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 1032.90 | loss  0.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 111.21s | training loss  0.30 |
    | end of validation epoch  20 | time: 70.73s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 1035.65 | loss  0.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 111.06s | training loss  0.28 |
    | end of validation epoch  21 | time: 70.88s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1032.57 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 110.97s | training loss  0.28 |
    | end of validation epoch  22 | time: 72.36s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1026.19 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 110.44s | training loss  0.26 |
    | end of validation epoch  23 | time: 71.43s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1029.32 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 111.67s | training loss  0.25 |
    | end of validation epoch  24 | time: 70.30s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1030.25 | loss  0.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 111.02s | training loss  0.25 |
    | end of validation epoch  25 | time: 70.27s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1037.53 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 111.61s | training loss  0.24 |
    | end of validation epoch  26 | time: 72.02s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1026.46 | loss  0.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 110.90s | training loss  0.23 |
    | end of validation epoch  27 | time: 71.30s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1026.43 | loss  0.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 110.48s | training loss  0.22 |
    | end of validation epoch  28 | time: 69.59s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1028.15 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 111.43s | training loss  0.20 |
    | end of validation epoch  29 | time: 70.59s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1022.02 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 110.28s | training loss  0.20 |
    | end of validation epoch  30 | time: 70.81s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1029.01 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 110.65s | training loss  0.19 |
    | end of validation epoch  31 | time: 70.61s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1032.86 | loss  0.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 111.00s | training loss  0.18 |
    | end of validation epoch  32 | time: 70.42s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1037.07 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 111.21s | training loss  0.18 |
    | end of validation epoch  33 | time: 70.60s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1029.96 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 110.93s | training loss  0.19 |
    | end of validation epoch  34 | time: 72.08s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717]
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1043.56 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 112.46s | training loss  0.17 |
    | end of validation epoch  35 | time: 70.89s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1045.77 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 113.35s | training loss  0.16 |
    | end of validation epoch  36 | time: 69.54s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1043.84 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 112.38s | training loss  0.18 |
    | end of validation epoch  37 | time: 71.49s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519]
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1042.96 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 112.26s | training loss  0.15 |
    | end of validation epoch  38 | time: 71.85s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 1032.66 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 111.12s | training loss  0.16 |
    | end of validation epoch  39 | time: 67.15s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126]
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 993.24 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 106.91s | training loss  0.15 |
    | end of validation epoch  40 | time: 66.42s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814]
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 993.32 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 106.97s | training loss  0.14 |
    | end of validation epoch  41 | time: 67.78s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 987.82 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 106.16s | training loss  0.14 |
    | end of validation epoch  42 | time: 67.15s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 979.73 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 106.05s | training loss  0.13 |
    | end of validation epoch  43 | time: 67.27s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 982.47 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 105.98s | training loss  0.13 |
    | end of validation epoch  44 | time: 68.22s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684]
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 986.78 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 106.01s | training loss  0.12 |
    | end of validation epoch  45 | time: 67.99s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 984.17 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 106.27s | training loss  0.13 |
    | end of validation epoch  46 | time: 67.70s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 981.86 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 106.57s | training loss  0.11 |
    | end of validation epoch  47 | time: 68.33s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 991.38 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 106.62s | training loss  0.11 |
    | end of validation epoch  48 | time: 67.52s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 978.09 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 105.28s | training loss  0.12 |
    | end of validation epoch  49 | time: 66.87s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671]
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 976.33 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 105.41s | training loss  0.12 |
    | end of validation epoch  50 | time: 68.06s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 992.00 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 106.50s | training loss  0.11 |
    | end of validation epoch  51 | time: 67.93s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 982.21 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 105.78s | training loss  0.11 |
    | end of validation epoch  52 | time: 68.31s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 984.03 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 105.85s | training loss  0.10 |
    | end of validation epoch  53 | time: 67.04s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 982.09 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 105.73s | training loss  0.12 |
    | end of validation epoch  54 | time: 67.31s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1103.82 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 118.90s | training loss  0.11 |
    | end of validation epoch  55 | time: 69.50s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 981.53 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 106.93s | training loss  0.10 |
    | end of validation epoch  56 | time: 67.92s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 977.00 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 105.29s | training loss  0.09 |
    | end of validation epoch  57 | time: 68.31s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 973.36 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 106.70s | training loss  0.08 |
    | end of validation epoch  58 | time: 68.05s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 981.17 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 105.41s | training loss  0.09 |
    | end of validation epoch  59 | time: 67.88s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 985.38 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 105.85s | training loss  0.09 |
    | end of validation epoch  60 | time: 66.26s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 978.72 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 106.89s | training loss  0.10 |
    | end of validation epoch  61 | time: 66.83s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 978.98 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 105.79s | training loss  0.09 |
    | end of validation epoch  62 | time: 67.01s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 981.25 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 105.76s | training loss  0.08 |
    | end of validation epoch  63 | time: 65.80s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 985.20 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 105.73s | training loss  0.08 |
    | end of validation epoch  64 | time: 66.27s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 982.75 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 106.48s | training loss  0.08 |
    | end of validation epoch  65 | time: 66.73s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 989.51 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 106.32s | training loss  0.08 |
    | end of validation epoch  66 | time: 66.05s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 988.38 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 106.06s | training loss  0.08 |
    | end of validation epoch  67 | time: 65.83s | validation loss  1.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 991.27 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 106.62s | training loss  0.08 |
    | end of validation epoch  68 | time: 67.06s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 984.65 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 105.77s | training loss  0.08 |
    | end of validation epoch  69 | time: 66.08s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 987.14 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 106.30s | training loss  0.07 |
    | end of validation epoch  70 | time: 67.17s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 990.06 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 106.46s | training loss  0.07 |
    | end of validation epoch  71 | time: 66.21s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 979.27 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 106.02s | training loss  0.07 |
    | end of validation epoch  72 | time: 65.84s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 984.76 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 106.27s | training loss  0.07 |
    | end of validation epoch  73 | time: 69.75s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 1022.30 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 110.61s | training loss  0.07 |
    | end of validation epoch  74 | time: 67.54s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 986.05 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 105.80s | training loss  0.07 |
    | end of validation epoch  75 | time: 65.77s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 997.19 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 107.38s | training loss  0.07 |
    | end of validation epoch  76 | time: 67.45s | validation loss  1.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 1008.20 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 109.49s | training loss  0.07 |
    | end of validation epoch  77 | time: 68.58s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 984.61 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 105.78s | training loss  0.06 |
    | end of validation epoch  78 | time: 67.30s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 987.70 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 106.27s | training loss  0.08 |
    | end of validation epoch  79 | time: 65.25s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 986.82 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 105.89s | training loss  0.07 |
    | end of validation epoch  80 | time: 66.18s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 981.48 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 105.98s | training loss  0.07 |
    | end of validation epoch  81 | time: 67.87s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 984.46 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 106.02s | training loss  0.07 |
    | end of validation epoch  82 | time: 67.14s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 979.61 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 106.35s | training loss  0.06 |
    | end of validation epoch  83 | time: 65.41s | validation loss  1.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 983.97 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 105.77s | training loss  0.05 |
    | end of validation epoch  84 | time: 66.50s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 980.87 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 105.19s | training loss  0.05 |
    | end of validation epoch  85 | time: 66.45s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 987.11 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 106.28s | training loss  0.06 |
    | end of validation epoch  86 | time: 65.73s | validation loss  1.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 985.06 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 106.07s | training loss  0.06 |
    | end of validation epoch  87 | time: 66.87s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 984.44 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 105.84s | training loss  0.06 |
    | end of validation epoch  88 | time: 68.10s | validation loss  1.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 991.15 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 106.88s | training loss  0.07 |
    | end of validation epoch  89 | time: 65.85s | validation loss  1.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 987.18 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 106.04s | training loss  0.06 |
    | end of validation epoch  90 | time: 65.77s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 984.50 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 106.03s | training loss  0.05 |
    | end of validation epoch  91 | time: 67.67s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 987.32 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 106.36s | training loss  0.05 |
    | end of validation epoch  92 | time: 67.35s | validation loss  1.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 983.56 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 105.73s | training loss  0.06 |
    | end of validation epoch  93 | time: 65.89s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 980.57 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 105.93s | training loss  0.05 |
    | end of validation epoch  94 | time: 66.17s | validation loss  1.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1091.02 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 117.58s | training loss  0.05 |
    | end of validation epoch  95 | time: 68.72s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196, 0.048895865779470755] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447, 1.6167982280797635]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 982.42 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 105.65s | training loss  0.04 |
    | end of validation epoch  96 | time: 67.47s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196, 0.048895865779470755, 0.040636252062311314] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447, 1.6167982280797635, 1.6161640401987825]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 982.96 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 105.88s | training loss  0.05 |
    | end of validation epoch  97 | time: 64.73s | validation loss  1.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196, 0.048895865779470755, 0.040636252062311314, 0.05358170541758473] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447, 1.6167982280797635, 1.6161640401987825, 1.709217517521817]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 986.71 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 106.21s | training loss  0.06 |
    | end of validation epoch  98 | time: 66.42s | validation loss  1.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196, 0.048895865779470755, 0.040636252062311314, 0.05358170541758473, 0.05610081021724319] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447, 1.6167982280797635, 1.6161640401987825, 1.709217517521817, 1.7711473140358673]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 985.19 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 105.74s | training loss  0.05 |
    | end of validation epoch  99 | time: 66.76s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [1.7344622010583277, 1.1871470094801069, 1.0205472090222814, 0.8914793880136164, 0.8087551827366287, 0.7454039234298844, 0.6579962407683467, 0.6164708048910708, 0.5617749578243977, 0.5333942975009884, 0.4964319376258163, 0.471658614573178, 0.42699524491756885, 0.4125418567711169, 0.38298441429395935, 0.3760805628052703, 0.34924757158434067, 0.31001772080455814, 0.3368325478456042, 0.298901466367481, 0.28189820749265654, 0.2767623410568581, 0.2638402424148611, 0.2500406524485296, 0.24693366019306956, 0.244898366968374, 0.22620894330310393, 0.22198550443391543, 0.19739212652852944, 0.19703102071542997, 0.19319661572441324, 0.17556836996395309, 0.1809586152635716, 0.18896669740075464, 0.16718098333290032, 0.15973612139219637, 0.17544999889828064, 0.14804930139232325, 0.15910403437174117, 0.1521539716972961, 0.13925561956591434, 0.14197653384359032, 0.13290977259879713, 0.1332642416562046, 0.12074323639542132, 0.1310181397239904, 0.10899854901137652, 0.11024562475850454, 0.11690937195745138, 0.11879687215965073, 0.11116108313404224, 0.10843197335195434, 0.10203563588092456, 0.1171600257826818, 0.10575597763464258, 0.10207470036573238, 0.09228885817333116, 0.08156186696309764, 0.09161995738946102, 0.09054625368265955, 0.09900945029011718, 0.08673544036778244, 0.07742989732808359, 0.0771573628005278, 0.0758976651489869, 0.07982295842186825, 0.08366428224185297, 0.08008532831864851, 0.075522630576145, 0.06890996121004359, 0.07085655123700162, 0.06923061597102263, 0.06935149644342092, 0.06553116725203958, 0.07179918743435058, 0.06764538966219973, 0.07222707994148, 0.06418023959037152, 0.07629181707261114, 0.06560575519059156, 0.06518924676788014, 0.06817353729091517, 0.05776330437259497, 0.05208488864622809, 0.04602328686228207, 0.05926223949168448, 0.06067016830684634, 0.05591289433105304, 0.0664527720545192, 0.06208229822584906, 0.049086819549703654, 0.05375593977871242, 0.05926577683101903, 0.05120733466318196, 0.048895865779470755, 0.040636252062311314, 0.05358170541758473, 0.05610081021724319, 0.049561345463132] validation loss is  [1.1347191488991182, 1.2026442454662174, 1.157066574941079, 1.0242247906862758, 1.083767748864678, 1.106816824215154, 1.1720233953868349, 1.1158591220155358, 1.0748755819707487, 1.1102814013332438, 1.1326551501988433, 1.1477143293789898, 1.0931827486492693, 1.0616088988802705, 1.1240507703332696, 1.2578408175613731, 1.2314532830690343, 1.151075951269983, 1.1215424152712028, 1.156857931734218, 1.2308356041515556, 1.2103857806529656, 1.183211166401937, 1.235871633097607, 1.2701916582882404, 1.2344393271487206, 1.233062040237807, 1.287492256272041, 1.2830596772303882, 1.2520564311368314, 1.1295015700743534, 1.2476205890416168, 1.2109644963250805, 1.351401319843717, 1.1946720604319125, 1.2437047892890405, 1.3076346046121519, 1.281429065866784, 1.2362224353225126, 1.2756180848227814, 1.4359650073650603, 1.3630074006311286, 1.4003282424237113, 1.242656046068684, 1.5083325885158654, 1.2785450227869053, 1.4649265118326487, 1.4433183103610645, 1.3906148475810671, 1.4688570277260926, 1.5120694592090633, 1.5548667742696125, 1.3562002972563885, 1.515384477126645, 1.5921527270741838, 1.5607737169436102, 1.5486043140699621, 1.4349258344785387, 1.508055055339355, 1.5321600963749613, 1.5276936405789456, 1.5076342529888886, 1.3694254833790183, 1.5154167403646472, 1.615079232690429, 1.4561436155321037, 1.687671684476906, 1.4120179125129653, 1.7773665359782171, 1.5871385288094946, 1.5778923388522041, 1.4904554638178524, 1.600768893684896, 1.5696293954145706, 1.6330268105327075, 1.7068618193579823, 1.5632686968698788, 1.6323612251107988, 1.7770583823439665, 1.528212575474754, 1.5351374473248143, 1.6006439944612794, 1.6571691461625353, 1.5874999977822881, 1.5919699192842625, 1.774526558816433, 1.52387006663533, 1.855498606186302, 1.7608103475649841, 1.684580430330243, 1.6801972416214994, 1.6671024359141786, 1.6249213943907914, 1.871382171586447, 1.6167982280797635, 1.6161640401987825, 1.709217517521817, 1.7711473140358673, 1.5792001101265971]
