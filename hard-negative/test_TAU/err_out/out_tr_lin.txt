/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 942.14 | loss  2.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 101.28s | training loss  2.95 |
    | end of validation epoch   1 | time: 67.59s | validation loss  2.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.9546813148636] validation loss is  [2.174508512020111]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 909.37 | loss  2.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 98.23s | training loss  2.60 |
    | end of validation epoch   2 | time: 67.83s | validation loss  1.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.9546813148636, 2.604789433178601] validation loss is  [2.174508512020111, 1.9321843534708023]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 935.69 | loss  2.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 100.65s | training loss  2.38 |
    | end of validation epoch   3 | time: 61.99s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 932.04 | loss  2.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 100.57s | training loss  2.23 |
    | end of validation epoch   4 | time: 65.07s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 916.75 | loss  2.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 98.70s | training loss  2.11 |
    | end of validation epoch   5 | time: 65.02s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 944.74 | loss  1.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 102.22s | training loss  2.00 |
    | end of validation epoch   6 | time: 63.17s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 924.65 | loss  2.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 99.80s | training loss  1.94 |
    | end of validation epoch   7 | time: 64.52s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 925.84 | loss  1.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 99.58s | training loss  1.88 |
    | end of validation epoch   8 | time: 65.29s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 936.08 | loss  1.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 100.50s | training loss  1.83 |
    | end of validation epoch   9 | time: 62.36s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 926.28 | loss  1.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 99.41s | training loss  1.75 |
    | end of validation epoch  10 | time: 64.76s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 923.12 | loss  1.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 99.56s | training loss  1.73 |
    | end of validation epoch  11 | time: 64.94s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 927.40 | loss  1.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 99.72s | training loss  1.68 |
    | end of validation epoch  12 | time: 62.19s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 925.93 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 99.56s | training loss  1.66 |
    | end of validation epoch  13 | time: 65.05s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 925.70 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 99.55s | training loss  1.65 |
    | end of validation epoch  14 | time: 65.05s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 929.30 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 100.01s | training loss  1.60 |
    | end of validation epoch  15 | time: 62.05s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 924.48 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 99.78s | training loss  1.59 |
    | end of validation epoch  16 | time: 65.10s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 952.02 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 102.22s | training loss  1.58 |
    | end of validation epoch  17 | time: 66.62s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 945.07 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 101.54s | training loss  1.55 |
    | end of validation epoch  18 | time: 62.96s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 938.42 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 100.83s | training loss  1.55 |
    | end of validation epoch  19 | time: 65.74s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 938.34 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 100.68s | training loss  1.52 |
    | end of validation epoch  20 | time: 65.60s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 942.71 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 101.35s | training loss  1.53 |
    | end of validation epoch  21 | time: 62.48s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 937.77 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 101.17s | training loss  1.50 |
    | end of validation epoch  22 | time: 65.62s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 935.59 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 100.59s | training loss  1.48 |
    | end of validation epoch  23 | time: 65.98s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 945.71 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 101.60s | training loss  1.50 |
    | end of validation epoch  24 | time: 62.74s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 937.70 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 101.45s | training loss  1.48 |
    | end of validation epoch  25 | time: 65.51s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 929.85 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 100.39s | training loss  1.46 |
    | end of validation epoch  26 | time: 65.18s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 929.97 | loss  1.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 100.59s | training loss  1.46 |
    | end of validation epoch  27 | time: 62.23s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 928.51 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 99.79s | training loss  1.45 |
    | end of validation epoch  28 | time: 65.17s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 928.02 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 99.79s | training loss  1.44 |
    | end of validation epoch  29 | time: 64.89s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 940.89 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 101.67s | training loss  1.44 |
    | end of validation epoch  30 | time: 62.82s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 940.30 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 101.26s | training loss  1.43 |
    | end of validation epoch  31 | time: 65.89s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 938.83 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 101.38s | training loss  1.43 |
    | end of validation epoch  32 | time: 65.85s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 955.35 | loss  1.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 102.75s | training loss  1.42 |
    | end of validation epoch  33 | time: 62.48s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 941.68 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 101.73s | training loss  1.43 |
    | end of validation epoch  34 | time: 65.34s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 935.23 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 100.41s | training loss  1.41 |
    | end of validation epoch  35 | time: 65.62s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 941.42 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 101.68s | training loss  1.43 |
    | end of validation epoch  36 | time: 62.64s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977]
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 949.38 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 102.01s | training loss  1.40 |
    | end of validation epoch  37 | time: 65.83s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 948.50 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 102.25s | training loss  1.40 |
    | end of validation epoch  38 | time: 65.71s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 963.23 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 104.18s | training loss  1.39 |
    | end of validation epoch  39 | time: 62.75s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 974.61 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 104.81s | training loss  1.40 |
    | end of validation epoch  40 | time: 65.76s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608]
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 951.97 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 102.22s | training loss  1.40 |
    | end of validation epoch  41 | time: 66.92s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 952.00 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 102.40s | training loss  1.40 |
    | end of validation epoch  42 | time: 62.66s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 945.38 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 101.70s | training loss  1.39 |
    | end of validation epoch  43 | time: 65.80s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 940.72 | loss  1.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 101.35s | training loss  1.39 |
    | end of validation epoch  44 | time: 65.84s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584]
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 948.94 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 102.11s | training loss  1.39 |
    | end of validation epoch  45 | time: 62.67s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 943.31 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 101.29s | training loss  1.39 |
    | end of validation epoch  46 | time: 65.33s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 940.23 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 101.60s | training loss  1.39 |
    | end of validation epoch  47 | time: 66.04s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 948.39 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 102.05s | training loss  1.39 |
    | end of validation epoch  48 | time: 63.01s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 941.72 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 101.24s | training loss  1.38 |
    | end of validation epoch  49 | time: 65.58s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 941.49 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 101.19s | training loss  1.39 |
    | end of validation epoch  50 | time: 65.65s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 947.26 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 101.85s | training loss  1.37 |
    | end of validation epoch  51 | time: 62.48s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 942.07 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 101.56s | training loss  1.37 |
    | end of validation epoch  52 | time: 65.59s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 936.27 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 100.75s | training loss  1.39 |
    | end of validation epoch  53 | time: 65.45s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 946.97 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 101.66s | training loss  1.38 |
    | end of validation epoch  54 | time: 62.68s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 941.11 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 101.93s | training loss  1.37 |
    | end of validation epoch  55 | time: 65.67s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 941.83 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 101.20s | training loss  1.37 |
    | end of validation epoch  56 | time: 65.99s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 946.25 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 101.60s | training loss  1.39 |
    | end of validation epoch  57 | time: 62.41s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 943.43 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 101.40s | training loss  1.36 |
    | end of validation epoch  58 | time: 65.62s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 939.10 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 101.02s | training loss  1.37 |
    | end of validation epoch  59 | time: 66.70s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 968.72 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 103.91s | training loss  1.37 |
    | end of validation epoch  60 | time: 63.18s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 938.61 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 100.95s | training loss  1.36 |
    | end of validation epoch  61 | time: 65.79s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 936.63 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 100.63s | training loss  1.37 |
    | end of validation epoch  62 | time: 66.13s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 941.11 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 100.99s | training loss  1.37 |
    | end of validation epoch  63 | time: 62.66s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754]
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 938.99 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 100.85s | training loss  1.36 |
    | end of validation epoch  64 | time: 65.65s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 932.26 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 100.06s | training loss  1.38 |
    | end of validation epoch  65 | time: 65.81s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 944.01 | loss  1.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 101.41s | training loss  1.38 |
    | end of validation epoch  66 | time: 62.90s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 941.25 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 101.39s | training loss  1.37 |
    | end of validation epoch  67 | time: 65.61s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 938.26 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 100.72s | training loss  1.38 |
    | end of validation epoch  68 | time: 65.90s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 945.99 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 101.51s | training loss  1.38 |
    | end of validation epoch  69 | time: 62.78s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 938.80 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 101.62s | training loss  1.38 |
    | end of validation epoch  70 | time: 65.85s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 937.79 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 100.89s | training loss  1.35 |
    | end of validation epoch  71 | time: 66.04s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 956.43 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 102.73s | training loss  1.36 |
    | end of validation epoch  72 | time: 62.89s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 943.42 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 101.29s | training loss  1.36 |
    | end of validation epoch  73 | time: 65.97s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 939.43 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 101.21s | training loss  1.38 |
    | end of validation epoch  74 | time: 65.74s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 948.42 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 101.93s | training loss  1.37 |
    | end of validation epoch  75 | time: 62.79s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 948.07 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 101.85s | training loss  1.37 |
    | end of validation epoch  76 | time: 65.79s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 945.74 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 101.61s | training loss  1.36 |
    | end of validation epoch  77 | time: 65.88s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 945.14 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 101.53s | training loss  1.36 |
    | end of validation epoch  78 | time: 62.73s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 941.54 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 101.37s | training loss  1.37 |
    | end of validation epoch  79 | time: 65.64s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 941.53 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 101.16s | training loss  1.36 |
    | end of validation epoch  80 | time: 65.90s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 946.35 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 101.73s | training loss  1.36 |
    | end of validation epoch  81 | time: 62.66s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 948.86 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 101.88s | training loss  1.36 |
    | end of validation epoch  82 | time: 65.91s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 935.19 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 101.37s | training loss  1.37 |
    | end of validation epoch  83 | time: 66.25s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 947.53 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 101.81s | training loss  1.37 |
    | end of validation epoch  84 | time: 63.21s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 942.52 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 101.32s | training loss  1.35 |
    | end of validation epoch  85 | time: 65.91s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 942.39 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 101.28s | training loss  1.38 |
    | end of validation epoch  86 | time: 65.81s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 949.94 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 102.05s | training loss  1.35 |
    | end of validation epoch  87 | time: 62.80s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 946.48 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 101.92s | training loss  1.37 |
    | end of validation epoch  88 | time: 65.85s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 941.79 | loss  1.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 101.22s | training loss  1.36 |
    | end of validation epoch  89 | time: 66.02s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 945.03 | loss  1.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 101.55s | training loss  1.36 |
    | end of validation epoch  90 | time: 62.70s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 947.86 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 101.90s | training loss  1.36 |
    | end of validation epoch  91 | time: 65.71s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 940.31 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 101.52s | training loss  1.35 |
    | end of validation epoch  92 | time: 66.09s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 943.39 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 101.47s | training loss  1.37 |
    | end of validation epoch  93 | time: 62.91s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 941.96 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 101.65s | training loss  1.35 |
    | end of validation epoch  94 | time: 65.75s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 942.69 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 101.34s | training loss  1.37 |
    | end of validation epoch  95 | time: 65.73s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775, 1.368133796227945] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555, 1.1741262959937255]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 945.35 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 101.55s | training loss  1.36 |
    | end of validation epoch  96 | time: 62.85s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775, 1.368133796227945, 1.361305020950936] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555, 1.1741262959937255, 1.1856406821558874]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 941.56 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 101.08s | training loss  1.36 |
    | end of validation epoch  97 | time: 65.49s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775, 1.368133796227945, 1.361305020950936, 1.359238695453953] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555, 1.1741262959937255, 1.1856406821558874, 1.1864092828085024]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 941.18 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 101.27s | training loss  1.35 |
    | end of validation epoch  98 | time: 65.88s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775, 1.368133796227945, 1.361305020950936, 1.359238695453953, 1.3544355834926571] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555, 1.1741262959937255, 1.1856406821558874, 1.1864092828085024, 1.1875877765317757]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 939.62 | loss  1.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 101.01s | training loss  1.37 |
    | end of validation epoch  99 | time: 62.68s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.9546813148636, 2.604789433178601, 2.382784253842122, 2.225198101353001, 2.1065598404085315, 2.0020004489400365, 1.9414539605647594, 1.8784815513335906, 1.825972825557262, 1.7530457876824044, 1.7319019560341362, 1.6839817987905967, 1.658834836504481, 1.6469153445046227, 1.6030251002526499, 1.587457894204973, 1.5833947003424704, 1.5535673160810728, 1.549722570556778, 1.5234643725661543, 1.527309462830827, 1.5045965222625044, 1.4840120721507717, 1.5015014098571227, 1.4837464964067615, 1.4648667692064166, 1.4556214573147062, 1.4453617055136878, 1.4445953702067469, 1.4403915952991795, 1.4318391449816592, 1.4317669728854756, 1.4218417792706877, 1.4261326145481419, 1.4116359100685463, 1.4290760798497244, 1.4041536208745595, 1.4029157709431004, 1.392473433468793, 1.4002151725528476, 1.3987478211119369, 1.3977521529068817, 1.394223980001501, 1.3945012307381845, 1.3943030662364788, 1.3893824289510917, 1.3931320637196034, 1.3876650720029264, 1.3823177213067408, 1.3915208470713984, 1.3654470540381767, 1.3720707227517892, 1.3893903246870987, 1.3750049230214711, 1.3728165046588794, 1.374351469246117, 1.3856025236146945, 1.3559467051480267, 1.3697135378648568, 1.3663386707907323, 1.3626728272652842, 1.3680118934528247, 1.3661240264102146, 1.364938551241213, 1.376477997582238, 1.3804912244951404, 1.366422092592394, 1.3806133635409243, 1.3789803681072887, 1.3799487696037636, 1.3528240596925891, 1.363158404827118, 1.3589175015956432, 1.3798413169276607, 1.3704136244885556, 1.3703886318851162, 1.357010285059611, 1.3616056098594322, 1.373402645996025, 1.363940142296456, 1.3645316953057642, 1.359861487740869, 1.3663367118921366, 1.3652395751025226, 1.3458126353787947, 1.376652353518718, 1.3548881449140944, 1.3715205278482523, 1.3578967736648009, 1.3629341984654333, 1.3602279070261363, 1.351199834196417, 1.3710013397105105, 1.3522461019120775, 1.368133796227945, 1.361305020950936, 1.359238695453953, 1.3544355834926571, 1.3669528220150922] validation loss is  [2.174508512020111, 1.9321843534708023, 1.7775071288148563, 1.6779195641477902, 1.585063320895036, 1.5379121253887813, 1.5046598389744759, 1.4589149095118046, 1.4307628124952316, 1.4027352295815945, 1.3852884309987228, 1.3639339866737525, 1.3656829223036766, 1.3476417064666748, 1.340011549492677, 1.3233667289217312, 1.329129238302509, 1.3006850617627304, 1.2901337705552578, 1.2815669141709805, 1.2741753583153088, 1.2741161758701007, 1.263308582206567, 1.259439495081703, 1.2644183269391458, 1.2465724721550941, 1.240550457810362, 1.2414452948917944, 1.243505023419857, 1.2324957667539518, 1.2407803535461426, 1.238375537097454, 1.2296087971578042, 1.2183584825446208, 1.2341661248356104, 1.2197993422547977, 1.2114069983363152, 1.212200127542019, 1.214127054437995, 1.2179366225997608, 1.2090353624274333, 1.2251129907866318, 1.221221908306082, 1.2132408395409584, 1.20391789637506, 1.2050525235633056, 1.1977479563405116, 1.2177644073963165, 1.200874714801709, 1.1927447977165382, 1.2000799744079511, 1.1939941806097825, 1.2069293279200792, 1.2051283245285351, 1.1929542645812035, 1.2044299362848203, 1.2024877381821473, 1.1907476894557476, 1.2041408736258745, 1.1893144889424245, 1.2144616208970547, 1.1894097340603669, 1.1964939751972754, 1.1921723429113626, 1.20443759051462, 1.1867521765331428, 1.1984043071667354, 1.201623070364197, 1.1998670883476734, 1.2066356564561527, 1.2034612006197374, 1.1971494338164728, 1.1915615803251665, 1.194366951783498, 1.1921250062684219, 1.1878147317717473, 1.200639517356952, 1.190585922760268, 1.2039102129638195, 1.1867601710061233, 1.2020114498833816, 1.1921834833920002, 1.1784364792207878, 1.2085038721561432, 1.182640839368105, 1.1805643364787102, 1.1979962388674419, 1.195292548276484, 1.1884105907132227, 1.197202742099762, 1.18279898352921, 1.185333798949917, 1.189886735752225, 1.1764884864290555, 1.1741262959937255, 1.1856406821558874, 1.1864092828085024, 1.1875877765317757, 1.1842276323586702]
