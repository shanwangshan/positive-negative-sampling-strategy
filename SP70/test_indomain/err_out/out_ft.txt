/run/nvme/job_2995876/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/scratch/asignal/shanshan/Audio-video-ACL/real_ssl_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 529.76 | loss  6.04 |
| epoch   1 |   200/  475 batches | ms/batch 303.53 | loss  5.70 |
| epoch   1 |   300/  475 batches | ms/batch 228.78 | loss  5.90 |
| epoch   1 |   400/  475 batches | ms/batch 191.04 | loss  5.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 82.84s | training loss  5.78 |
    | end of validation epoch   1 | time: 39.56s | validation loss  5.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.779148289529901] validation loss is  [5.314779506010168]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 105.23 | loss  5.42 |
| epoch   2 |   200/  475 batches | ms/batch 89.75 | loss  5.50 |
| epoch   2 |   300/  475 batches | ms/batch 84.82 | loss  5.28 |
| epoch   2 |   400/  475 batches | ms/batch 82.06 | loss  5.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 38.94s | training loss  5.31 |
    | end of validation epoch   2 | time: 40.88s | validation loss  4.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.779148289529901, 5.311436365027177] validation loss is  [5.314779506010168, 4.808883001824387]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 90.50 | loss  5.18 |
| epoch   3 |   200/  475 batches | ms/batch 81.07 | loss  5.07 |
| epoch   3 |   300/  475 batches | ms/batch 77.67 | loss  4.90 |
| epoch   3 |   400/  475 batches | ms/batch 76.20 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 36.30s | training loss  5.04 |
    | end of validation epoch   3 | time: 53.32s | validation loss  4.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 301.37 | loss  4.76 |
| epoch   4 |   200/  475 batches | ms/batch 186.76 | loss  4.55 |
| epoch   4 |   300/  475 batches | ms/batch 147.99 | loss  4.85 |
| epoch   4 |   400/  475 batches | ms/batch 128.84 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 57.24s | training loss  4.82 |
    | end of validation epoch   4 | time: 47.02s | validation loss  4.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 91.99 | loss  4.70 |
| epoch   5 |   200/  475 batches | ms/batch 81.63 | loss  4.71 |
| epoch   5 |   300/  475 batches | ms/batch 77.64 | loss  4.67 |
| epoch   5 |   400/  475 batches | ms/batch 75.77 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 35.99s | training loss  4.68 |
    | end of validation epoch   5 | time: 41.64s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 90.61 | loss  4.41 |
| epoch   6 |   200/  475 batches | ms/batch 82.00 | loss  4.33 |
| epoch   6 |   300/  475 batches | ms/batch 78.14 | loss  4.60 |
| epoch   6 |   400/  475 batches | ms/batch 76.38 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 36.23s | training loss  4.54 |
    | end of validation epoch   6 | time: 42.09s | validation loss  3.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 99.76 | loss  4.67 |
| epoch   7 |   200/  475 batches | ms/batch 85.93 | loss  4.18 |
| epoch   7 |   300/  475 batches | ms/batch 81.26 | loss  4.29 |
| epoch   7 |   400/  475 batches | ms/batch 78.93 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 37.46s | training loss  4.43 |
    | end of validation epoch   7 | time: 41.70s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 91.98 | loss  4.50 |
| epoch   8 |   200/  475 batches | ms/batch 81.14 | loss  4.20 |
| epoch   8 |   300/  475 batches | ms/batch 77.62 | loss  4.49 |
| epoch   8 |   400/  475 batches | ms/batch 75.96 | loss  3.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 36.16s | training loss  4.34 |
    | end of validation epoch   8 | time: 53.30s | validation loss  3.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 91.95 | loss  4.20 |
| epoch   9 |   200/  475 batches | ms/batch 103.27 | loss  4.43 |
| epoch   9 |   300/  475 batches | ms/batch 92.63 | loss  3.99 |
| epoch   9 |   400/  475 batches | ms/batch 87.78 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 48.52s | training loss  4.26 |
    | end of validation epoch   9 | time: 48.89s | validation loss  3.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 134.36 | loss  4.27 |
| epoch  10 |   200/  475 batches | ms/batch 102.78 | loss  4.01 |
| epoch  10 |   300/  475 batches | ms/batch 91.94 | loss  4.67 |
| epoch  10 |   400/  475 batches | ms/batch 87.05 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 40.69s | training loss  4.19 |
    | end of validation epoch  10 | time: 41.77s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 91.13 | loss  4.14 |
| epoch  11 |   200/  475 batches | ms/batch 81.98 | loss  4.33 |
| epoch  11 |   300/  475 batches | ms/batch 78.40 | loss  3.89 |
| epoch  11 |   400/  475 batches | ms/batch 76.25 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 36.42s | training loss  4.13 |
    | end of validation epoch  11 | time: 41.48s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 92.84 | loss  4.04 |
| epoch  12 |   200/  475 batches | ms/batch 81.74 | loss  4.09 |
| epoch  12 |   300/  475 batches | ms/batch 77.73 | loss  4.32 |
| epoch  12 |   400/  475 batches | ms/batch 75.99 | loss  3.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 36.31s | training loss  4.09 |
    | end of validation epoch  12 | time: 44.42s | validation loss  3.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 459.69 | loss  4.07 |
| epoch  13 |   200/  475 batches | ms/batch 265.34 | loss  3.80 |
| epoch  13 |   300/  475 batches | ms/batch 200.34 | loss  3.94 |
| epoch  13 |   400/  475 batches | ms/batch 167.91 | loss  3.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 73.03s | training loss  4.03 |
    | end of validation epoch  13 | time: 41.32s | validation loss  3.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 92.08 | loss  3.78 |
| epoch  14 |   200/  475 batches | ms/batch 81.39 | loss  3.89 |
| epoch  14 |   300/  475 batches | ms/batch 77.95 | loss  4.14 |
| epoch  14 |   400/  475 batches | ms/batch 76.39 | loss  3.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 36.53s | training loss  3.99 |
    | end of validation epoch  14 | time: 41.65s | validation loss  3.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 96.18 | loss  3.82 |
| epoch  15 |   200/  475 batches | ms/batch 83.97 | loss  4.18 |
| epoch  15 |   300/  475 batches | ms/batch 79.92 | loss  3.82 |
| epoch  15 |   400/  475 batches | ms/batch 82.13 | loss  4.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 38.49s | training loss  3.94 |
    | end of validation epoch  15 | time: 41.74s | validation loss  3.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 92.69 | loss  4.05 |
| epoch  16 |   200/  475 batches | ms/batch 81.83 | loss  3.92 |
| epoch  16 |   300/  475 batches | ms/batch 78.61 | loss  3.66 |
| epoch  16 |   400/  475 batches | ms/batch 76.19 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 36.37s | training loss  3.90 |
    | end of validation epoch  16 | time: 42.24s | validation loss  3.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 91.98 | loss  3.53 |
| epoch  17 |   200/  475 batches | ms/batch 81.60 | loss  4.00 |
| epoch  17 |   300/  475 batches | ms/batch 77.88 | loss  3.43 |
| epoch  17 |   400/  475 batches | ms/batch 78.58 | loss  3.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 37.11s | training loss  3.87 |
    | end of validation epoch  17 | time: 62.25s | validation loss  3.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 309.00 | loss  3.62 |
| epoch  18 |   200/  475 batches | ms/batch 190.32 | loss  3.64 |
| epoch  18 |   300/  475 batches | ms/batch 149.80 | loss  4.02 |
| epoch  18 |   400/  475 batches | ms/batch 129.66 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 57.80s | training loss  3.84 |
    | end of validation epoch  18 | time: 41.81s | validation loss  3.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 93.03 | loss  3.99 |
| epoch  19 |   200/  475 batches | ms/batch 81.74 | loss  3.95 |
| epoch  19 |   300/  475 batches | ms/batch 78.26 | loss  4.25 |
| epoch  19 |   400/  475 batches | ms/batch 76.56 | loss  3.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 36.31s | training loss  3.83 |
    | end of validation epoch  19 | time: 41.95s | validation loss  3.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 92.81 | loss  4.40 |
| epoch  20 |   200/  475 batches | ms/batch 81.68 | loss  3.71 |
| epoch  20 |   300/  475 batches | ms/batch 78.03 | loss  3.77 |
| epoch  20 |   400/  475 batches | ms/batch 77.75 | loss  4.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 36.22s | training loss  3.78 |
    | end of validation epoch  20 | time: 41.96s | validation loss  3.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 92.50 | loss  3.35 |
| epoch  21 |   200/  475 batches | ms/batch 82.45 | loss  3.50 |
| epoch  21 |   300/  475 batches | ms/batch 78.88 | loss  3.32 |
| epoch  21 |   400/  475 batches | ms/batch 76.90 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 36.59s | training loss  3.78 |
    | end of validation epoch  21 | time: 41.79s | validation loss  3.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 95.57 | loss  3.42 |
| epoch  22 |   200/  475 batches | ms/batch 83.16 | loss  3.24 |
| epoch  22 |   300/  475 batches | ms/batch 78.93 | loss  3.36 |
| epoch  22 |   400/  475 batches | ms/batch 76.81 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 36.59s | training loss  3.71 |
    | end of validation epoch  22 | time: 41.40s | validation loss  3.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 91.66 | loss  3.34 |
| epoch  23 |   200/  475 batches | ms/batch 82.52 | loss  3.94 |
| epoch  23 |   300/  475 batches | ms/batch 78.14 | loss  3.40 |
| epoch  23 |   400/  475 batches | ms/batch 76.35 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 36.30s | training loss  3.71 |
    | end of validation epoch  23 | time: 41.61s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 92.16 | loss  3.73 |
| epoch  24 |   200/  475 batches | ms/batch 81.63 | loss  3.57 |
| epoch  24 |   300/  475 batches | ms/batch 78.12 | loss  3.53 |
| epoch  24 |   400/  475 batches | ms/batch 76.36 | loss  3.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 36.36s | training loss  3.68 |
    | end of validation epoch  24 | time: 41.98s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 93.48 | loss  3.18 |
| epoch  25 |   200/  475 batches | ms/batch 82.29 | loss  3.64 |
| epoch  25 |   300/  475 batches | ms/batch 78.31 | loss  3.78 |
| epoch  25 |   400/  475 batches | ms/batch 76.45 | loss  4.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 36.54s | training loss  3.65 |
    | end of validation epoch  25 | time: 41.89s | validation loss  2.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 91.72 | loss  3.81 |
| epoch  26 |   200/  475 batches | ms/batch 82.12 | loss  3.63 |
| epoch  26 |   300/  475 batches | ms/batch 78.34 | loss  4.15 |
| epoch  26 |   400/  475 batches | ms/batch 76.70 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 36.54s | training loss  3.63 |
    | end of validation epoch  26 | time: 42.43s | validation loss  2.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 94.74 | loss  3.20 |
| epoch  27 |   200/  475 batches | ms/batch 83.16 | loss  3.89 |
| epoch  27 |   300/  475 batches | ms/batch 79.27 | loss  3.59 |
| epoch  27 |   400/  475 batches | ms/batch 77.19 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 36.73s | training loss  3.62 |
    | end of validation epoch  27 | time: 42.53s | validation loss  3.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 295.34 | loss  3.83 |
| epoch  28 |   200/  475 batches | ms/batch 183.30 | loss  3.62 |
| epoch  28 |   300/  475 batches | ms/batch 145.66 | loss  3.65 |
| epoch  28 |   400/  475 batches | ms/batch 126.94 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 56.66s | training loss  3.59 |
    | end of validation epoch  28 | time: 41.37s | validation loss  2.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 94.15 | loss  3.62 |
| epoch  29 |   200/  475 batches | ms/batch 83.17 | loss  3.50 |
| epoch  29 |   300/  475 batches | ms/batch 79.20 | loss  3.44 |
| epoch  29 |   400/  475 batches | ms/batch 77.25 | loss  3.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 36.61s | training loss  3.57 |
    | end of validation epoch  29 | time: 41.51s | validation loss  2.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 92.79 | loss  3.56 |
| epoch  30 |   200/  475 batches | ms/batch 81.42 | loss  3.48 |
| epoch  30 |   300/  475 batches | ms/batch 84.58 | loss  3.59 |
| epoch  30 |   400/  475 batches | ms/batch 78.62 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 37.20s | training loss  3.56 |
    | end of validation epoch  30 | time: 42.00s | validation loss  2.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 95.24 | loss  3.22 |
| epoch  31 |   200/  475 batches | ms/batch 83.69 | loss  3.67 |
| epoch  31 |   300/  475 batches | ms/batch 79.38 | loss  3.59 |
| epoch  31 |   400/  475 batches | ms/batch 76.91 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 36.56s | training loss  3.54 |
    | end of validation epoch  31 | time: 42.02s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 96.40 | loss  3.26 |
| epoch  32 |   200/  475 batches | ms/batch 83.35 | loss  3.63 |
| epoch  32 |   300/  475 batches | ms/batch 79.91 | loss  3.35 |
| epoch  32 |   400/  475 batches | ms/batch 77.48 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 36.93s | training loss  3.52 |
    | end of validation epoch  32 | time: 41.26s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 265.04 | loss  3.54 |
| epoch  33 |   200/  475 batches | ms/batch 168.08 | loss  3.31 |
| epoch  33 |   300/  475 batches | ms/batch 136.20 | loss  3.72 |
| epoch  33 |   400/  475 batches | ms/batch 119.98 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 53.90s | training loss  3.52 |
    | end of validation epoch  33 | time: 36.63s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 92.92 | loss  3.92 |
| epoch  34 |   200/  475 batches | ms/batch 81.87 | loss  3.25 |
| epoch  34 |   300/  475 batches | ms/batch 78.22 | loss  3.68 |
| epoch  34 |   400/  475 batches | ms/batch 76.64 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 36.63s | training loss  3.49 |
    | end of validation epoch  34 | time: 36.60s | validation loss  2.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 94.08 | loss  3.50 |
| epoch  35 |   200/  475 batches | ms/batch 82.60 | loss  3.75 |
| epoch  35 |   300/  475 batches | ms/batch 79.06 | loss  4.11 |
| epoch  35 |   400/  475 batches | ms/batch 77.61 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 36.99s | training loss  3.48 |
    | end of validation epoch  35 | time: 36.06s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 131.91 | loss  3.54 |
| epoch  36 |   200/  475 batches | ms/batch 100.86 | loss  3.22 |
| epoch  36 |   300/  475 batches | ms/batch 91.16 | loss  2.99 |
| epoch  36 |   400/  475 batches | ms/batch 86.62 | loss  3.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 40.59s | training loss  3.47 |
    | end of validation epoch  36 | time: 36.62s | validation loss  2.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 93.73 | loss  3.72 |
| epoch  37 |   200/  475 batches | ms/batch 81.89 | loss  3.26 |
| epoch  37 |   300/  475 batches | ms/batch 78.46 | loss  3.29 |
| epoch  37 |   400/  475 batches | ms/batch 76.80 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 36.82s | training loss  3.47 |
    | end of validation epoch  37 | time: 36.45s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 96.04 | loss  3.61 |
| epoch  38 |   200/  475 batches | ms/batch 83.37 | loss  3.52 |
| epoch  38 |   300/  475 batches | ms/batch 79.62 | loss  3.64 |
| epoch  38 |   400/  475 batches | ms/batch 77.79 | loss  3.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 36.99s | training loss  3.45 |
    | end of validation epoch  38 | time: 36.03s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 92.67 | loss  3.75 |
| epoch  39 |   200/  475 batches | ms/batch 91.53 | loss  3.59 |
| epoch  39 |   300/  475 batches | ms/batch 84.69 | loss  3.55 |
| epoch  39 |   400/  475 batches | ms/batch 81.95 | loss  3.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 39.91s | training loss  3.42 |
    | end of validation epoch  39 | time: 36.25s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 94.10 | loss  3.04 |
| epoch  40 |   200/  475 batches | ms/batch 82.70 | loss  3.49 |
| epoch  40 |   300/  475 batches | ms/batch 79.50 | loss  3.76 |
| epoch  40 |   400/  475 batches | ms/batch 77.37 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 36.91s | training loss  3.41 |
    | end of validation epoch  40 | time: 35.59s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 95.37 | loss  3.99 |
| epoch  41 |   200/  475 batches | ms/batch 83.83 | loss  3.55 |
| epoch  41 |   300/  475 batches | ms/batch 79.60 | loss  3.72 |
| epoch  41 |   400/  475 batches | ms/batch 78.00 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 37.24s | training loss  3.42 |
    | end of validation epoch  41 | time: 35.65s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494]
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 90.91 | loss  3.29 |
| epoch  42 |   200/  475 batches | ms/batch 81.36 | loss  3.63 |
| epoch  42 |   300/  475 batches | ms/batch 78.00 | loss  3.90 |
| epoch  42 |   400/  475 batches | ms/batch 76.45 | loss  2.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 36.73s | training loss  3.39 |
    | end of validation epoch  42 | time: 35.56s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 91.93 | loss  3.26 |
| epoch  43 |   200/  475 batches | ms/batch 81.29 | loss  3.28 |
| epoch  43 |   300/  475 batches | ms/batch 78.66 | loss  2.70 |
| epoch  43 |   400/  475 batches | ms/batch 76.63 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 36.67s | training loss  3.38 |
    | end of validation epoch  43 | time: 37.58s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 91.30 | loss  3.44 |
| epoch  44 |   200/  475 batches | ms/batch 80.98 | loss  3.16 |
| epoch  44 |   300/  475 batches | ms/batch 78.05 | loss  3.33 |
| epoch  44 |   400/  475 batches | ms/batch 76.39 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 36.45s | training loss  3.37 |
    | end of validation epoch  44 | time: 35.84s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 91.84 | loss  3.56 |
| epoch  45 |   200/  475 batches | ms/batch 92.84 | loss  3.77 |
| epoch  45 |   300/  475 batches | ms/batch 82.24 | loss  3.49 |
| epoch  45 |   400/  475 batches | ms/batch 79.69 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 37.73s | training loss  3.35 |
    | end of validation epoch  45 | time: 34.92s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 93.04 | loss  3.04 |
| epoch  46 |   200/  475 batches | ms/batch 81.88 | loss  3.82 |
| epoch  46 |   300/  475 batches | ms/batch 78.68 | loss  3.90 |
| epoch  46 |   400/  475 batches | ms/batch 77.61 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 36.92s | training loss  3.35 |
    | end of validation epoch  46 | time: 35.33s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 92.16 | loss  3.63 |
| epoch  47 |   200/  475 batches | ms/batch 81.98 | loss  3.06 |
| epoch  47 |   300/  475 batches | ms/batch 78.88 | loss  3.70 |
| epoch  47 |   400/  475 batches | ms/batch 77.29 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 36.80s | training loss  3.32 |
    | end of validation epoch  47 | time: 35.48s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 95.20 | loss  3.15 |
| epoch  48 |   200/  475 batches | ms/batch 83.39 | loss  3.46 |
| epoch  48 |   300/  475 batches | ms/batch 79.97 | loss  3.30 |
| epoch  48 |   400/  475 batches | ms/batch 78.13 | loss  3.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 37.24s | training loss  3.34 |
    | end of validation epoch  48 | time: 34.16s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809]
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 92.64 | loss  3.63 |
| epoch  49 |   200/  475 batches | ms/batch 81.30 | loss  3.19 |
| epoch  49 |   300/  475 batches | ms/batch 78.61 | loss  3.51 |
| epoch  49 |   400/  475 batches | ms/batch 77.19 | loss  3.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 36.71s | training loss  3.32 |
    | end of validation epoch  49 | time: 34.49s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 93.45 | loss  2.53 |
| epoch  50 |   200/  475 batches | ms/batch 82.23 | loss  3.33 |
| epoch  50 |   300/  475 batches | ms/batch 78.84 | loss  3.31 |
| epoch  50 |   400/  475 batches | ms/batch 76.91 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 36.73s | training loss  3.31 |
    | end of validation epoch  50 | time: 34.63s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 91.79 | loss  3.19 |
| epoch  51 |   200/  475 batches | ms/batch 81.31 | loss  2.93 |
| epoch  51 |   300/  475 batches | ms/batch 78.86 | loss  3.43 |
| epoch  51 |   400/  475 batches | ms/batch 77.85 | loss  3.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 37.15s | training loss  3.30 |
    | end of validation epoch  51 | time: 34.60s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 92.88 | loss  3.37 |
| epoch  52 |   200/  475 batches | ms/batch 82.78 | loss  3.77 |
| epoch  52 |   300/  475 batches | ms/batch 79.86 | loss  3.53 |
| epoch  52 |   400/  475 batches | ms/batch 78.08 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 37.77s | training loss  3.28 |
    | end of validation epoch  52 | time: 34.44s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 92.49 | loss  3.30 |
| epoch  53 |   200/  475 batches | ms/batch 83.53 | loss  2.99 |
| epoch  53 |   300/  475 batches | ms/batch 79.47 | loss  3.14 |
| epoch  53 |   400/  475 batches | ms/batch 77.78 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 36.81s | training loss  3.28 |
    | end of validation epoch  53 | time: 34.10s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 92.03 | loss  3.16 |
| epoch  54 |   200/  475 batches | ms/batch 82.59 | loss  3.27 |
| epoch  54 |   300/  475 batches | ms/batch 78.86 | loss  3.08 |
| epoch  54 |   400/  475 batches | ms/batch 77.50 | loss  2.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 36.92s | training loss  3.27 |
    | end of validation epoch  54 | time: 34.05s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 93.11 | loss  3.25 |
| epoch  55 |   200/  475 batches | ms/batch 81.72 | loss  3.76 |
| epoch  55 |   300/  475 batches | ms/batch 78.21 | loss  3.14 |
| epoch  55 |   400/  475 batches | ms/batch 76.58 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 36.54s | training loss  3.25 |
    | end of validation epoch  55 | time: 33.81s | validation loss  2.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 93.57 | loss  3.47 |
| epoch  56 |   200/  475 batches | ms/batch 82.16 | loss  2.93 |
| epoch  56 |   300/  475 batches | ms/batch 79.25 | loss  3.41 |
| epoch  56 |   400/  475 batches | ms/batch 77.50 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 36.99s | training loss  3.24 |
    | end of validation epoch  56 | time: 34.07s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 90.69 | loss  3.04 |
| epoch  57 |   200/  475 batches | ms/batch 81.25 | loss  2.85 |
| epoch  57 |   300/  475 batches | ms/batch 78.70 | loss  3.13 |
| epoch  57 |   400/  475 batches | ms/batch 77.54 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 36.99s | training loss  3.22 |
    | end of validation epoch  57 | time: 39.91s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 161.62 | loss  3.41 |
| epoch  58 |   200/  475 batches | ms/batch 116.99 | loss  3.20 |
| epoch  58 |   300/  475 batches | ms/batch 101.57 | loss  3.27 |
| epoch  58 |   400/  475 batches | ms/batch 93.75 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 43.37s | training loss  3.23 |
    | end of validation epoch  58 | time: 33.93s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 93.64 | loss  3.46 |
| epoch  59 |   200/  475 batches | ms/batch 83.06 | loss  2.92 |
| epoch  59 |   300/  475 batches | ms/batch 79.34 | loss  3.26 |
| epoch  59 |   400/  475 batches | ms/batch 77.56 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 36.85s | training loss  3.23 |
    | end of validation epoch  59 | time: 34.33s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193]
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 93.11 | loss  3.34 |
| epoch  60 |   200/  475 batches | ms/batch 83.48 | loss  3.50 |
| epoch  60 |   300/  475 batches | ms/batch 79.15 | loss  3.17 |
| epoch  60 |   400/  475 batches | ms/batch 77.07 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 36.66s | training loss  3.21 |
    | end of validation epoch  60 | time: 33.94s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 92.36 | loss  3.13 |
| epoch  61 |   200/  475 batches | ms/batch 81.75 | loss  3.02 |
| epoch  61 |   300/  475 batches | ms/batch 78.38 | loss  3.20 |
| epoch  61 |   400/  475 batches | ms/batch 76.30 | loss  3.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 36.30s | training loss  3.22 |
    | end of validation epoch  61 | time: 34.22s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302]
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 93.46 | loss  3.02 |
| epoch  62 |   200/  475 batches | ms/batch 83.74 | loss  3.13 |
| epoch  62 |   300/  475 batches | ms/batch 79.78 | loss  3.30 |
| epoch  62 |   400/  475 batches | ms/batch 77.76 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 37.03s | training loss  3.21 |
    | end of validation epoch  62 | time: 34.86s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 92.94 | loss  3.66 |
| epoch  63 |   200/  475 batches | ms/batch 83.06 | loss  3.30 |
| epoch  63 |   300/  475 batches | ms/batch 79.46 | loss  3.07 |
| epoch  63 |   400/  475 batches | ms/batch 77.19 | loss  3.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 36.78s | training loss  3.20 |
    | end of validation epoch  63 | time: 36.59s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 150.23 | loss  3.08 |
| epoch  64 |   200/  475 batches | ms/batch 111.85 | loss  3.12 |
| epoch  64 |   300/  475 batches | ms/batch 98.07 | loss  2.84 |
| epoch  64 |   400/  475 batches | ms/batch 91.42 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 42.40s | training loss  3.18 |
    | end of validation epoch  64 | time: 36.78s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 94.42 | loss  2.93 |
| epoch  65 |   200/  475 batches | ms/batch 82.55 | loss  3.39 |
| epoch  65 |   300/  475 batches | ms/batch 78.43 | loss  3.37 |
| epoch  65 |   400/  475 batches | ms/batch 76.73 | loss  3.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 36.68s | training loss  3.18 |
    | end of validation epoch  65 | time: 37.09s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 93.99 | loss  3.04 |
| epoch  66 |   200/  475 batches | ms/batch 82.42 | loss  3.14 |
| epoch  66 |   300/  475 batches | ms/batch 77.97 | loss  3.10 |
| epoch  66 |   400/  475 batches | ms/batch 76.45 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 36.62s | training loss  3.17 |
    | end of validation epoch  66 | time: 37.01s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 94.10 | loss  3.17 |
| epoch  67 |   200/  475 batches | ms/batch 83.26 | loss  3.38 |
| epoch  67 |   300/  475 batches | ms/batch 79.50 | loss  3.01 |
| epoch  67 |   400/  475 batches | ms/batch 77.87 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 37.07s | training loss  3.17 |
    | end of validation epoch  67 | time: 40.53s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 155.25 | loss  3.41 |
| epoch  68 |   200/  475 batches | ms/batch 113.32 | loss  3.15 |
| epoch  68 |   300/  475 batches | ms/batch 99.55 | loss  3.37 |
| epoch  68 |   400/  475 batches | ms/batch 92.19 | loss  4.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 42.74s | training loss  3.17 |
    | end of validation epoch  68 | time: 40.06s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 92.44 | loss  2.99 |
| epoch  69 |   200/  475 batches | ms/batch 82.27 | loss  3.35 |
| epoch  69 |   300/  475 batches | ms/batch 78.78 | loss  3.20 |
| epoch  69 |   400/  475 batches | ms/batch 77.20 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 36.74s | training loss  3.15 |
    | end of validation epoch  69 | time: 39.78s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 92.03 | loss  3.49 |
| epoch  70 |   200/  475 batches | ms/batch 82.11 | loss  2.86 |
| epoch  70 |   300/  475 batches | ms/batch 78.70 | loss  3.12 |
| epoch  70 |   400/  475 batches | ms/batch 76.66 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 36.49s | training loss  3.14 |
    | end of validation epoch  70 | time: 40.28s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 93.65 | loss  2.91 |
| epoch  71 |   200/  475 batches | ms/batch 83.20 | loss  2.91 |
| epoch  71 |   300/  475 batches | ms/batch 78.86 | loss  2.82 |
| epoch  71 |   400/  475 batches | ms/batch 76.79 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 36.61s | training loss  3.13 |
    | end of validation epoch  71 | time: 39.90s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 92.38 | loss  2.96 |
| epoch  72 |   200/  475 batches | ms/batch 82.77 | loss  3.49 |
| epoch  72 |   300/  475 batches | ms/batch 78.60 | loss  3.31 |
| epoch  72 |   400/  475 batches | ms/batch 76.83 | loss  2.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 36.63s | training loss  3.12 |
    | end of validation epoch  72 | time: 40.85s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 91.54 | loss  3.23 |
| epoch  73 |   200/  475 batches | ms/batch 80.74 | loss  3.30 |
| epoch  73 |   300/  475 batches | ms/batch 77.58 | loss  3.34 |
| epoch  73 |   400/  475 batches | ms/batch 76.04 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 36.29s | training loss  3.12 |
    | end of validation epoch  73 | time: 41.81s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 147.48 | loss  2.60 |
| epoch  74 |   200/  475 batches | ms/batch 108.80 | loss  3.46 |
| epoch  74 |   300/  475 batches | ms/batch 95.92 | loss  3.03 |
| epoch  74 |   400/  475 batches | ms/batch 89.83 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 41.77s | training loss  3.12 |
    | end of validation epoch  74 | time: 41.32s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 90.98 | loss  3.20 |
| epoch  75 |   200/  475 batches | ms/batch 81.32 | loss  3.03 |
| epoch  75 |   300/  475 batches | ms/batch 77.87 | loss  2.99 |
| epoch  75 |   400/  475 batches | ms/batch 75.69 | loss  3.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 36.12s | training loss  3.10 |
    | end of validation epoch  75 | time: 41.76s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 93.05 | loss  2.72 |
| epoch  76 |   200/  475 batches | ms/batch 82.21 | loss  3.02 |
| epoch  76 |   300/  475 batches | ms/batch 78.65 | loss  2.99 |
| epoch  76 |   400/  475 batches | ms/batch 76.73 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 36.44s | training loss  3.11 |
    | end of validation epoch  76 | time: 41.30s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 91.82 | loss  2.93 |
| epoch  77 |   200/  475 batches | ms/batch 81.66 | loss  3.44 |
| epoch  77 |   300/  475 batches | ms/batch 78.38 | loss  2.93 |
| epoch  77 |   400/  475 batches | ms/batch 76.57 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 36.30s | training loss  3.09 |
    | end of validation epoch  77 | time: 41.97s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 92.53 | loss  3.23 |
| epoch  78 |   200/  475 batches | ms/batch 81.95 | loss  3.32 |
| epoch  78 |   300/  475 batches | ms/batch 77.57 | loss  2.90 |
| epoch  78 |   400/  475 batches | ms/batch 77.18 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 36.08s | training loss  3.07 |
    | end of validation epoch  78 | time: 41.31s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 92.57 | loss  3.25 |
| epoch  79 |   200/  475 batches | ms/batch 82.60 | loss  3.01 |
| epoch  79 |   300/  475 batches | ms/batch 78.79 | loss  2.93 |
| epoch  79 |   400/  475 batches | ms/batch 76.59 | loss  3.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 36.41s | training loss  3.08 |
    | end of validation epoch  79 | time: 41.17s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 92.01 | loss  2.96 |
| epoch  80 |   200/  475 batches | ms/batch 82.04 | loss  2.90 |
| epoch  80 |   300/  475 batches | ms/batch 78.38 | loss  3.49 |
| epoch  80 |   400/  475 batches | ms/batch 76.47 | loss  2.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 36.42s | training loss  3.07 |
    | end of validation epoch  80 | time: 41.47s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 94.42 | loss  3.01 |
| epoch  81 |   200/  475 batches | ms/batch 81.84 | loss  2.44 |
| epoch  81 |   300/  475 batches | ms/batch 78.85 | loss  2.95 |
| epoch  81 |   400/  475 batches | ms/batch 76.71 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 36.47s | training loss  3.06 |
    | end of validation epoch  81 | time: 41.41s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 92.34 | loss  3.08 |
| epoch  82 |   200/  475 batches | ms/batch 81.87 | loss  2.89 |
| epoch  82 |   300/  475 batches | ms/batch 77.96 | loss  2.52 |
| epoch  82 |   400/  475 batches | ms/batch 76.01 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 36.37s | training loss  3.06 |
    | end of validation epoch  82 | time: 40.90s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 91.86 | loss  2.54 |
| epoch  83 |   200/  475 batches | ms/batch 81.44 | loss  2.82 |
| epoch  83 |   300/  475 batches | ms/batch 77.27 | loss  2.80 |
| epoch  83 |   400/  475 batches | ms/batch 75.77 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 36.14s | training loss  3.05 |
    | end of validation epoch  83 | time: 41.41s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 92.48 | loss  3.15 |
| epoch  84 |   200/  475 batches | ms/batch 81.03 | loss  2.95 |
| epoch  84 |   300/  475 batches | ms/batch 78.25 | loss  3.11 |
| epoch  84 |   400/  475 batches | ms/batch 76.15 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 36.14s | training loss  3.04 |
    | end of validation epoch  84 | time: 41.01s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 91.97 | loss  2.59 |
| epoch  85 |   200/  475 batches | ms/batch 81.41 | loss  3.66 |
| epoch  85 |   300/  475 batches | ms/batch 78.71 | loss  3.19 |
| epoch  85 |   400/  475 batches | ms/batch 76.31 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 36.36s | training loss  3.05 |
    | end of validation epoch  85 | time: 40.39s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 90.88 | loss  2.58 |
| epoch  86 |   200/  475 batches | ms/batch 80.87 | loss  2.91 |
| epoch  86 |   300/  475 batches | ms/batch 77.39 | loss  2.83 |
| epoch  86 |   400/  475 batches | ms/batch 75.41 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 36.01s | training loss  3.03 |
    | end of validation epoch  86 | time: 33.79s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 112.37 | loss  3.26 |
| epoch  87 |   200/  475 batches | ms/batch 90.98 | loss  2.61 |
| epoch  87 |   300/  475 batches | ms/batch 83.68 | loss  3.38 |
| epoch  87 |   400/  475 batches | ms/batch 80.05 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 37.93s | training loss  3.04 |
    | end of validation epoch  87 | time: 31.82s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 91.08 | loss  3.02 |
| epoch  88 |   200/  475 batches | ms/batch 80.84 | loss  3.28 |
| epoch  88 |   300/  475 batches | ms/batch 77.58 | loss  3.03 |
| epoch  88 |   400/  475 batches | ms/batch 75.84 | loss  2.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 36.28s | training loss  3.00 |
    | end of validation epoch  88 | time: 32.94s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 99.35 | loss  3.10 |
| epoch  89 |   200/  475 batches | ms/batch 84.74 | loss  2.86 |
| epoch  89 |   300/  475 batches | ms/batch 79.61 | loss  2.86 |
| epoch  89 |   400/  475 batches | ms/batch 77.13 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 36.64s | training loss  3.02 |
    | end of validation epoch  89 | time: 32.02s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 97.91 | loss  2.48 |
| epoch  90 |   200/  475 batches | ms/batch 83.73 | loss  2.96 |
| epoch  90 |   300/  475 batches | ms/batch 79.47 | loss  2.57 |
| epoch  90 |   400/  475 batches | ms/batch 77.89 | loss  2.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 36.68s | training loss  2.99 |
    | end of validation epoch  90 | time: 32.54s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 95.63 | loss  2.99 |
| epoch  91 |   200/  475 batches | ms/batch 83.11 | loss  3.10 |
| epoch  91 |   300/  475 batches | ms/batch 78.39 | loss  2.91 |
| epoch  91 |   400/  475 batches | ms/batch 76.11 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 36.14s | training loss  2.99 |
    | end of validation epoch  91 | time: 32.47s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 94.37 | loss  3.38 |
| epoch  92 |   200/  475 batches | ms/batch 81.48 | loss  3.16 |
| epoch  92 |   300/  475 batches | ms/batch 77.76 | loss  3.38 |
| epoch  92 |   400/  475 batches | ms/batch 78.34 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 36.07s | training loss  3.00 |
    | end of validation epoch  92 | time: 32.81s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 92.45 | loss  3.18 |
| epoch  93 |   200/  475 batches | ms/batch 81.03 | loss  2.94 |
| epoch  93 |   300/  475 batches | ms/batch 98.16 | loss  2.96 |
| epoch  93 |   400/  475 batches | ms/batch 93.07 | loss  3.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 42.25s | training loss  2.99 |
    | end of validation epoch  93 | time: 58.92s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 94.97 | loss  3.16 |
| epoch  94 |   200/  475 batches | ms/batch 82.86 | loss  3.38 |
| epoch  94 |   300/  475 batches | ms/batch 78.55 | loss  3.07 |
| epoch  94 |   400/  475 batches | ms/batch 76.35 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 36.27s | training loss  2.99 |
    | end of validation epoch  94 | time: 32.14s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 93.14 | loss  3.13 |
| epoch  95 |   200/  475 batches | ms/batch 82.03 | loss  2.96 |
| epoch  95 |   300/  475 batches | ms/batch 81.58 | loss  3.29 |
| epoch  95 |   400/  475 batches | ms/batch 78.68 | loss  2.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 37.21s | training loss  2.98 |
    | end of validation epoch  95 | time: 32.72s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207, 2.9786598687422905] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266, 2.533104961659728]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 90.81 | loss  3.07 |
| epoch  96 |   200/  475 batches | ms/batch 80.18 | loss  3.00 |
| epoch  96 |   300/  475 batches | ms/batch 76.90 | loss  2.86 |
| epoch  96 |   400/  475 batches | ms/batch 75.07 | loss  3.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 35.83s | training loss  2.96 |
    | end of validation epoch  96 | time: 32.19s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207, 2.9786598687422905, 2.9611191548799214] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266, 2.533104961659728, 2.503634722292924]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 93.10 | loss  3.19 |
| epoch  97 |   200/  475 batches | ms/batch 81.91 | loss  2.74 |
| epoch  97 |   300/  475 batches | ms/batch 77.83 | loss  3.18 |
| epoch  97 |   400/  475 batches | ms/batch 79.56 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 37.70s | training loss  2.98 |
    | end of validation epoch  97 | time: 32.20s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207, 2.9786598687422905, 2.9611191548799214, 2.975996033517938] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266, 2.533104961659728, 2.503634722292924, 2.5198411230279616]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 90.85 | loss  2.92 |
| epoch  98 |   200/  475 batches | ms/batch 80.60 | loss  2.70 |
| epoch  98 |   300/  475 batches | ms/batch 77.15 | loss  3.48 |
| epoch  98 |   400/  475 batches | ms/batch 74.96 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 35.89s | training loss  2.96 |
    | end of validation epoch  98 | time: 32.30s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207, 2.9786598687422905, 2.9611191548799214, 2.975996033517938, 2.958650709453382] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266, 2.533104961659728, 2.503634722292924, 2.5198411230279616, 2.515384629994881]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 92.12 | loss  2.77 |
| epoch  99 |   200/  475 batches | ms/batch 80.40 | loss  3.01 |
| epoch  99 |   300/  475 batches | ms/batch 76.93 | loss  3.60 |
| epoch  99 |   400/  475 batches | ms/batch 75.42 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 35.95s | training loss  2.96 |
    | end of validation epoch  99 | time: 33.61s | validation loss  2.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.779148289529901, 5.311436365027177, 5.041350533334833, 4.8217259216308594, 4.675768329218815, 4.53732459419652, 4.425294225592363, 4.341909499419363, 4.261022354427137, 4.194240551496807, 4.134803599809345, 4.0886185967294795, 4.029594018835771, 3.9887374175222297, 3.940095594305741, 3.9001432353571843, 3.8684972763061523, 3.835772704576191, 3.826932561774003, 3.778674169841566, 3.776180386292307, 3.714506486089606, 3.7149904110557155, 3.6786593743374474, 3.653617999930131, 3.630894895854749, 3.6191204909274455, 3.588794033652858, 3.573020289571662, 3.5637440525858026, 3.5443560891402397, 3.5240715172416284, 3.523826066569278, 3.4893267681724147, 3.482505013817235, 3.4700470121283282, 3.4710913075898824, 3.450289480811671, 3.424190159345928, 3.406453144675807, 3.4206730390849867, 3.393803829393889, 3.380983307487086, 3.37033360581649, 3.351692633879812, 3.348477251655177, 3.3207572911915024, 3.339344774547376, 3.3192496455343146, 3.306414946505898, 3.2951784761328446, 3.277601153223138, 3.279143790194863, 3.267193306872719, 3.2506974421049417, 3.238823948408428, 3.2218172821245696, 3.2311918173338237, 3.2314101093693783, 3.2132306917090165, 3.2186453447843855, 3.209746232283743, 3.1970420119636938, 3.1794059060749253, 3.182616922478927, 3.1683921442533793, 3.1668158024235775, 3.1712038120470547, 3.154865517867239, 3.1433661671688684, 3.1329737984506707, 3.1229820386986984, 3.1191832130833674, 3.1198300045414973, 3.095065820593583, 3.1135622752340217, 3.0934749231840435, 3.072635996969123, 3.081452458030299, 3.06769035540129, 3.0598160999699644, 3.0627686741477564, 3.0517468552840383, 3.0386540834527267, 3.0507883232518247, 3.025411390505339, 3.0352817038485878, 3.00390758213244, 3.0210455713774027, 2.991324072888023, 2.9915167833629406, 3.0042594282250654, 2.9930624429803148, 2.988837767651207, 2.9786598687422905, 2.9611191548799214, 2.975996033517938, 2.958650709453382, 2.963291579798648] validation loss is  [5.314779506010168, 4.808883001824387, 4.469616913995823, 4.2245851324385955, 4.0870166886754395, 3.9619119728312775, 3.7880672166327467, 3.6744372443992552, 3.6049320958241693, 3.5280775242492934, 3.527569730742639, 3.383530442454234, 3.358396920837274, 3.3351388358268417, 3.3171430495606753, 3.2326185082187173, 3.1520429198481454, 3.1553091542059635, 3.0941248601224243, 3.1110618114471436, 3.0512173035565544, 3.044977087934478, 3.022098108499992, 3.0229672684389004, 2.9419421989376806, 2.9887628114524007, 3.0366634961937655, 2.9687192880806803, 2.9438763105568766, 2.9563114823413494, 2.872038328347086, 2.868838296217077, 2.8417377932732846, 2.8641408531605697, 2.8405125581917643, 2.8966960025434734, 2.7675020314064347, 2.782693390084916, 2.818364351737399, 2.748175120153347, 2.8684305243131494, 2.754322484761727, 2.7837153222380566, 2.770934297257111, 2.699672514651002, 2.737368463468151, 2.688354335913137, 2.731095306011809, 2.6924007099215723, 2.7043104191788103, 2.6763829884408903, 2.7029539877627076, 2.674079684650197, 2.6526601665160237, 2.709972059025484, 2.6699250185189127, 2.61064963080302, 2.585045485937295, 2.6520401990714193, 2.612997466776551, 2.654192900457302, 2.5967388764149, 2.61198432205104, 2.6036603220370638, 2.5973334883441446, 2.6071434361594066, 2.6067258329952465, 2.6040871714343545, 2.5809488707229873, 2.6355275065959, 2.57113593967021, 2.5950222836823023, 2.543464970187981, 2.6000855319640217, 2.582413180535581, 2.5695560659681047, 2.5940880364730576, 2.6225981942745817, 2.5535416803440127, 2.5220866233360866, 2.532162350766799, 2.51588108158913, 2.530494531663526, 2.525981885044515, 2.5172747844407537, 2.5342723992692324, 2.574556350708008, 2.517662778621962, 2.57698944336226, 2.503201263291495, 2.5448613647653273, 2.5313681684622242, 2.4881888067021087, 2.516244728024266, 2.533104961659728, 2.503634722292924, 2.5198411230279616, 2.515384629994881, 2.4444069832312962]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
