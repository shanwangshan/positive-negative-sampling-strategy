/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 3156.42 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 336.22s | training loss  3.12 |
    | end of validation epoch   1 | time: 129.06s | validation loss  2.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [3.1218397767694146] validation loss is  [2.1604266315698624]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 3068.23 | loss  3.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 328.38s | training loss  2.72 |
    | end of validation epoch   2 | time: 122.95s | validation loss  1.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [3.1218397767694146, 2.7172949249679976] validation loss is  [2.1604266315698624, 1.9431880513827007]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 3117.40 | loss  2.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 331.23s | training loss  2.55 |
    | end of validation epoch   3 | time: 118.93s | validation loss  1.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 3094.99 | loss  2.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 329.47s | training loss  2.37 |
    | end of validation epoch   4 | time: 122.33s | validation loss  1.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 3080.38 | loss  2.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 330.06s | training loss  2.28 |
    | end of validation epoch   5 | time: 120.29s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 3074.36 | loss  2.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 330.26s | training loss  2.16 |
    | end of validation epoch   6 | time: 119.38s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 3083.59 | loss  1.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 330.84s | training loss  2.08 |
    | end of validation epoch   7 | time: 119.64s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 3148.71 | loss  1.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 334.09s | training loss  2.03 |
    | end of validation epoch   8 | time: 120.21s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 3114.44 | loss  1.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 333.00s | training loss  1.96 |
    | end of validation epoch   9 | time: 120.15s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 3109.10 | loss  1.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 331.25s | training loss  1.91 |
    | end of validation epoch  10 | time: 119.05s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 3134.83 | loss  2.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 331.85s | training loss  1.86 |
    | end of validation epoch  11 | time: 119.47s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 3095.47 | loss  1.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 331.04s | training loss  1.82 |
    | end of validation epoch  12 | time: 118.35s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 3073.41 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 330.72s | training loss  1.81 |
    | end of validation epoch  13 | time: 120.58s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 3106.29 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 332.02s | training loss  1.76 |
    | end of validation epoch  14 | time: 118.36s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 3133.13 | loss  2.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 334.11s | training loss  1.75 |
    | end of validation epoch  15 | time: 120.02s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 3094.14 | loss  1.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 334.53s | training loss  1.71 |
    | end of validation epoch  16 | time: 124.97s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 3110.98 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 331.49s | training loss  1.69 |
    | end of validation epoch  17 | time: 117.79s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 3115.87 | loss  1.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 332.63s | training loss  1.65 |
    | end of validation epoch  18 | time: 119.16s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 3101.31 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 330.80s | training loss  1.65 |
    | end of validation epoch  19 | time: 119.33s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 3116.26 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 331.56s | training loss  1.64 |
    | end of validation epoch  20 | time: 118.53s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 3101.65 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 330.69s | training loss  1.62 |
    | end of validation epoch  21 | time: 118.91s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 3104.33 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 332.31s | training loss  1.59 |
    | end of validation epoch  22 | time: 118.24s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 3098.16 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 331.53s | training loss  1.60 |
    | end of validation epoch  23 | time: 118.15s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122]
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 3111.10 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 331.82s | training loss  1.60 |
    | end of validation epoch  24 | time: 119.56s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 3097.07 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 331.34s | training loss  1.58 |
    | end of validation epoch  25 | time: 118.89s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 3117.74 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 332.33s | training loss  1.56 |
    | end of validation epoch  26 | time: 119.83s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 3129.37 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 332.77s | training loss  1.55 |
    | end of validation epoch  27 | time: 118.84s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 3109.74 | loss  1.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 331.62s | training loss  1.54 |
    | end of validation epoch  28 | time: 120.15s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 3133.23 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 332.29s | training loss  1.53 |
    | end of validation epoch  29 | time: 119.08s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 3107.46 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 332.43s | training loss  1.53 |
    | end of validation epoch  30 | time: 119.23s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 3106.98 | loss  1.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 331.97s | training loss  1.54 |
    | end of validation epoch  31 | time: 121.03s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535]
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 3138.55 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 336.02s | training loss  1.51 |
    | end of validation epoch  32 | time: 118.60s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 3105.41 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 332.73s | training loss  1.49 |
    | end of validation epoch  33 | time: 118.50s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 3106.76 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 333.17s | training loss  1.50 |
    | end of validation epoch  34 | time: 119.03s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 3115.69 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 332.07s | training loss  1.50 |
    | end of validation epoch  35 | time: 119.30s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503]
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 3117.04 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 332.56s | training loss  1.52 |
    | end of validation epoch  36 | time: 119.15s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816]
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 3111.83 | loss  1.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 332.16s | training loss  1.49 |
    | end of validation epoch  37 | time: 119.18s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 3130.11 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 332.05s | training loss  1.49 |
    | end of validation epoch  38 | time: 118.84s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 3108.74 | loss  1.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 332.31s | training loss  1.50 |
    | end of validation epoch  39 | time: 119.78s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 3093.66 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 332.44s | training loss  1.48 |
    | end of validation epoch  40 | time: 119.21s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 3108.31 | loss  1.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 331.72s | training loss  1.48 |
    | end of validation epoch  41 | time: 119.73s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 3117.41 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 332.33s | training loss  1.49 |
    | end of validation epoch  42 | time: 119.35s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 3111.84 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 332.39s | training loss  1.49 |
    | end of validation epoch  43 | time: 118.67s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 3105.79 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 332.08s | training loss  1.48 |
    | end of validation epoch  44 | time: 117.91s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 3104.14 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 331.83s | training loss  1.47 |
    | end of validation epoch  45 | time: 119.20s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 3091.99 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 331.58s | training loss  1.48 |
    | end of validation epoch  46 | time: 118.34s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 3119.89 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 332.58s | training loss  1.47 |
    | end of validation epoch  47 | time: 119.76s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 3158.34 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 336.59s | training loss  1.47 |
    | end of validation epoch  48 | time: 119.04s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 3108.39 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 332.06s | training loss  1.46 |
    | end of validation epoch  49 | time: 119.05s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 3115.91 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 331.82s | training loss  1.47 |
    | end of validation epoch  50 | time: 120.19s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 3107.40 | loss  1.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 331.49s | training loss  1.47 |
    | end of validation epoch  51 | time: 119.54s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 3106.32 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 331.29s | training loss  1.46 |
    | end of validation epoch  52 | time: 119.03s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 3117.42 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 332.64s | training loss  1.47 |
    | end of validation epoch  53 | time: 119.20s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 3098.12 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 330.90s | training loss  1.46 |
    | end of validation epoch  54 | time: 118.80s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 3106.42 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 330.50s | training loss  1.46 |
    | end of validation epoch  55 | time: 119.86s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 3094.97 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 329.95s | training loss  1.45 |
    | end of validation epoch  56 | time: 119.04s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 3083.83 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 329.45s | training loss  1.46 |
    | end of validation epoch  57 | time: 119.09s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 3101.27 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 331.09s | training loss  1.45 |
    | end of validation epoch  58 | time: 118.00s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 3087.31 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 330.32s | training loss  1.45 |
    | end of validation epoch  59 | time: 118.96s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 3079.87 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 330.27s | training loss  1.46 |
    | end of validation epoch  60 | time: 118.77s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 3098.09 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 331.66s | training loss  1.46 |
    | end of validation epoch  61 | time: 118.00s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 3093.82 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 330.83s | training loss  1.46 |
    | end of validation epoch  62 | time: 118.97s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 3123.97 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 332.30s | training loss  1.44 |
    | end of validation epoch  63 | time: 119.89s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 3160.05 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 336.52s | training loss  1.46 |
    | end of validation epoch  64 | time: 119.37s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 3108.03 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 331.14s | training loss  1.45 |
    | end of validation epoch  65 | time: 119.42s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 3102.27 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 330.78s | training loss  1.46 |
    | end of validation epoch  66 | time: 120.19s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 3089.43 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 330.91s | training loss  1.45 |
    | end of validation epoch  67 | time: 118.50s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 3103.94 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 332.21s | training loss  1.44 |
    | end of validation epoch  68 | time: 119.04s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 3108.49 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 331.67s | training loss  1.45 |
    | end of validation epoch  69 | time: 119.25s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 3099.61 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 332.54s | training loss  1.45 |
    | end of validation epoch  70 | time: 119.82s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 3128.97 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 332.95s | training loss  1.44 |
    | end of validation epoch  71 | time: 118.02s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 3106.06 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 331.46s | training loss  1.44 |
    | end of validation epoch  72 | time: 118.74s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 3103.11 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 331.62s | training loss  1.45 |
    | end of validation epoch  73 | time: 118.89s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 3082.89 | loss  1.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 331.15s | training loss  1.46 |
    | end of validation epoch  74 | time: 119.69s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 3109.93 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 331.09s | training loss  1.46 |
    | end of validation epoch  75 | time: 119.57s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 3094.78 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 331.18s | training loss  1.46 |
    | end of validation epoch  76 | time: 118.27s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 3085.96 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 331.50s | training loss  1.45 |
    | end of validation epoch  77 | time: 118.70s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 3109.06 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 331.41s | training loss  1.45 |
    | end of validation epoch  78 | time: 119.10s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 3116.71 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 331.88s | training loss  1.44 |
    | end of validation epoch  79 | time: 118.93s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 3135.89 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 335.09s | training loss  1.45 |
    | end of validation epoch  80 | time: 119.27s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 3093.76 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 331.78s | training loss  1.46 |
    | end of validation epoch  81 | time: 119.09s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 3110.33 | loss  1.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 331.88s | training loss  1.43 |
    | end of validation epoch  82 | time: 119.60s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 3134.12 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 332.12s | training loss  1.46 |
    | end of validation epoch  83 | time: 119.64s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 3104.00 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 331.42s | training loss  1.44 |
    | end of validation epoch  84 | time: 120.38s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 3120.63 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 331.38s | training loss  1.45 |
    | end of validation epoch  85 | time: 119.47s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 3110.23 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 331.09s | training loss  1.46 |
    | end of validation epoch  86 | time: 120.20s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 3098.41 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 331.81s | training loss  1.45 |
    | end of validation epoch  87 | time: 119.16s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 3090.12 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 331.68s | training loss  1.46 |
    | end of validation epoch  88 | time: 118.75s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 3080.32 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 330.63s | training loss  1.43 |
    | end of validation epoch  89 | time: 119.24s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 3098.96 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 330.95s | training loss  1.43 |
    | end of validation epoch  90 | time: 119.31s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 3123.61 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 331.98s | training loss  1.45 |
    | end of validation epoch  91 | time: 119.51s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 3093.61 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 332.40s | training loss  1.45 |
    | end of validation epoch  92 | time: 132.72s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 3224.42 | loss  1.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 341.54s | training loss  1.45 |
    | end of validation epoch  93 | time: 118.12s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 3089.97 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 331.14s | training loss  1.45 |
    | end of validation epoch  94 | time: 120.11s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 3129.85 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 331.46s | training loss  1.46 |
    | end of validation epoch  95 | time: 119.44s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803, 1.4576538274954032] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806, 1.148332160897553]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 3151.20 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 336.34s | training loss  1.46 |
    | end of validation epoch  96 | time: 120.58s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803, 1.4576538274954032, 1.4555937674668458] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806, 1.148332160897553, 1.1531510408967733]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 3101.37 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 331.14s | training loss  1.46 |
    | end of validation epoch  97 | time: 120.60s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803, 1.4576538274954032, 1.4555937674668458, 1.463547167477307] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806, 1.148332160897553, 1.1531510408967733, 1.155228537817796]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 3109.11 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 330.87s | training loss  1.45 |
    | end of validation epoch  98 | time: 120.34s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803, 1.4576538274954032, 1.4555937674668458, 1.463547167477307, 1.4547397413769283] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806, 1.148332160897553, 1.1531510408967733, 1.155228537817796, 1.1623695533101757]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 3099.99 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 330.58s | training loss  1.45 |
    | end of validation epoch  99 | time: 119.37s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [3.1218397767694146, 2.7172949249679976, 2.553797668164915, 2.369211918598897, 2.2766670053069658, 2.1602831445298754, 2.08404329016402, 2.0289781673534497, 1.9580777786873482, 1.9100077141512621, 1.8642312631950722, 1.8248587846755981, 1.8127130495535362, 1.7603878577550252, 1.7547404186145679, 1.7059233242327028, 1.69134770964717, 1.6546640868659492, 1.654809313851434, 1.644514978468955, 1.6217333497227848, 1.5930270128421955, 1.6031562390628162, 1.601989473308529, 1.578911572963268, 1.555784110550408, 1.5518553922842215, 1.539390654177279, 1.5262708578023825, 1.5346852895375844, 1.536123117885074, 1.5146752542203612, 1.4914032081225972, 1.5007799595325917, 1.4996080999975805, 1.5169161611849122, 1.4901777967676386, 1.4894831481280628, 1.5033752703451895, 1.478254782186972, 1.4800057679683238, 1.4885783174016454, 1.4925358563930065, 1.481270358369157, 1.4702060888479422, 1.4775161947216, 1.471998767809825, 1.4708365416741587, 1.4605775330517743, 1.4674824078877766, 1.4706498803319157, 1.462636534158174, 1.470585018664867, 1.4592284468917158, 1.45700128121419, 1.4548956435005944, 1.455401678343077, 1.4491629546827025, 1.4484656383325387, 1.4581407093786978, 1.4643960289053015, 1.4576801856358845, 1.4400182704667788, 1.4615629331485644, 1.4471830978049889, 1.4596669942409068, 1.4510294327864777, 1.4429840021305256, 1.4547826008753733, 1.451616465508401, 1.4445036488610346, 1.4414026071359445, 1.4482703047829706, 1.4627088179459442, 1.4597284514624793, 1.4595106969008576, 1.4495929468859423, 1.449309471491221, 1.435844536300178, 1.4451682707210918, 1.4576271063572652, 1.4327601419912803, 1.4570039897351652, 1.438514272371928, 1.4511229207923821, 1.4584375652107033, 1.454664633080766, 1.4646629760931205, 1.4345073377763904, 1.4308031432263486, 1.45074412199828, 1.449397439355249, 1.4452917189211458, 1.451366822998803, 1.4576538274954032, 1.4555937674668458, 1.463547167477307, 1.4547397413769283, 1.4517315486529927] validation loss is  [2.1604266315698624, 1.9431880513827007, 1.7876885707179706, 1.6705374742547672, 1.5790591537952423, 1.529883049428463, 1.4663494278987248, 1.4229179496566455, 1.4062641349931557, 1.3751778919249773, 1.3493984726568062, 1.3367965426295996, 1.3184914619972308, 1.3209533461680014, 1.301175429796179, 1.3014348031332095, 1.270633276551962, 1.2825058087085683, 1.2651805318892002, 1.2397026879092057, 1.2526633186886709, 1.2278736277172964, 1.2323666717857122, 1.2154349889606237, 1.214538876588146, 1.2207851186394691, 1.2172371121123433, 1.2166149100909631, 1.2072944538667798, 1.1999326758086681, 1.2151616600652535, 1.2080821168298523, 1.2020444860681891, 1.1910159314672153, 1.1919950532416503, 1.203236433987816, 1.1927935338268678, 1.1900043323015173, 1.1864345629389088, 1.1846182700246572, 1.186045251165827, 1.179638272151351, 1.1732743605971336, 1.1723626675084233, 1.1790133966132998, 1.1711002370963495, 1.1705841061969597, 1.1783005306497216, 1.1859752424061298, 1.1682606730610132, 1.1722317521149914, 1.163314846965174, 1.1585122166822355, 1.170420864596963, 1.170826370517413, 1.184684111115833, 1.1695829449842374, 1.1642667759830754, 1.1575199207291007, 1.1831530276685953, 1.1671325163915753, 1.1625502274061243, 1.1613220733900864, 1.156611136160791, 1.1713656016315024, 1.1601286775742967, 1.1473964424803853, 1.159339268381397, 1.1725633752842743, 1.1549307654301326, 1.1516731089601915, 1.1637584948912263, 1.1671711128825943, 1.15241530071944, 1.1538964258506894, 1.158940293515722, 1.1763811527440946, 1.1533944448456168, 1.1528047450507681, 1.1726228076343734, 1.155811669304967, 1.1515717102835576, 1.1624778024852276, 1.1599097872773807, 1.1487761090199153, 1.1617832897851865, 1.1573598741864164, 1.1686726169039805, 1.1569730872288346, 1.167050334935387, 1.166963132719199, 1.1444567277406652, 1.151628262673815, 1.1547650586192806, 1.148332160897553, 1.1531510408967733, 1.155228537817796, 1.1623695533101757, 1.158609762787819]
