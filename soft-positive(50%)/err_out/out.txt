/run/nvme/job_2957379/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 309, 'ckp_path': './checkpoint/', 'vgg_path': '/vgg-sound/', 'unwanted_files_path': '../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64}
use_cude True
all together the number of training files is 169683
total number of training files is 38007
total number of training files is 38007
Let's use 4 GPUs!
Directory  ./checkpoint/  already exists
this is epoch 0
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   0 |   100/  123 batches | ms/batch 7021.87 | loss  5.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   0 | time: 798.90s | training loss  5.74 |
[5.737029114389808]
this is epoch 1
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   1 |   100/  123 batches | ms/batch 6777.68 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 788.87s | training loss  5.67 |
[5.737029114389808, 5.6679070092798245]
this is epoch 2
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   2 |   100/  123 batches | ms/batch 6962.26 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 781.79s | training loss  5.62 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154]
this is epoch 3
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   3 |   100/  123 batches | ms/batch 7044.33 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 793.82s | training loss  5.58 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755]
this is epoch 4
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   4 |   100/  123 batches | ms/batch 6880.36 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 780.46s | training loss  5.57 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954]
this is epoch 5
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   5 |   100/  123 batches | ms/batch 6899.18 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 777.22s | training loss  5.54 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976]
this is epoch 6
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   6 |   100/  123 batches | ms/batch 6936.05 | loss  5.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 783.15s | training loss  5.52 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206]
this is epoch 7
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   7 |   100/  123 batches | ms/batch 6883.78 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 777.54s | training loss  5.50 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363]
this is epoch 8
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   8 |   100/  123 batches | ms/batch 6916.77 | loss  5.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 797.36s | training loss  5.49 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045]
this is epoch 9
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   9 |   100/  123 batches | ms/batch 6942.73 | loss  5.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 779.80s | training loss  5.46 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927]
this is epoch 10
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  10 |   100/  123 batches | ms/batch 6956.45 | loss  5.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 780.09s | training loss  5.44 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638]
this is epoch 11
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  11 |   100/  123 batches | ms/batch 7035.53 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 786.31s | training loss  5.43 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335]
this is epoch 12
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  12 |   100/  123 batches | ms/batch 7162.87 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 803.67s | training loss  5.41 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554]
this is epoch 13
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  13 |   100/  123 batches | ms/batch 6872.36 | loss  5.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 787.94s | training loss  5.39 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773]
this is epoch 14
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  14 |   100/  123 batches | ms/batch 6888.11 | loss  5.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 795.88s | training loss  5.37 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043]
this is epoch 15
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  15 |   100/  123 batches | ms/batch 6834.38 | loss  5.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 764.52s | training loss  5.35 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373]
this is epoch 16
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  16 |   100/  123 batches | ms/batch 6929.57 | loss  5.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 776.10s | training loss  5.35 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336]
this is epoch 17
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  17 |   100/  123 batches | ms/batch 6834.97 | loss  5.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 768.49s | training loss  5.33 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926]
this is epoch 18
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  18 |   100/  123 batches | ms/batch 6893.12 | loss  5.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 782.70s | training loss  5.31 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955]
this is epoch 19
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  19 |   100/  123 batches | ms/batch 6913.47 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 779.66s | training loss  5.29 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266]
this is epoch 20
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  20 |   100/  123 batches | ms/batch 6897.11 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 788.70s | training loss  5.27 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361]
this is epoch 21
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  21 |   100/  123 batches | ms/batch 7122.55 | loss  5.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 795.24s | training loss  5.26 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689]
this is epoch 22
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  22 |   100/  123 batches | ms/batch 6985.18 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 786.06s | training loss  5.24 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193]
this is epoch 23
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  23 |   100/  123 batches | ms/batch 6914.96 | loss  5.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 778.99s | training loss  5.21 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097]
this is epoch 24
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  24 |   100/  123 batches | ms/batch 6883.34 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 770.17s | training loss  5.20 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268]
this is epoch 25
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  25 |   100/  123 batches | ms/batch 6914.05 | loss  5.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 787.10s | training loss  5.18 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459]
this is epoch 26
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  26 |   100/  123 batches | ms/batch 6914.82 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 784.86s | training loss  5.16 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875]
this is epoch 27
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  27 |   100/  123 batches | ms/batch 7108.27 | loss  5.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 790.10s | training loss  5.15 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237]
this is epoch 28
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  28 |   100/  123 batches | ms/batch 6891.38 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 783.10s | training loss  5.13 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992]
this is epoch 29
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  29 |   100/  123 batches | ms/batch 6883.99 | loss  5.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 779.24s | training loss  5.12 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581]
this is epoch 30
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  30 |   100/  123 batches | ms/batch 6757.29 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 778.16s | training loss  5.09 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844]
this is epoch 31
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  31 |   100/  123 batches | ms/batch 6922.97 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 773.64s | training loss  5.08 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381]
this is epoch 32
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  32 |   100/  123 batches | ms/batch 6900.94 | loss  5.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 778.84s | training loss  5.07 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815]
this is epoch 33
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  33 |   100/  123 batches | ms/batch 6849.33 | loss  5.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 767.67s | training loss  5.05 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926]
this is epoch 34
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  34 |   100/  123 batches | ms/batch 6732.73 | loss  5.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 776.95s | training loss  5.02 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281]
this is epoch 35
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  35 |   100/  123 batches | ms/batch 6984.76 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 787.97s | training loss  5.01 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255]
this is epoch 36
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  36 |   100/  123 batches | ms/batch 7038.82 | loss  4.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 786.83s | training loss  5.00 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439]
this is epoch 37
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  37 |   100/  123 batches | ms/batch 6902.96 | loss  4.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 778.15s | training loss  4.98 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581]
this is epoch 38
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  38 |   100/  123 batches | ms/batch 6901.70 | loss  4.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 777.36s | training loss  4.96 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489]
this is epoch 39
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  39 |   100/  123 batches | ms/batch 6933.93 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 781.72s | training loss  4.96 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004]
this is epoch 40
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  40 |   100/  123 batches | ms/batch 6888.25 | loss  4.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 783.03s | training loss  4.94 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245]
this is epoch 41
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  41 |   100/  123 batches | ms/batch 6889.94 | loss  4.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 795.07s | training loss  4.92 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596]
this is epoch 42
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  42 |   100/  123 batches | ms/batch 6934.23 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 783.81s | training loss  4.91 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365]
this is epoch 43
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  43 |   100/  123 batches | ms/batch 7158.65 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 807.19s | training loss  4.88 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411]
this is epoch 44
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  44 |   100/  123 batches | ms/batch 6802.65 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 772.08s | training loss  4.87 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548]
this is epoch 45
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  45 |   100/  123 batches | ms/batch 6909.21 | loss  4.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 786.68s | training loss  4.85 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755]
this is epoch 46
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  46 |   100/  123 batches | ms/batch 6980.66 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 782.82s | training loss  4.84 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074]
this is epoch 47
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  47 |   100/  123 batches | ms/batch 7091.27 | loss  4.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 794.04s | training loss  4.82 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703]
this is epoch 48
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  48 |   100/  123 batches | ms/batch 6944.57 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 791.55s | training loss  4.81 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097]
this is epoch 49
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  49 |   100/  123 batches | ms/batch 6887.25 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 781.64s | training loss  4.79 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095]
this is epoch 50
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  50 |   100/  123 batches | ms/batch 6934.05 | loss  4.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 779.94s | training loss  4.79 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713]
this is epoch 51
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  51 |   100/  123 batches | ms/batch 6811.93 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 778.19s | training loss  4.76 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617]
this is epoch 52
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  52 |   100/  123 batches | ms/batch 6962.99 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 785.45s | training loss  4.75 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185]
this is epoch 53
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  53 |   100/  123 batches | ms/batch 7116.85 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 797.02s | training loss  4.74 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343]
this is epoch 54
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  54 |   100/  123 batches | ms/batch 6852.15 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 787.76s | training loss  4.71 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139]
this is epoch 55
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  55 |   100/  123 batches | ms/batch 6878.13 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 789.72s | training loss  4.72 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786]
this is epoch 56
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  56 |   100/  123 batches | ms/batch 6971.02 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 775.84s | training loss  4.69 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486]
this is epoch 57
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  57 |   100/  123 batches | ms/batch 6758.47 | loss  4.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 773.98s | training loss  4.66 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392]
this is epoch 58
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  58 |   100/  123 batches | ms/batch 6917.69 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 781.32s | training loss  4.68 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701]
this is epoch 59
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  59 |   100/  123 batches | ms/batch 6940.39 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 784.99s | training loss  4.66 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282]
this is epoch 60
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  60 |   100/  123 batches | ms/batch 6958.14 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 787.87s | training loss  4.64 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838]
this is epoch 61
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  61 |   100/  123 batches | ms/batch 6920.33 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 774.77s | training loss  4.61 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331]
this is epoch 62
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  62 |   100/  123 batches | ms/batch 6938.37 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 787.92s | training loss  4.61 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725]
this is epoch 63
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  63 |   100/  123 batches | ms/batch 6866.98 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 785.81s | training loss  4.61 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435]
this is epoch 64
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  64 |   100/  123 batches | ms/batch 7128.28 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 793.47s | training loss  4.58 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134]
this is epoch 65
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  65 |   100/  123 batches | ms/batch 6897.70 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 775.45s | training loss  4.57 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932]
this is epoch 66
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  66 |   100/  123 batches | ms/batch 7016.99 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 786.85s | training loss  4.56 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557]
this is epoch 67
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  67 |   100/  123 batches | ms/batch 7017.64 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 791.34s | training loss  4.55 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735]
this is epoch 68
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  68 |   100/  123 batches | ms/batch 6928.64 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 779.09s | training loss  4.54 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374]
this is epoch 69
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  69 |   100/  123 batches | ms/batch 6992.93 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 784.88s | training loss  4.52 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205]
this is epoch 70
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  70 |   100/  123 batches | ms/batch 6880.28 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 784.14s | training loss  4.51 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541]
this is epoch 71
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  71 |   100/  123 batches | ms/batch 6853.32 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 783.95s | training loss  4.49 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139]
this is epoch 72
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  72 |   100/  123 batches | ms/batch 7014.99 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 791.33s | training loss  4.47 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723]
this is epoch 73
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  73 |   100/  123 batches | ms/batch 6883.80 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 781.17s | training loss  4.48 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543]
this is epoch 74
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  74 |   100/  123 batches | ms/batch 6951.33 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 785.09s | training loss  4.46 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639]
this is epoch 75
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  75 |   100/  123 batches | ms/batch 7012.84 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 782.79s | training loss  4.47 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385]
this is epoch 76
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  76 |   100/  123 batches | ms/batch 6828.51 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 787.82s | training loss  4.44 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163]
this is epoch 77
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  77 |   100/  123 batches | ms/batch 6918.59 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 781.50s | training loss  4.42 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327]
this is epoch 78
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  78 |   100/  123 batches | ms/batch 6924.32 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 791.10s | training loss  4.42 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933]
this is epoch 79
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  79 |   100/  123 batches | ms/batch 6999.93 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 783.02s | training loss  4.41 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193]
this is epoch 80
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  80 |   100/  123 batches | ms/batch 7001.58 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 788.40s | training loss  4.40 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134]
this is epoch 81
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  81 |   100/  123 batches | ms/batch 6868.38 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 769.57s | training loss  4.38 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376]
this is epoch 82
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  82 |   100/  123 batches | ms/batch 6942.98 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 778.33s | training loss  4.37 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735]
this is epoch 83
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  83 |   100/  123 batches | ms/batch 6884.80 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 776.44s | training loss  4.37 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199]
this is epoch 84
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  84 |   100/  123 batches | ms/batch 6932.28 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 780.05s | training loss  4.35 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344]
this is epoch 85
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  85 |   100/  123 batches | ms/batch 7033.46 | loss  4.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 791.33s | training loss  4.34 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342]
this is epoch 86
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  86 |   100/  123 batches | ms/batch 7024.83 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 785.51s | training loss  4.32 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129]
this is epoch 87
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  87 |   100/  123 batches | ms/batch 6842.43 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 775.47s | training loss  4.32 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844]
this is epoch 88
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  88 |   100/  123 batches | ms/batch 6921.43 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 781.22s | training loss  4.30 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095]
this is epoch 89
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  89 |   100/  123 batches | ms/batch 6904.74 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 795.91s | training loss  4.28 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136]
this is epoch 90
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  90 |   100/  123 batches | ms/batch 6931.41 | loss  4.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 775.38s | training loss  4.29 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299]
this is epoch 91
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  91 |   100/  123 batches | ms/batch 6954.87 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 782.26s | training loss  4.28 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946]
this is epoch 92
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  92 |   100/  123 batches | ms/batch 7049.67 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 785.47s | training loss  4.25 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788]
this is epoch 93
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  93 |   100/  123 batches | ms/batch 7114.99 | loss  4.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 784.60s | training loss  4.25 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771]
this is epoch 94
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  94 |   100/  123 batches | ms/batch 6806.49 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 774.94s | training loss  4.24 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795]
this is epoch 95
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  95 |   100/  123 batches | ms/batch 6953.91 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 785.27s | training loss  4.22 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795, 4.221993987153216]
this is epoch 96
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  96 |   100/  123 batches | ms/batch 7017.42 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 780.11s | training loss  4.23 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795, 4.221993987153216, 4.226444259891665]
this is epoch 97
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  97 |   100/  123 batches | ms/batch 6905.14 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 781.93s | training loss  4.22 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795, 4.221993987153216, 4.226444259891665, 4.215760808650071]
this is epoch 98
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  98 |   100/  123 batches | ms/batch 6909.51 | loss  4.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 796.42s | training loss  4.20 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795, 4.221993987153216, 4.226444259891665, 4.215760808650071, 4.202749302716759]
this is epoch 99
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  99 |   100/  123 batches | ms/batch 6975.76 | loss  4.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 781.67s | training loss  4.21 |
[5.737029114389808, 5.6679070092798245, 5.618968060346154, 5.5843534043164755, 5.565486372970954, 5.541080172468976, 5.517209607411206, 5.497962032876363, 5.4850902712442045, 5.461108471319927, 5.436697653638638, 5.427996701341335, 5.41125730576554, 5.387957956732773, 5.372739764733043, 5.354289283597373, 5.346260640679336, 5.325899329611926, 5.309357255454955, 5.293633309806266, 5.27319237856361, 5.256509063689689, 5.236364531323193, 5.210917011509097, 5.197631331963268, 5.183858216293459, 5.164156906003875, 5.151721679098237, 5.132589875197992, 5.119974361202581, 5.089673585038844, 5.082505167984381, 5.065118409753815, 5.047544932946926, 5.02148833701281, 5.014269309315255, 4.999200460387439, 4.983476348039581, 4.959289166985489, 4.960948025308004, 4.9384602414883245, 4.922987228486596, 4.9079004574597365, 4.878218220501411, 4.870229379917548, 4.8497988615578755, 4.835113533144074, 4.819801439114703, 4.80663935731097, 4.7862993372165095, 4.785228973481713, 4.758547294430617, 4.7472827105018185, 4.743135863203343, 4.713937030575139, 4.716961930437786, 4.692584774358486, 4.664196153966392, 4.682428383245701, 4.656152562397282, 4.63559441837838, 4.613919812489331, 4.6138774709003725, 4.613835241736435, 4.575505210132134, 4.573844529748932, 4.557247227769557, 4.551020843226735, 4.53523887851374, 4.5202453543500205, 4.510007869906541, 4.486029636569139, 4.469479650016723, 4.48199945930543, 4.457305520530639, 4.465509872126385, 4.435371224473163, 4.418889200784327, 4.420690974568933, 4.405065703198193, 4.398469172842134, 4.376741583754376, 4.3737963816014735, 4.372220074258199, 4.348383876366344, 4.340913528349342, 4.322550808511129, 4.315178630797844, 4.301159668744095, 4.284377384961136, 4.287393992509299, 4.27642510576946, 4.249909501734788, 4.247186827465771, 4.243531114686795, 4.221993987153216, 4.226444259891665, 4.215760808650071, 4.202749302716759, 4.207334981701238]
