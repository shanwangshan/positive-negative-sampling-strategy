/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_sou_prob_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 166.59 | loss  6.75 |
| epoch   1 |   200/  475 batches | ms/batch 152.67 | loss  6.79 |
| epoch   1 |   300/  475 batches | ms/batch 147.88 | loss  6.67 |
| epoch   1 |   400/  475 batches | ms/batch 145.14 | loss  6.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 68.14s | training loss  6.81 |
    | end of validation epoch   1 | time: 52.67s | validation loss  5.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.806852164017527] validation loss is  [5.493682853314055]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 154.12 | loss  6.57 |
| epoch   2 |   200/  475 batches | ms/batch 141.76 | loss  6.46 |
| epoch   2 |   300/  475 batches | ms/batch 139.32 | loss  6.04 |
| epoch   2 |   400/  475 batches | ms/batch 137.75 | loss  6.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 65.44s | training loss  6.33 |
    | end of validation epoch   2 | time: 51.80s | validation loss  5.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.806852164017527, 6.330485973358154] validation loss is  [5.493682853314055, 5.163561227942715]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 157.93 | loss  5.88 |
| epoch   3 |   200/  475 batches | ms/batch 144.49 | loss  6.26 |
| epoch   3 |   300/  475 batches | ms/batch 141.54 | loss  5.67 |
| epoch   3 |   400/  475 batches | ms/batch 139.89 | loss  5.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 66.82s | training loss  6.00 |
    | end of validation epoch   3 | time: 52.38s | validation loss  4.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 157.79 | loss  5.86 |
| epoch   4 |   200/  475 batches | ms/batch 147.07 | loss  5.61 |
| epoch   4 |   300/  475 batches | ms/batch 143.03 | loss  5.74 |
| epoch   4 |   400/  475 batches | ms/batch 140.70 | loss  5.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 67.36s | training loss  5.75 |
    | end of validation epoch   4 | time: 51.90s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 159.18 | loss  5.56 |
| epoch   5 |   200/  475 batches | ms/batch 145.89 | loss  5.55 |
| epoch   5 |   300/  475 batches | ms/batch 141.19 | loss  5.39 |
| epoch   5 |   400/  475 batches | ms/batch 138.96 | loss  5.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 66.16s | training loss  5.54 |
    | end of validation epoch   5 | time: 52.67s | validation loss  4.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 161.66 | loss  5.32 |
| epoch   6 |   200/  475 batches | ms/batch 148.20 | loss  5.36 |
| epoch   6 |   300/  475 batches | ms/batch 143.45 | loss  5.56 |
| epoch   6 |   400/  475 batches | ms/batch 141.49 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 67.51s | training loss  5.40 |
    | end of validation epoch   6 | time: 51.66s | validation loss  4.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 159.83 | loss  5.37 |
| epoch   7 |   200/  475 batches | ms/batch 147.08 | loss  5.36 |
| epoch   7 |   300/  475 batches | ms/batch 143.68 | loss  5.01 |
| epoch   7 |   400/  475 batches | ms/batch 141.30 | loss  5.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 67.04s | training loss  5.25 |
    | end of validation epoch   7 | time: 51.95s | validation loss  4.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 156.10 | loss  4.95 |
| epoch   8 |   200/  475 batches | ms/batch 144.52 | loss  5.32 |
| epoch   8 |   300/  475 batches | ms/batch 141.21 | loss  5.00 |
| epoch   8 |   400/  475 batches | ms/batch 140.63 | loss  5.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 67.08s | training loss  5.15 |
    | end of validation epoch   8 | time: 51.87s | validation loss  4.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 160.92 | loss  5.10 |
| epoch   9 |   200/  475 batches | ms/batch 145.33 | loss  5.21 |
| epoch   9 |   300/  475 batches | ms/batch 141.57 | loss  5.40 |
| epoch   9 |   400/  475 batches | ms/batch 139.38 | loss  5.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 66.43s | training loss  5.04 |
    | end of validation epoch   9 | time: 52.49s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 158.60 | loss  5.07 |
| epoch  10 |   200/  475 batches | ms/batch 145.32 | loss  5.00 |
| epoch  10 |   300/  475 batches | ms/batch 141.38 | loss  4.95 |
| epoch  10 |   400/  475 batches | ms/batch 139.01 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 66.45s | training loss  4.96 |
    | end of validation epoch  10 | time: 52.37s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 157.44 | loss  4.80 |
| epoch  11 |   200/  475 batches | ms/batch 143.16 | loss  5.02 |
| epoch  11 |   300/  475 batches | ms/batch 139.62 | loss  4.64 |
| epoch  11 |   400/  475 batches | ms/batch 138.23 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 66.16s | training loss  4.89 |
    | end of validation epoch  11 | time: 51.68s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 157.65 | loss  4.87 |
| epoch  12 |   200/  475 batches | ms/batch 144.74 | loss  4.31 |
| epoch  12 |   300/  475 batches | ms/batch 140.67 | loss  4.92 |
| epoch  12 |   400/  475 batches | ms/batch 138.13 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 66.08s | training loss  4.81 |
    | end of validation epoch  12 | time: 51.86s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 154.88 | loss  5.11 |
| epoch  13 |   200/  475 batches | ms/batch 142.40 | loss  4.60 |
| epoch  13 |   300/  475 batches | ms/batch 138.16 | loss  4.83 |
| epoch  13 |   400/  475 batches | ms/batch 136.64 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 65.66s | training loss  4.77 |
    | end of validation epoch  13 | time: 52.38s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 158.91 | loss  4.07 |
| epoch  14 |   200/  475 batches | ms/batch 144.05 | loss  4.52 |
| epoch  14 |   300/  475 batches | ms/batch 139.11 | loss  4.84 |
| epoch  14 |   400/  475 batches | ms/batch 136.78 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 65.53s | training loss  4.70 |
    | end of validation epoch  14 | time: 50.44s | validation loss  3.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 156.91 | loss  4.69 |
| epoch  15 |   200/  475 batches | ms/batch 142.37 | loss  4.77 |
| epoch  15 |   300/  475 batches | ms/batch 139.25 | loss  4.73 |
| epoch  15 |   400/  475 batches | ms/batch 137.29 | loss  4.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 66.19s | training loss  4.67 |
    | end of validation epoch  15 | time: 49.85s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 156.82 | loss  4.82 |
| epoch  16 |   200/  475 batches | ms/batch 143.40 | loss  4.55 |
| epoch  16 |   300/  475 batches | ms/batch 139.22 | loss  4.79 |
| epoch  16 |   400/  475 batches | ms/batch 137.43 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 65.52s | training loss  4.63 |
    | end of validation epoch  16 | time: 52.04s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 156.29 | loss  4.23 |
| epoch  17 |   200/  475 batches | ms/batch 144.66 | loss  4.63 |
| epoch  17 |   300/  475 batches | ms/batch 139.69 | loss  4.59 |
| epoch  17 |   400/  475 batches | ms/batch 138.08 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 66.08s | training loss  4.59 |
    | end of validation epoch  17 | time: 50.86s | validation loss  3.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 158.27 | loss  4.48 |
| epoch  18 |   200/  475 batches | ms/batch 144.24 | loss  4.73 |
| epoch  18 |   300/  475 batches | ms/batch 139.69 | loss  4.28 |
| epoch  18 |   400/  475 batches | ms/batch 137.59 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 65.42s | training loss  4.56 |
    | end of validation epoch  18 | time: 50.36s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 156.75 | loss  4.46 |
| epoch  19 |   200/  475 batches | ms/batch 143.76 | loss  4.40 |
| epoch  19 |   300/  475 batches | ms/batch 139.55 | loss  4.36 |
| epoch  19 |   400/  475 batches | ms/batch 137.69 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 65.44s | training loss  4.53 |
    | end of validation epoch  19 | time: 51.27s | validation loss  3.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 158.28 | loss  4.50 |
| epoch  20 |   200/  475 batches | ms/batch 142.84 | loss  4.79 |
| epoch  20 |   300/  475 batches | ms/batch 138.89 | loss  4.31 |
| epoch  20 |   400/  475 batches | ms/batch 138.06 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 66.10s | training loss  4.50 |
    | end of validation epoch  20 | time: 50.98s | validation loss  3.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 157.17 | loss  4.76 |
| epoch  21 |   200/  475 batches | ms/batch 143.71 | loss  4.30 |
| epoch  21 |   300/  475 batches | ms/batch 138.98 | loss  4.99 |
| epoch  21 |   400/  475 batches | ms/batch 137.99 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 65.82s | training loss  4.48 |
    | end of validation epoch  21 | time: 50.42s | validation loss  3.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 154.66 | loss  4.36 |
| epoch  22 |   200/  475 batches | ms/batch 142.49 | loss  4.79 |
| epoch  22 |   300/  475 batches | ms/batch 137.89 | loss  4.08 |
| epoch  22 |   400/  475 batches | ms/batch 136.40 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 65.20s | training loss  4.47 |
    | end of validation epoch  22 | time: 50.64s | validation loss  3.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 155.02 | loss  4.42 |
| epoch  23 |   200/  475 batches | ms/batch 144.40 | loss  4.16 |
| epoch  23 |   300/  475 batches | ms/batch 139.79 | loss  4.52 |
| epoch  23 |   400/  475 batches | ms/batch 137.48 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 65.60s | training loss  4.43 |
    | end of validation epoch  23 | time: 51.27s | validation loss  3.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 156.84 | loss  4.68 |
| epoch  24 |   200/  475 batches | ms/batch 143.78 | loss  4.38 |
| epoch  24 |   300/  475 batches | ms/batch 140.24 | loss  4.78 |
| epoch  24 |   400/  475 batches | ms/batch 138.44 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 66.00s | training loss  4.43 |
    | end of validation epoch  24 | time: 50.72s | validation loss  3.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 162.55 | loss  4.10 |
| epoch  25 |   200/  475 batches | ms/batch 145.60 | loss  4.98 |
| epoch  25 |   300/  475 batches | ms/batch 139.29 | loss  4.26 |
| epoch  25 |   400/  475 batches | ms/batch 137.33 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 65.73s | training loss  4.41 |
    | end of validation epoch  25 | time: 50.74s | validation loss  3.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 160.09 | loss  4.55 |
| epoch  26 |   200/  475 batches | ms/batch 143.89 | loss  4.48 |
| epoch  26 |   300/  475 batches | ms/batch 140.28 | loss  4.54 |
| epoch  26 |   400/  475 batches | ms/batch 138.08 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 65.68s | training loss  4.40 |
    | end of validation epoch  26 | time: 51.68s | validation loss  3.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 156.56 | loss  4.22 |
| epoch  27 |   200/  475 batches | ms/batch 143.75 | loss  4.47 |
| epoch  27 |   300/  475 batches | ms/batch 138.65 | loss  4.72 |
| epoch  27 |   400/  475 batches | ms/batch 137.22 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 65.54s | training loss  4.37 |
    | end of validation epoch  27 | time: 51.11s | validation loss  3.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 156.55 | loss  4.52 |
| epoch  28 |   200/  475 batches | ms/batch 143.04 | loss  4.42 |
| epoch  28 |   300/  475 batches | ms/batch 138.42 | loss  4.71 |
| epoch  28 |   400/  475 batches | ms/batch 137.61 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 65.44s | training loss  4.36 |
    | end of validation epoch  28 | time: 51.33s | validation loss  3.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 156.97 | loss  4.46 |
| epoch  29 |   200/  475 batches | ms/batch 144.02 | loss  4.44 |
| epoch  29 |   300/  475 batches | ms/batch 139.12 | loss  4.66 |
| epoch  29 |   400/  475 batches | ms/batch 136.77 | loss  4.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 65.26s | training loss  4.34 |
    | end of validation epoch  29 | time: 51.43s | validation loss  3.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 161.39 | loss  4.34 |
| epoch  30 |   200/  475 batches | ms/batch 145.84 | loss  4.26 |
| epoch  30 |   300/  475 batches | ms/batch 139.74 | loss  4.25 |
| epoch  30 |   400/  475 batches | ms/batch 137.80 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 65.85s | training loss  4.32 |
    | end of validation epoch  30 | time: 50.29s | validation loss  3.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 160.14 | loss  4.63 |
| epoch  31 |   200/  475 batches | ms/batch 145.07 | loss  4.64 |
| epoch  31 |   300/  475 batches | ms/batch 140.44 | loss  4.58 |
| epoch  31 |   400/  475 batches | ms/batch 137.85 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 65.83s | training loss  4.33 |
    | end of validation epoch  31 | time: 50.15s | validation loss  3.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 155.15 | loss  4.18 |
| epoch  32 |   200/  475 batches | ms/batch 144.01 | loss  4.02 |
| epoch  32 |   300/  475 batches | ms/batch 139.22 | loss  4.28 |
| epoch  32 |   400/  475 batches | ms/batch 138.61 | loss  4.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 66.28s | training loss  4.31 |
    | end of validation epoch  32 | time: 51.32s | validation loss  3.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 160.54 | loss  4.49 |
| epoch  33 |   200/  475 batches | ms/batch 144.74 | loss  4.09 |
| epoch  33 |   300/  475 batches | ms/batch 139.48 | loss  4.49 |
| epoch  33 |   400/  475 batches | ms/batch 137.58 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 65.79s | training loss  4.31 |
    | end of validation epoch  33 | time: 51.06s | validation loss  3.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 157.39 | loss  4.54 |
| epoch  34 |   200/  475 batches | ms/batch 144.60 | loss  4.46 |
| epoch  34 |   300/  475 batches | ms/batch 139.70 | loss  4.26 |
| epoch  34 |   400/  475 batches | ms/batch 137.35 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 65.64s | training loss  4.31 |
    | end of validation epoch  34 | time: 49.83s | validation loss  3.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 154.96 | loss  3.74 |
| epoch  35 |   200/  475 batches | ms/batch 143.81 | loss  4.57 |
| epoch  35 |   300/  475 batches | ms/batch 139.54 | loss  4.21 |
| epoch  35 |   400/  475 batches | ms/batch 139.03 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 66.16s | training loss  4.30 |
    | end of validation epoch  35 | time: 51.34s | validation loss  3.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 158.25 | loss  4.47 |
| epoch  36 |   200/  475 batches | ms/batch 144.45 | loss  4.10 |
| epoch  36 |   300/  475 batches | ms/batch 140.67 | loss  3.92 |
| epoch  36 |   400/  475 batches | ms/batch 138.51 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 65.66s | training loss  4.28 |
    | end of validation epoch  36 | time: 51.31s | validation loss  3.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 155.23 | loss  4.33 |
| epoch  37 |   200/  475 batches | ms/batch 143.26 | loss  4.53 |
| epoch  37 |   300/  475 batches | ms/batch 139.60 | loss  4.64 |
| epoch  37 |   400/  475 batches | ms/batch 137.00 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 65.62s | training loss  4.28 |
    | end of validation epoch  37 | time: 52.00s | validation loss  3.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 151.84 | loss  4.07 |
| epoch  38 |   200/  475 batches | ms/batch 140.90 | loss  4.27 |
| epoch  38 |   300/  475 batches | ms/batch 138.05 | loss  4.15 |
| epoch  38 |   400/  475 batches | ms/batch 136.32 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 65.11s | training loss  4.27 |
    | end of validation epoch  38 | time: 51.77s | validation loss  3.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 158.01 | loss  4.22 |
| epoch  39 |   200/  475 batches | ms/batch 144.13 | loss  4.03 |
| epoch  39 |   300/  475 batches | ms/batch 139.17 | loss  3.99 |
| epoch  39 |   400/  475 batches | ms/batch 137.36 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 65.53s | training loss  4.27 |
    | end of validation epoch  39 | time: 52.00s | validation loss  3.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685]
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 154.18 | loss  4.40 |
| epoch  40 |   200/  475 batches | ms/batch 141.70 | loss  4.14 |
| epoch  40 |   300/  475 batches | ms/batch 138.37 | loss  4.55 |
| epoch  40 |   400/  475 batches | ms/batch 138.17 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 65.70s | training loss  4.27 |
    | end of validation epoch  40 | time: 51.42s | validation loss  3.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 156.93 | loss  3.96 |
| epoch  41 |   200/  475 batches | ms/batch 142.42 | loss  4.08 |
| epoch  41 |   300/  475 batches | ms/batch 137.53 | loss  4.15 |
| epoch  41 |   400/  475 batches | ms/batch 135.62 | loss  4.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 64.98s | training loss  4.27 |
    | end of validation epoch  41 | time: 52.90s | validation loss  3.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246]
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 156.77 | loss  4.08 |
| epoch  42 |   200/  475 batches | ms/batch 143.23 | loss  4.15 |
| epoch  42 |   300/  475 batches | ms/batch 138.14 | loss  4.03 |
| epoch  42 |   400/  475 batches | ms/batch 136.73 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 65.49s | training loss  4.26 |
    | end of validation epoch  42 | time: 52.48s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 157.79 | loss  4.54 |
| epoch  43 |   200/  475 batches | ms/batch 145.48 | loss  4.62 |
| epoch  43 |   300/  475 batches | ms/batch 141.01 | loss  3.98 |
| epoch  43 |   400/  475 batches | ms/batch 138.39 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 65.86s | training loss  4.25 |
    | end of validation epoch  43 | time: 52.91s | validation loss  3.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 157.07 | loss  4.14 |
| epoch  44 |   200/  475 batches | ms/batch 144.36 | loss  4.44 |
| epoch  44 |   300/  475 batches | ms/batch 139.89 | loss  3.96 |
| epoch  44 |   400/  475 batches | ms/batch 137.27 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 65.51s | training loss  4.27 |
    | end of validation epoch  44 | time: 50.88s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 156.49 | loss  4.29 |
| epoch  45 |   200/  475 batches | ms/batch 142.74 | loss  4.26 |
| epoch  45 |   300/  475 batches | ms/batch 138.65 | loss  4.13 |
| epoch  45 |   400/  475 batches | ms/batch 137.81 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 65.64s | training loss  4.26 |
    | end of validation epoch  45 | time: 52.14s | validation loss  3.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914]
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 158.69 | loss  4.31 |
| epoch  46 |   200/  475 batches | ms/batch 144.32 | loss  4.44 |
| epoch  46 |   300/  475 batches | ms/batch 141.02 | loss  4.25 |
| epoch  46 |   400/  475 batches | ms/batch 138.82 | loss  4.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 66.41s | training loss  4.24 |
    | end of validation epoch  46 | time: 51.17s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 156.44 | loss  4.36 |
| epoch  47 |   200/  475 batches | ms/batch 142.97 | loss  4.59 |
| epoch  47 |   300/  475 batches | ms/batch 138.99 | loss  4.59 |
| epoch  47 |   400/  475 batches | ms/batch 137.49 | loss  4.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 65.47s | training loss  4.25 |
    | end of validation epoch  47 | time: 51.97s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891]
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 155.54 | loss  4.22 |
| epoch  48 |   200/  475 batches | ms/batch 142.34 | loss  4.32 |
| epoch  48 |   300/  475 batches | ms/batch 138.25 | loss  3.98 |
| epoch  48 |   400/  475 batches | ms/batch 136.63 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 65.41s | training loss  4.25 |
    | end of validation epoch  48 | time: 52.60s | validation loss  3.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647]
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 155.86 | loss  4.12 |
| epoch  49 |   200/  475 batches | ms/batch 143.46 | loss  4.50 |
| epoch  49 |   300/  475 batches | ms/batch 140.26 | loss  4.47 |
| epoch  49 |   400/  475 batches | ms/batch 137.91 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 65.71s | training loss  4.24 |
    | end of validation epoch  49 | time: 50.03s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 158.52 | loss  4.19 |
| epoch  50 |   200/  475 batches | ms/batch 144.52 | loss  4.29 |
| epoch  50 |   300/  475 batches | ms/batch 139.19 | loss  4.35 |
| epoch  50 |   400/  475 batches | ms/batch 136.43 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 64.96s | training loss  4.24 |
    | end of validation epoch  50 | time: 51.86s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 155.05 | loss  4.27 |
| epoch  51 |   200/  475 batches | ms/batch 144.29 | loss  4.06 |
| epoch  51 |   300/  475 batches | ms/batch 139.98 | loss  4.24 |
| epoch  51 |   400/  475 batches | ms/batch 136.97 | loss  4.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 65.44s | training loss  4.24 |
    | end of validation epoch  51 | time: 51.29s | validation loss  3.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844]
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 159.11 | loss  4.25 |
| epoch  52 |   200/  475 batches | ms/batch 143.69 | loss  3.90 |
| epoch  52 |   300/  475 batches | ms/batch 139.97 | loss  4.21 |
| epoch  52 |   400/  475 batches | ms/batch 136.57 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 64.97s | training loss  4.22 |
    | end of validation epoch  52 | time: 50.94s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 155.31 | loss  3.95 |
| epoch  53 |   200/  475 batches | ms/batch 143.50 | loss  3.71 |
| epoch  53 |   300/  475 batches | ms/batch 139.12 | loss  4.08 |
| epoch  53 |   400/  475 batches | ms/batch 138.27 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 65.95s | training loss  4.24 |
    | end of validation epoch  53 | time: 50.93s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084]
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 156.79 | loss  4.36 |
| epoch  54 |   200/  475 batches | ms/batch 145.33 | loss  4.39 |
| epoch  54 |   300/  475 batches | ms/batch 141.68 | loss  4.20 |
| epoch  54 |   400/  475 batches | ms/batch 139.31 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 65.68s | training loss  4.23 |
    | end of validation epoch  54 | time: 51.29s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 156.37 | loss  4.39 |
| epoch  55 |   200/  475 batches | ms/batch 143.53 | loss  4.41 |
| epoch  55 |   300/  475 batches | ms/batch 139.82 | loss  4.53 |
| epoch  55 |   400/  475 batches | ms/batch 137.26 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 65.31s | training loss  4.24 |
    | end of validation epoch  55 | time: 50.19s | validation loss  3.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 153.35 | loss  4.35 |
| epoch  56 |   200/  475 batches | ms/batch 142.14 | loss  4.13 |
| epoch  56 |   300/  475 batches | ms/batch 138.71 | loss  4.21 |
| epoch  56 |   400/  475 batches | ms/batch 137.75 | loss  4.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 65.70s | training loss  4.22 |
    | end of validation epoch  56 | time: 50.37s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 159.22 | loss  4.27 |
| epoch  57 |   200/  475 batches | ms/batch 147.90 | loss  4.22 |
| epoch  57 |   300/  475 batches | ms/batch 142.84 | loss  4.06 |
| epoch  57 |   400/  475 batches | ms/batch 139.06 | loss  4.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 66.41s | training loss  4.23 |
    | end of validation epoch  57 | time: 51.59s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334]
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 160.57 | loss  4.09 |
| epoch  58 |   200/  475 batches | ms/batch 146.48 | loss  4.14 |
| epoch  58 |   300/  475 batches | ms/batch 141.51 | loss  3.93 |
| epoch  58 |   400/  475 batches | ms/batch 139.72 | loss  4.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 66.40s | training loss  4.22 |
    | end of validation epoch  58 | time: 51.13s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801]
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 159.08 | loss  4.30 |
| epoch  59 |   200/  475 batches | ms/batch 145.71 | loss  4.27 |
| epoch  59 |   300/  475 batches | ms/batch 141.19 | loss  4.24 |
| epoch  59 |   400/  475 batches | ms/batch 138.84 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 66.35s | training loss  4.22 |
    | end of validation epoch  59 | time: 52.75s | validation loss  3.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 160.85 | loss  4.20 |
| epoch  60 |   200/  475 batches | ms/batch 148.49 | loss  4.21 |
| epoch  60 |   300/  475 batches | ms/batch 142.63 | loss  4.03 |
| epoch  60 |   400/  475 batches | ms/batch 139.97 | loss  4.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 66.64s | training loss  4.22 |
    | end of validation epoch  60 | time: 52.91s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 152.29 | loss  4.55 |
| epoch  61 |   200/  475 batches | ms/batch 143.20 | loss  4.18 |
| epoch  61 |   300/  475 batches | ms/batch 139.68 | loss  4.24 |
| epoch  61 |   400/  475 batches | ms/batch 137.40 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 65.45s | training loss  4.22 |
    | end of validation epoch  61 | time: 51.22s | validation loss  3.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806]
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 159.19 | loss  3.95 |
| epoch  62 |   200/  475 batches | ms/batch 143.57 | loss  4.06 |
| epoch  62 |   300/  475 batches | ms/batch 139.10 | loss  4.27 |
| epoch  62 |   400/  475 batches | ms/batch 137.05 | loss  4.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 65.57s | training loss  4.23 |
    | end of validation epoch  62 | time: 51.22s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 157.42 | loss  4.67 |
| epoch  63 |   200/  475 batches | ms/batch 143.25 | loss  3.95 |
| epoch  63 |   300/  475 batches | ms/batch 140.31 | loss  4.12 |
| epoch  63 |   400/  475 batches | ms/batch 138.60 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 66.21s | training loss  4.23 |
    | end of validation epoch  63 | time: 51.42s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 156.21 | loss  4.63 |
| epoch  64 |   200/  475 batches | ms/batch 146.67 | loss  3.96 |
| epoch  64 |   300/  475 batches | ms/batch 140.36 | loss  4.24 |
| epoch  64 |   400/  475 batches | ms/batch 139.04 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 65.89s | training loss  4.22 |
    | end of validation epoch  64 | time: 51.60s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595]
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 152.05 | loss  4.37 |
| epoch  65 |   200/  475 batches | ms/batch 142.11 | loss  4.48 |
| epoch  65 |   300/  475 batches | ms/batch 138.85 | loss  4.16 |
| epoch  65 |   400/  475 batches | ms/batch 137.29 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 65.57s | training loss  4.22 |
    | end of validation epoch  65 | time: 50.66s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 159.51 | loss  3.93 |
| epoch  66 |   200/  475 batches | ms/batch 147.23 | loss  4.42 |
| epoch  66 |   300/  475 batches | ms/batch 143.18 | loss  4.05 |
| epoch  66 |   400/  475 batches | ms/batch 140.31 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 66.50s | training loss  4.22 |
    | end of validation epoch  66 | time: 51.29s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 158.31 | loss  4.04 |
| epoch  67 |   200/  475 batches | ms/batch 144.29 | loss  4.49 |
| epoch  67 |   300/  475 batches | ms/batch 140.18 | loss  4.49 |
| epoch  67 |   400/  475 batches | ms/batch 137.49 | loss  3.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 65.59s | training loss  4.22 |
    | end of validation epoch  67 | time: 51.58s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756]
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 155.06 | loss  4.19 |
| epoch  68 |   200/  475 batches | ms/batch 142.20 | loss  4.15 |
| epoch  68 |   300/  475 batches | ms/batch 138.38 | loss  4.00 |
| epoch  68 |   400/  475 batches | ms/batch 137.21 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 65.37s | training loss  4.21 |
    | end of validation epoch  68 | time: 51.20s | validation loss  3.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 159.51 | loss  4.07 |
| epoch  69 |   200/  475 batches | ms/batch 145.32 | loss  3.94 |
| epoch  69 |   300/  475 batches | ms/batch 140.29 | loss  3.73 |
| epoch  69 |   400/  475 batches | ms/batch 137.73 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 65.81s | training loss  4.21 |
    | end of validation epoch  69 | time: 51.21s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 157.40 | loss  4.29 |
| epoch  70 |   200/  475 batches | ms/batch 143.19 | loss  4.29 |
| epoch  70 |   300/  475 batches | ms/batch 139.65 | loss  4.64 |
| epoch  70 |   400/  475 batches | ms/batch 138.20 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 65.69s | training loss  4.21 |
    | end of validation epoch  70 | time: 52.88s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076]
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 154.99 | loss  4.26 |
| epoch  71 |   200/  475 batches | ms/batch 144.19 | loss  4.53 |
| epoch  71 |   300/  475 batches | ms/batch 139.51 | loss  4.11 |
| epoch  71 |   400/  475 batches | ms/batch 136.86 | loss  3.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 65.26s | training loss  4.21 |
    | end of validation epoch  71 | time: 51.03s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 161.72 | loss  4.07 |
| epoch  72 |   200/  475 batches | ms/batch 146.24 | loss  4.21 |
| epoch  72 |   300/  475 batches | ms/batch 140.41 | loss  4.11 |
| epoch  72 |   400/  475 batches | ms/batch 137.62 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 65.66s | training loss  4.23 |
    | end of validation epoch  72 | time: 53.43s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 154.36 | loss  3.86 |
| epoch  73 |   200/  475 batches | ms/batch 142.64 | loss  4.15 |
| epoch  73 |   300/  475 batches | ms/batch 139.78 | loss  4.19 |
| epoch  73 |   400/  475 batches | ms/batch 140.88 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 67.06s | training loss  4.21 |
    | end of validation epoch  73 | time: 52.65s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 157.41 | loss  4.06 |
| epoch  74 |   200/  475 batches | ms/batch 144.81 | loss  4.37 |
| epoch  74 |   300/  475 batches | ms/batch 139.46 | loss  4.67 |
| epoch  74 |   400/  475 batches | ms/batch 137.33 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 65.58s | training loss  4.21 |
    | end of validation epoch  74 | time: 51.63s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 158.50 | loss  4.03 |
| epoch  75 |   200/  475 batches | ms/batch 145.21 | loss  4.38 |
| epoch  75 |   300/  475 batches | ms/batch 139.34 | loss  3.77 |
| epoch  75 |   400/  475 batches | ms/batch 137.47 | loss  3.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 65.35s | training loss  4.23 |
    | end of validation epoch  75 | time: 51.57s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 155.33 | loss  4.39 |
| epoch  76 |   200/  475 batches | ms/batch 142.85 | loss  4.07 |
| epoch  76 |   300/  475 batches | ms/batch 139.32 | loss  4.19 |
| epoch  76 |   400/  475 batches | ms/batch 137.93 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 65.46s | training loss  4.21 |
    | end of validation epoch  76 | time: 50.78s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 156.63 | loss  3.90 |
| epoch  77 |   200/  475 batches | ms/batch 143.71 | loss  3.91 |
| epoch  77 |   300/  475 batches | ms/batch 139.94 | loss  4.29 |
| epoch  77 |   400/  475 batches | ms/batch 137.61 | loss  4.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 65.67s | training loss  4.19 |
    | end of validation epoch  77 | time: 51.39s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 157.00 | loss  4.43 |
| epoch  78 |   200/  475 batches | ms/batch 143.54 | loss  4.32 |
| epoch  78 |   300/  475 batches | ms/batch 140.31 | loss  4.28 |
| epoch  78 |   400/  475 batches | ms/batch 137.24 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 65.42s | training loss  4.22 |
    | end of validation epoch  78 | time: 51.09s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 161.97 | loss  4.43 |
| epoch  79 |   200/  475 batches | ms/batch 146.91 | loss  4.27 |
| epoch  79 |   300/  475 batches | ms/batch 142.00 | loss  4.27 |
| epoch  79 |   400/  475 batches | ms/batch 138.54 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 65.49s | training loss  4.21 |
    | end of validation epoch  79 | time: 51.37s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 153.53 | loss  3.95 |
| epoch  80 |   200/  475 batches | ms/batch 142.02 | loss  4.11 |
| epoch  80 |   300/  475 batches | ms/batch 138.31 | loss  4.25 |
| epoch  80 |   400/  475 batches | ms/batch 136.77 | loss  3.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 65.40s | training loss  4.22 |
    | end of validation epoch  80 | time: 50.69s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 158.90 | loss  4.19 |
| epoch  81 |   200/  475 batches | ms/batch 148.87 | loss  4.49 |
| epoch  81 |   300/  475 batches | ms/batch 143.16 | loss  4.25 |
| epoch  81 |   400/  475 batches | ms/batch 139.89 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 66.46s | training loss  4.21 |
    | end of validation epoch  81 | time: 50.98s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 157.32 | loss  4.10 |
| epoch  82 |   200/  475 batches | ms/batch 143.60 | loss  4.24 |
| epoch  82 |   300/  475 batches | ms/batch 140.41 | loss  4.13 |
| epoch  82 |   400/  475 batches | ms/batch 138.85 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 65.78s | training loss  4.22 |
    | end of validation epoch  82 | time: 52.62s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 157.57 | loss  4.04 |
| epoch  83 |   200/  475 batches | ms/batch 143.25 | loss  3.74 |
| epoch  83 |   300/  475 batches | ms/batch 139.49 | loss  4.28 |
| epoch  83 |   400/  475 batches | ms/batch 138.52 | loss  4.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 65.73s | training loss  4.22 |
    | end of validation epoch  83 | time: 50.81s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 158.19 | loss  4.32 |
| epoch  84 |   200/  475 batches | ms/batch 144.28 | loss  4.33 |
| epoch  84 |   300/  475 batches | ms/batch 139.84 | loss  4.23 |
| epoch  84 |   400/  475 batches | ms/batch 137.23 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 65.39s | training loss  4.22 |
    | end of validation epoch  84 | time: 51.42s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 163.24 | loss  3.88 |
| epoch  85 |   200/  475 batches | ms/batch 146.99 | loss  4.39 |
| epoch  85 |   300/  475 batches | ms/batch 142.21 | loss  4.09 |
| epoch  85 |   400/  475 batches | ms/batch 140.08 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 66.66s | training loss  4.22 |
    | end of validation epoch  85 | time: 50.71s | validation loss  3.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 162.90 | loss  4.28 |
| epoch  86 |   200/  475 batches | ms/batch 146.01 | loss  4.23 |
| epoch  86 |   300/  475 batches | ms/batch 141.79 | loss  3.79 |
| epoch  86 |   400/  475 batches | ms/batch 140.21 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 66.44s | training loss  4.22 |
    | end of validation epoch  86 | time: 51.48s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 157.79 | loss  4.22 |
| epoch  87 |   200/  475 batches | ms/batch 144.87 | loss  4.30 |
| epoch  87 |   300/  475 batches | ms/batch 140.71 | loss  4.39 |
| epoch  87 |   400/  475 batches | ms/batch 138.40 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 66.06s | training loss  4.22 |
    | end of validation epoch  87 | time: 50.53s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 161.65 | loss  3.97 |
| epoch  88 |   200/  475 batches | ms/batch 149.27 | loss  3.95 |
| epoch  88 |   300/  475 batches | ms/batch 142.58 | loss  4.46 |
| epoch  88 |   400/  475 batches | ms/batch 140.40 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 66.58s | training loss  4.22 |
    | end of validation epoch  88 | time: 51.32s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 156.59 | loss  4.00 |
| epoch  89 |   200/  475 batches | ms/batch 143.53 | loss  4.24 |
| epoch  89 |   300/  475 batches | ms/batch 138.50 | loss  4.06 |
| epoch  89 |   400/  475 batches | ms/batch 137.12 | loss  4.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 65.37s | training loss  4.23 |
    | end of validation epoch  89 | time: 51.03s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 161.96 | loss  4.22 |
| epoch  90 |   200/  475 batches | ms/batch 146.99 | loss  3.70 |
| epoch  90 |   300/  475 batches | ms/batch 141.31 | loss  3.83 |
| epoch  90 |   400/  475 batches | ms/batch 139.45 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 66.23s | training loss  4.21 |
    | end of validation epoch  90 | time: 50.10s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 158.10 | loss  4.19 |
| epoch  91 |   200/  475 batches | ms/batch 142.43 | loss  4.28 |
| epoch  91 |   300/  475 batches | ms/batch 138.79 | loss  4.10 |
| epoch  91 |   400/  475 batches | ms/batch 136.51 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 64.84s | training loss  4.20 |
    | end of validation epoch  91 | time: 51.34s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 156.86 | loss  4.04 |
| epoch  92 |   200/  475 batches | ms/batch 143.65 | loss  3.89 |
| epoch  92 |   300/  475 batches | ms/batch 139.69 | loss  4.23 |
| epoch  92 |   400/  475 batches | ms/batch 138.37 | loss  4.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 65.79s | training loss  4.21 |
    | end of validation epoch  92 | time: 50.85s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 159.22 | loss  4.27 |
| epoch  93 |   200/  475 batches | ms/batch 145.66 | loss  3.90 |
| epoch  93 |   300/  475 batches | ms/batch 140.90 | loss  3.89 |
| epoch  93 |   400/  475 batches | ms/batch 137.77 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 65.51s | training loss  4.22 |
    | end of validation epoch  93 | time: 51.66s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 158.30 | loss  4.42 |
| epoch  94 |   200/  475 batches | ms/batch 145.76 | loss  4.53 |
| epoch  94 |   300/  475 batches | ms/batch 139.75 | loss  4.00 |
| epoch  94 |   400/  475 batches | ms/batch 138.55 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 65.78s | training loss  4.21 |
    | end of validation epoch  94 | time: 50.86s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 161.52 | loss  4.27 |
| epoch  95 |   200/  475 batches | ms/batch 145.79 | loss  4.34 |
| epoch  95 |   300/  475 batches | ms/batch 139.88 | loss  4.21 |
| epoch  95 |   400/  475 batches | ms/batch 138.06 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 65.62s | training loss  4.22 |
    | end of validation epoch  95 | time: 51.96s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004, 4.2194053745269775] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132, 3.4758321277233732]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 161.00 | loss  4.45 |
| epoch  96 |   200/  475 batches | ms/batch 146.24 | loss  4.07 |
| epoch  96 |   300/  475 batches | ms/batch 141.57 | loss  3.79 |
| epoch  96 |   400/  475 batches | ms/batch 137.89 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 65.57s | training loss  4.21 |
    | end of validation epoch  96 | time: 50.43s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004, 4.2194053745269775, 4.210238691129183] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132, 3.4758321277233732, 3.46036310556556]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 161.45 | loss  3.92 |
| epoch  97 |   200/  475 batches | ms/batch 147.70 | loss  4.15 |
| epoch  97 |   300/  475 batches | ms/batch 142.08 | loss  4.54 |
| epoch  97 |   400/  475 batches | ms/batch 139.45 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 66.12s | training loss  4.22 |
    | end of validation epoch  97 | time: 50.90s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004, 4.2194053745269775, 4.210238691129183, 4.218617022664923] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132, 3.4758321277233732, 3.46036310556556, 3.483854249745858]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 156.70 | loss  4.23 |
| epoch  98 |   200/  475 batches | ms/batch 145.02 | loss  4.15 |
| epoch  98 |   300/  475 batches | ms/batch 139.59 | loss  4.46 |
| epoch  98 |   400/  475 batches | ms/batch 137.41 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 65.22s | training loss  4.23 |
    | end of validation epoch  98 | time: 51.26s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004, 4.2194053745269775, 4.210238691129183, 4.218617022664923, 4.228158633081536] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132, 3.4758321277233732, 3.46036310556556, 3.483854249745858, 3.4462737696511403]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 159.12 | loss  4.37 |
| epoch  99 |   200/  475 batches | ms/batch 143.94 | loss  3.87 |
| epoch  99 |   300/  475 batches | ms/batch 140.44 | loss  4.42 |
| epoch  99 |   400/  475 batches | ms/batch 138.71 | loss  4.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 65.97s | training loss  4.20 |
    | end of validation epoch  99 | time: 50.62s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.806852164017527, 6.330485973358154, 5.997632864901894, 5.753968061145983, 5.542423300492136, 5.402544973273026, 5.252950001766807, 5.149941025784141, 5.043456752174779, 4.960589519299959, 4.885608206297222, 4.814431170413369, 4.770241949181807, 4.699480985842253, 4.6744732294584574, 4.628134790721693, 4.592697459271079, 4.560711298992759, 4.5301227619773465, 4.503485484374197, 4.477358254884419, 4.465211098319606, 4.431854234494661, 4.427100484245702, 4.413895410738493, 4.3959191136611135, 4.370450644242136, 4.360954387062474, 4.340985074294241, 4.322327167611373, 4.325899720442923, 4.31398031335128, 4.31086947692068, 4.307468175386128, 4.298641297189813, 4.279254922866821, 4.275914418571873, 4.270601044203106, 4.272677069212261, 4.2747447033932335, 4.270651023764359, 4.258923032158299, 4.250983252274363, 4.267301483154297, 4.257905456643355, 4.243892613963077, 4.245173113973517, 4.250779282921239, 4.2396230120407905, 4.236691070857801, 4.240614192862259, 4.223809158927516, 4.239108988611322, 4.229994555523521, 4.2400354109312355, 4.222190572337101, 4.229507038718776, 4.2243069478085165, 4.220234130056281, 4.218014032966212, 4.220928394919947, 4.226933439656308, 4.229453823189987, 4.223116018897609, 4.218692935140509, 4.222023088555587, 4.220928027504369, 4.211943958684018, 4.210051786523116, 4.211628513838115, 4.2121504949268544, 4.22542054879038, 4.213647168811999, 4.2065394441705, 4.2286906975194025, 4.213238737708644, 4.194641422472502, 4.217074063451666, 4.208771163036949, 4.22047133997867, 4.209841853693912, 4.219478903318707, 4.223603091490896, 4.219510978899504, 4.2159265643671935, 4.218441628907856, 4.217833966204995, 4.218175469448692, 4.228229173861052, 4.2138040392022384, 4.202109677164178, 4.207388605318571, 4.21582959978204, 4.212686973371004, 4.2194053745269775, 4.210238691129183, 4.218617022664923, 4.228158633081536, 4.20267867891412] validation loss is  [5.493682853314055, 5.163561227942715, 4.910963182689763, 4.753966716157288, 4.565386828254251, 4.434237952993698, 4.325449885440474, 4.235623133282702, 4.1650044276934715, 4.107604399448683, 4.0233597855608005, 3.99189406683465, 3.946732478983262, 3.910611469204686, 3.892986900666181, 3.860623794443467, 3.8336511018897306, 3.7969764100403345, 3.7753687586103166, 3.756381577804309, 3.728902718600105, 3.7297551271294345, 3.7129924096981015, 3.685799949309405, 3.656025229381914, 3.678263157355685, 3.663750941012086, 3.651930642729046, 3.6434703414179697, 3.6249619111293505, 3.616643338644204, 3.6239183950824896, 3.6057560784476146, 3.603732257330117, 3.589415700495744, 3.5838393103174804, 3.5842342897623527, 3.57268445632037, 3.578307237945685, 3.5492662441830674, 3.5718140421795246, 3.541715141104049, 3.553849238307536, 3.525933349833769, 3.555880728889914, 3.537260636562059, 3.543805627261891, 3.5542514945278647, 3.532055946959167, 3.523703671303116, 3.5457723861982844, 3.5179855102250555, 3.5255368236734084, 3.521276171467885, 3.5137862257596826, 3.4978617099152896, 3.517749199346334, 3.497876676190801, 3.5106077013897297, 3.499997926359417, 3.510511670793806, 3.4841686457145116, 3.5030682607859123, 3.4986465458108595, 3.498823089759891, 3.4949569141163543, 3.493524397120756, 3.509996650599632, 3.4866146279984163, 3.5029198722679076, 3.4886141564665722, 3.462217164640667, 3.4774471851957944, 3.4854418169550536, 3.464531016950848, 3.503983042821163, 3.480798597095393, 3.4907501324886034, 3.4884733833184765, 3.4659496976547883, 3.4706918011192514, 3.483169569688685, 3.491922616958618, 3.473102321143912, 3.491389573121271, 3.5039903596669686, 3.468793257945726, 3.4581390949858335, 3.481978424456941, 3.476513732381228, 3.4726088587977304, 3.4615896549545417, 3.4957028617378043, 3.465007060716132, 3.4758321277233732, 3.46036310556556, 3.483854249745858, 3.4462737696511403, 3.4778258640225195]
