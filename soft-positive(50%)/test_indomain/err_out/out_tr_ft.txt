/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_sou_prob_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 168.14 | loss  5.85 |
| epoch   1 |   200/  475 batches | ms/batch 155.66 | loss  5.59 |
| epoch   1 |   300/  475 batches | ms/batch 150.60 | loss  5.48 |
| epoch   1 |   400/  475 batches | ms/batch 148.35 | loss  5.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 70.53s | training loss  5.70 |
    | end of validation epoch   1 | time: 54.22s | validation loss  5.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.704766763385973] validation loss is  [5.08311832251669]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 164.63 | loss  5.30 |
| epoch   2 |   200/  475 batches | ms/batch 151.40 | loss  5.26 |
| epoch   2 |   300/  475 batches | ms/batch 146.11 | loss  4.97 |
| epoch   2 |   400/  475 batches | ms/batch 144.24 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 68.24s | training loss  5.14 |
    | end of validation epoch   2 | time: 53.42s | validation loss  4.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.704766763385973, 5.138340708080091] validation loss is  [5.08311832251669, 4.542349663101325]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 162.57 | loss  4.92 |
| epoch   3 |   200/  475 batches | ms/batch 148.21 | loss  4.89 |
| epoch   3 |   300/  475 batches | ms/batch 144.54 | loss  4.69 |
| epoch   3 |   400/  475 batches | ms/batch 142.69 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 67.96s | training loss  4.83 |
    | end of validation epoch   3 | time: 53.79s | validation loss  4.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 160.27 | loss  4.79 |
| epoch   4 |   200/  475 batches | ms/batch 149.65 | loss  4.72 |
| epoch   4 |   300/  475 batches | ms/batch 144.15 | loss  4.98 |
| epoch   4 |   400/  475 batches | ms/batch 142.43 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 67.86s | training loss  4.64 |
    | end of validation epoch   4 | time: 54.24s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 167.22 | loss  4.31 |
| epoch   5 |   200/  475 batches | ms/batch 153.96 | loss  4.67 |
| epoch   5 |   300/  475 batches | ms/batch 148.17 | loss  3.97 |
| epoch   5 |   400/  475 batches | ms/batch 144.60 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 68.78s | training loss  4.49 |
    | end of validation epoch   5 | time: 53.77s | validation loss  3.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 164.79 | loss  4.19 |
| epoch   6 |   200/  475 batches | ms/batch 150.47 | loss  4.30 |
| epoch   6 |   300/  475 batches | ms/batch 145.72 | loss  4.50 |
| epoch   6 |   400/  475 batches | ms/batch 143.18 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 68.11s | training loss  4.38 |
    | end of validation epoch   6 | time: 53.07s | validation loss  3.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 161.38 | loss  4.24 |
| epoch   7 |   200/  475 batches | ms/batch 149.64 | loss  4.47 |
| epoch   7 |   300/  475 batches | ms/batch 145.81 | loss  4.31 |
| epoch   7 |   400/  475 batches | ms/batch 143.59 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 68.39s | training loss  4.28 |
    | end of validation epoch   7 | time: 52.36s | validation loss  3.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 162.81 | loss  4.36 |
| epoch   8 |   200/  475 batches | ms/batch 150.89 | loss  4.23 |
| epoch   8 |   300/  475 batches | ms/batch 145.61 | loss  4.07 |
| epoch   8 |   400/  475 batches | ms/batch 143.31 | loss  3.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 67.67s | training loss  4.20 |
    | end of validation epoch   8 | time: 54.28s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 162.73 | loss  4.10 |
| epoch   9 |   200/  475 batches | ms/batch 150.46 | loss  4.25 |
| epoch   9 |   300/  475 batches | ms/batch 146.35 | loss  3.79 |
| epoch   9 |   400/  475 batches | ms/batch 143.22 | loss  3.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 67.64s | training loss  4.12 |
    | end of validation epoch   9 | time: 53.31s | validation loss  3.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 166.92 | loss  4.03 |
| epoch  10 |   200/  475 batches | ms/batch 152.82 | loss  3.98 |
| epoch  10 |   300/  475 batches | ms/batch 147.67 | loss  3.82 |
| epoch  10 |   400/  475 batches | ms/batch 144.45 | loss  3.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 68.22s | training loss  4.07 |
    | end of validation epoch  10 | time: 54.35s | validation loss  3.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 170.38 | loss  4.23 |
| epoch  11 |   200/  475 batches | ms/batch 152.67 | loss  4.09 |
| epoch  11 |   300/  475 batches | ms/batch 147.27 | loss  4.35 |
| epoch  11 |   400/  475 batches | ms/batch 145.17 | loss  4.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 69.02s | training loss  4.02 |
    | end of validation epoch  11 | time: 53.78s | validation loss  3.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 166.08 | loss  4.23 |
| epoch  12 |   200/  475 batches | ms/batch 151.69 | loss  3.71 |
| epoch  12 |   300/  475 batches | ms/batch 145.79 | loss  3.69 |
| epoch  12 |   400/  475 batches | ms/batch 142.90 | loss  3.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 68.04s | training loss  3.97 |
    | end of validation epoch  12 | time: 53.27s | validation loss  3.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 161.96 | loss  4.00 |
| epoch  13 |   200/  475 batches | ms/batch 150.50 | loss  4.13 |
| epoch  13 |   300/  475 batches | ms/batch 146.28 | loss  3.85 |
| epoch  13 |   400/  475 batches | ms/batch 143.68 | loss  3.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 68.25s | training loss  3.93 |
    | end of validation epoch  13 | time: 51.84s | validation loss  3.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 157.42 | loss  3.70 |
| epoch  14 |   200/  475 batches | ms/batch 148.72 | loss  3.79 |
| epoch  14 |   300/  475 batches | ms/batch 147.19 | loss  3.68 |
| epoch  14 |   400/  475 batches | ms/batch 143.63 | loss  3.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 67.85s | training loss  3.88 |
    | end of validation epoch  14 | time: 54.38s | validation loss  3.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 163.84 | loss  3.67 |
| epoch  15 |   200/  475 batches | ms/batch 150.03 | loss  3.19 |
| epoch  15 |   300/  475 batches | ms/batch 144.78 | loss  3.87 |
| epoch  15 |   400/  475 batches | ms/batch 142.76 | loss  3.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 67.87s | training loss  3.84 |
    | end of validation epoch  15 | time: 53.73s | validation loss  3.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 163.26 | loss  3.86 |
| epoch  16 |   200/  475 batches | ms/batch 150.85 | loss  3.50 |
| epoch  16 |   300/  475 batches | ms/batch 145.56 | loss  3.83 |
| epoch  16 |   400/  475 batches | ms/batch 143.28 | loss  3.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 68.05s | training loss  3.81 |
    | end of validation epoch  16 | time: 54.02s | validation loss  3.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 163.72 | loss  3.66 |
| epoch  17 |   200/  475 batches | ms/batch 152.24 | loss  3.50 |
| epoch  17 |   300/  475 batches | ms/batch 145.69 | loss  3.83 |
| epoch  17 |   400/  475 batches | ms/batch 142.13 | loss  3.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 67.71s | training loss  3.79 |
    | end of validation epoch  17 | time: 55.45s | validation loss  3.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 165.49 | loss  3.64 |
| epoch  18 |   200/  475 batches | ms/batch 150.56 | loss  4.15 |
| epoch  18 |   300/  475 batches | ms/batch 145.93 | loss  3.51 |
| epoch  18 |   400/  475 batches | ms/batch 144.44 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 68.19s | training loss  3.74 |
    | end of validation epoch  18 | time: 54.90s | validation loss  3.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 161.28 | loss  3.36 |
| epoch  19 |   200/  475 batches | ms/batch 148.83 | loss  3.67 |
| epoch  19 |   300/  475 batches | ms/batch 143.91 | loss  3.77 |
| epoch  19 |   400/  475 batches | ms/batch 141.64 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 67.37s | training loss  3.72 |
    | end of validation epoch  19 | time: 54.63s | validation loss  2.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 161.94 | loss  4.13 |
| epoch  20 |   200/  475 batches | ms/batch 150.08 | loss  3.70 |
| epoch  20 |   300/  475 batches | ms/batch 145.68 | loss  4.25 |
| epoch  20 |   400/  475 batches | ms/batch 143.66 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 68.32s | training loss  3.69 |
    | end of validation epoch  20 | time: 54.30s | validation loss  2.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 162.72 | loss  3.84 |
| epoch  21 |   200/  475 batches | ms/batch 147.34 | loss  3.57 |
| epoch  21 |   300/  475 batches | ms/batch 143.01 | loss  3.90 |
| epoch  21 |   400/  475 batches | ms/batch 141.16 | loss  3.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 67.42s | training loss  3.68 |
    | end of validation epoch  21 | time: 53.78s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 164.40 | loss  3.63 |
| epoch  22 |   200/  475 batches | ms/batch 150.54 | loss  3.43 |
| epoch  22 |   300/  475 batches | ms/batch 145.14 | loss  3.95 |
| epoch  22 |   400/  475 batches | ms/batch 142.34 | loss  3.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 67.59s | training loss  3.65 |
    | end of validation epoch  22 | time: 55.16s | validation loss  2.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 154.19 | loss  3.20 |
| epoch  23 |   200/  475 batches | ms/batch 145.94 | loss  3.59 |
| epoch  23 |   300/  475 batches | ms/batch 141.87 | loss  3.76 |
| epoch  23 |   400/  475 batches | ms/batch 139.50 | loss  3.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 66.36s | training loss  3.61 |
    | end of validation epoch  23 | time: 53.59s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 157.26 | loss  3.76 |
| epoch  24 |   200/  475 batches | ms/batch 145.10 | loss  3.45 |
| epoch  24 |   300/  475 batches | ms/batch 142.32 | loss  3.71 |
| epoch  24 |   400/  475 batches | ms/batch 140.25 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 66.93s | training loss  3.61 |
    | end of validation epoch  24 | time: 54.96s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 158.12 | loss  3.46 |
| epoch  25 |   200/  475 batches | ms/batch 145.23 | loss  3.49 |
| epoch  25 |   300/  475 batches | ms/batch 141.89 | loss  3.17 |
| epoch  25 |   400/  475 batches | ms/batch 140.30 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 66.97s | training loss  3.58 |
    | end of validation epoch  25 | time: 55.77s | validation loss  2.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 159.96 | loss  3.61 |
| epoch  26 |   200/  475 batches | ms/batch 147.06 | loss  3.55 |
| epoch  26 |   300/  475 batches | ms/batch 142.16 | loss  3.97 |
| epoch  26 |   400/  475 batches | ms/batch 141.28 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 67.54s | training loss  3.55 |
    | end of validation epoch  26 | time: 55.26s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 160.12 | loss  3.37 |
| epoch  27 |   200/  475 batches | ms/batch 148.06 | loss  3.70 |
| epoch  27 |   300/  475 batches | ms/batch 144.10 | loss  3.23 |
| epoch  27 |   400/  475 batches | ms/batch 141.94 | loss  3.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 67.34s | training loss  3.56 |
    | end of validation epoch  27 | time: 55.01s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272]
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 160.60 | loss  3.51 |
| epoch  28 |   200/  475 batches | ms/batch 149.42 | loss  3.62 |
| epoch  28 |   300/  475 batches | ms/batch 144.76 | loss  3.72 |
| epoch  28 |   400/  475 batches | ms/batch 143.01 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 67.68s | training loss  3.55 |
    | end of validation epoch  28 | time: 55.11s | validation loss  2.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 170.41 | loss  3.09 |
| epoch  29 |   200/  475 batches | ms/batch 152.68 | loss  3.54 |
| epoch  29 |   300/  475 batches | ms/batch 146.29 | loss  3.69 |
| epoch  29 |   400/  475 batches | ms/batch 144.06 | loss  2.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 68.11s | training loss  3.50 |
    | end of validation epoch  29 | time: 53.60s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 157.87 | loss  3.67 |
| epoch  30 |   200/  475 batches | ms/batch 149.23 | loss  3.99 |
| epoch  30 |   300/  475 batches | ms/batch 144.20 | loss  3.62 |
| epoch  30 |   400/  475 batches | ms/batch 142.17 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 67.72s | training loss  3.50 |
    | end of validation epoch  30 | time: 53.77s | validation loss  2.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 158.52 | loss  3.38 |
| epoch  31 |   200/  475 batches | ms/batch 147.39 | loss  3.37 |
| epoch  31 |   300/  475 batches | ms/batch 144.07 | loss  3.42 |
| epoch  31 |   400/  475 batches | ms/batch 142.00 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 67.30s | training loss  3.49 |
    | end of validation epoch  31 | time: 53.77s | validation loss  2.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 160.35 | loss  3.42 |
| epoch  32 |   200/  475 batches | ms/batch 148.22 | loss  3.11 |
| epoch  32 |   300/  475 batches | ms/batch 144.25 | loss  2.81 |
| epoch  32 |   400/  475 batches | ms/batch 141.55 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 67.76s | training loss  3.45 |
    | end of validation epoch  32 | time: 53.23s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 160.01 | loss  3.11 |
| epoch  33 |   200/  475 batches | ms/batch 150.96 | loss  3.54 |
| epoch  33 |   300/  475 batches | ms/batch 145.75 | loss  3.57 |
| epoch  33 |   400/  475 batches | ms/batch 143.31 | loss  3.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 68.08s | training loss  3.45 |
    | end of validation epoch  33 | time: 54.43s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 160.66 | loss  3.76 |
| epoch  34 |   200/  475 batches | ms/batch 150.05 | loss  3.73 |
| epoch  34 |   300/  475 batches | ms/batch 145.45 | loss  3.35 |
| epoch  34 |   400/  475 batches | ms/batch 142.03 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 67.63s | training loss  3.43 |
    | end of validation epoch  34 | time: 53.14s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 168.22 | loss  3.79 |
| epoch  35 |   200/  475 batches | ms/batch 153.66 | loss  3.45 |
| epoch  35 |   300/  475 batches | ms/batch 147.66 | loss  3.71 |
| epoch  35 |   400/  475 batches | ms/batch 144.82 | loss  3.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 68.75s | training loss  3.42 |
    | end of validation epoch  35 | time: 54.33s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 166.51 | loss  3.39 |
| epoch  36 |   200/  475 batches | ms/batch 151.35 | loss  3.46 |
| epoch  36 |   300/  475 batches | ms/batch 145.96 | loss  3.85 |
| epoch  36 |   400/  475 batches | ms/batch 143.71 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 68.12s | training loss  3.41 |
    | end of validation epoch  36 | time: 54.73s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 162.12 | loss  3.43 |
| epoch  37 |   200/  475 batches | ms/batch 149.95 | loss  3.84 |
| epoch  37 |   300/  475 batches | ms/batch 144.82 | loss  3.41 |
| epoch  37 |   400/  475 batches | ms/batch 143.37 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 68.43s | training loss  3.39 |
    | end of validation epoch  37 | time: 54.88s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 160.96 | loss  3.68 |
| epoch  38 |   200/  475 batches | ms/batch 147.53 | loss  3.21 |
| epoch  38 |   300/  475 batches | ms/batch 143.87 | loss  3.42 |
| epoch  38 |   400/  475 batches | ms/batch 142.27 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 67.82s | training loss  3.38 |
    | end of validation epoch  38 | time: 54.81s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 162.96 | loss  3.34 |
| epoch  39 |   200/  475 batches | ms/batch 150.76 | loss  3.70 |
| epoch  39 |   300/  475 batches | ms/batch 145.88 | loss  3.01 |
| epoch  39 |   400/  475 batches | ms/batch 143.18 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 68.03s | training loss  3.37 |
    | end of validation epoch  39 | time: 53.00s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 162.03 | loss  2.89 |
| epoch  40 |   200/  475 batches | ms/batch 151.88 | loss  3.15 |
| epoch  40 |   300/  475 batches | ms/batch 146.07 | loss  3.71 |
| epoch  40 |   400/  475 batches | ms/batch 142.96 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 67.79s | training loss  3.34 |
    | end of validation epoch  40 | time: 52.26s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 169.58 | loss  3.13 |
| epoch  41 |   200/  475 batches | ms/batch 151.58 | loss  3.59 |
| epoch  41 |   300/  475 batches | ms/batch 146.13 | loss  3.30 |
| epoch  41 |   400/  475 batches | ms/batch 144.03 | loss  3.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 68.58s | training loss  3.35 |
    | end of validation epoch  41 | time: 54.44s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 162.03 | loss  3.23 |
| epoch  42 |   200/  475 batches | ms/batch 149.17 | loss  3.36 |
| epoch  42 |   300/  475 batches | ms/batch 146.58 | loss  2.88 |
| epoch  42 |   400/  475 batches | ms/batch 143.99 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 68.31s | training loss  3.33 |
    | end of validation epoch  42 | time: 54.98s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 173.11 | loss  3.70 |
| epoch  43 |   200/  475 batches | ms/batch 154.92 | loss  3.95 |
| epoch  43 |   300/  475 batches | ms/batch 148.72 | loss  3.84 |
| epoch  43 |   400/  475 batches | ms/batch 145.59 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 68.81s | training loss  3.33 |
    | end of validation epoch  43 | time: 54.55s | validation loss  2.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 160.46 | loss  3.39 |
| epoch  44 |   200/  475 batches | ms/batch 146.72 | loss  3.41 |
| epoch  44 |   300/  475 batches | ms/batch 143.96 | loss  3.69 |
| epoch  44 |   400/  475 batches | ms/batch 141.83 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 67.75s | training loss  3.32 |
    | end of validation epoch  44 | time: 55.03s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 164.76 | loss  3.28 |
| epoch  45 |   200/  475 batches | ms/batch 151.13 | loss  3.19 |
| epoch  45 |   300/  475 batches | ms/batch 145.55 | loss  3.01 |
| epoch  45 |   400/  475 batches | ms/batch 143.50 | loss  3.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 68.20s | training loss  3.29 |
    | end of validation epoch  45 | time: 54.06s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 167.44 | loss  3.13 |
| epoch  46 |   200/  475 batches | ms/batch 151.26 | loss  3.09 |
| epoch  46 |   300/  475 batches | ms/batch 145.48 | loss  3.38 |
| epoch  46 |   400/  475 batches | ms/batch 141.96 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 67.54s | training loss  3.30 |
    | end of validation epoch  46 | time: 53.15s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 159.94 | loss  3.68 |
| epoch  47 |   200/  475 batches | ms/batch 146.50 | loss  3.45 |
| epoch  47 |   300/  475 batches | ms/batch 143.65 | loss  3.63 |
| epoch  47 |   400/  475 batches | ms/batch 143.18 | loss  3.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 68.12s | training loss  3.29 |
    | end of validation epoch  47 | time: 54.56s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 168.73 | loss  3.16 |
| epoch  48 |   200/  475 batches | ms/batch 150.49 | loss  3.36 |
| epoch  48 |   300/  475 batches | ms/batch 144.93 | loss  3.47 |
| epoch  48 |   400/  475 batches | ms/batch 143.39 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 68.39s | training loss  3.28 |
    | end of validation epoch  48 | time: 55.71s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 163.78 | loss  3.34 |
| epoch  49 |   200/  475 batches | ms/batch 151.42 | loss  3.15 |
| epoch  49 |   300/  475 batches | ms/batch 146.00 | loss  3.62 |
| epoch  49 |   400/  475 batches | ms/batch 143.22 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 68.47s | training loss  3.27 |
    | end of validation epoch  49 | time: 54.32s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 163.58 | loss  3.14 |
| epoch  50 |   200/  475 batches | ms/batch 151.34 | loss  3.48 |
| epoch  50 |   300/  475 batches | ms/batch 146.42 | loss  3.45 |
| epoch  50 |   400/  475 batches | ms/batch 144.38 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 68.57s | training loss  3.26 |
    | end of validation epoch  50 | time: 54.22s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 167.60 | loss  3.10 |
| epoch  51 |   200/  475 batches | ms/batch 150.22 | loss  3.11 |
| epoch  51 |   300/  475 batches | ms/batch 146.77 | loss  3.36 |
| epoch  51 |   400/  475 batches | ms/batch 144.30 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 68.66s | training loss  3.26 |
    | end of validation epoch  51 | time: 55.02s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 160.29 | loss  3.09 |
| epoch  52 |   200/  475 batches | ms/batch 148.35 | loss  3.25 |
| epoch  52 |   300/  475 batches | ms/batch 144.16 | loss  3.53 |
| epoch  52 |   400/  475 batches | ms/batch 141.78 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 67.52s | training loss  3.23 |
    | end of validation epoch  52 | time: 54.85s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 163.60 | loss  3.73 |
| epoch  53 |   200/  475 batches | ms/batch 151.70 | loss  2.98 |
| epoch  53 |   300/  475 batches | ms/batch 146.60 | loss  3.00 |
| epoch  53 |   400/  475 batches | ms/batch 144.55 | loss  3.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 68.77s | training loss  3.23 |
    | end of validation epoch  53 | time: 54.62s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 158.73 | loss  2.94 |
| epoch  54 |   200/  475 batches | ms/batch 149.28 | loss  4.48 |
| epoch  54 |   300/  475 batches | ms/batch 144.62 | loss  3.53 |
| epoch  54 |   400/  475 batches | ms/batch 142.54 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 67.54s | training loss  3.21 |
    | end of validation epoch  54 | time: 54.14s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 164.81 | loss  3.38 |
| epoch  55 |   200/  475 batches | ms/batch 151.10 | loss  2.97 |
| epoch  55 |   300/  475 batches | ms/batch 147.51 | loss  3.23 |
| epoch  55 |   400/  475 batches | ms/batch 145.86 | loss  3.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 69.30s | training loss  3.22 |
    | end of validation epoch  55 | time: 56.15s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303]
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 161.32 | loss  3.33 |
| epoch  56 |   200/  475 batches | ms/batch 150.18 | loss  3.43 |
| epoch  56 |   300/  475 batches | ms/batch 146.70 | loss  3.22 |
| epoch  56 |   400/  475 batches | ms/batch 144.55 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 68.61s | training loss  3.19 |
    | end of validation epoch  56 | time: 53.56s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 162.75 | loss  3.24 |
| epoch  57 |   200/  475 batches | ms/batch 148.94 | loss  2.99 |
| epoch  57 |   300/  475 batches | ms/batch 144.02 | loss  3.24 |
| epoch  57 |   400/  475 batches | ms/batch 143.30 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 67.98s | training loss  3.19 |
    | end of validation epoch  57 | time: 55.31s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 164.45 | loss  2.84 |
| epoch  58 |   200/  475 batches | ms/batch 150.63 | loss  2.85 |
| epoch  58 |   300/  475 batches | ms/batch 145.68 | loss  3.26 |
| epoch  58 |   400/  475 batches | ms/batch 144.81 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 68.52s | training loss  3.19 |
    | end of validation epoch  58 | time: 54.82s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393]
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 161.93 | loss  3.24 |
| epoch  59 |   200/  475 batches | ms/batch 150.25 | loss  2.92 |
| epoch  59 |   300/  475 batches | ms/batch 145.55 | loss  2.95 |
| epoch  59 |   400/  475 batches | ms/batch 143.36 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 68.01s | training loss  3.18 |
    | end of validation epoch  59 | time: 53.97s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 163.64 | loss  3.27 |
| epoch  60 |   200/  475 batches | ms/batch 149.94 | loss  3.00 |
| epoch  60 |   300/  475 batches | ms/batch 145.65 | loss  3.12 |
| epoch  60 |   400/  475 batches | ms/batch 143.62 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 68.17s | training loss  3.19 |
    | end of validation epoch  60 | time: 54.04s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243]
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 164.73 | loss  3.26 |
| epoch  61 |   200/  475 batches | ms/batch 149.36 | loss  3.04 |
| epoch  61 |   300/  475 batches | ms/batch 146.25 | loss  2.52 |
| epoch  61 |   400/  475 batches | ms/batch 143.19 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 67.67s | training loss  3.16 |
    | end of validation epoch  61 | time: 54.29s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 162.55 | loss  3.02 |
| epoch  62 |   200/  475 batches | ms/batch 148.14 | loss  3.51 |
| epoch  62 |   300/  475 batches | ms/batch 144.69 | loss  2.94 |
| epoch  62 |   400/  475 batches | ms/batch 142.99 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 67.72s | training loss  3.16 |
    | end of validation epoch  62 | time: 55.80s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 153.52 | loss  3.04 |
| epoch  63 |   200/  475 batches | ms/batch 148.12 | loss  3.44 |
| epoch  63 |   300/  475 batches | ms/batch 144.73 | loss  2.89 |
| epoch  63 |   400/  475 batches | ms/batch 143.00 | loss  2.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 67.82s | training loss  3.15 |
    | end of validation epoch  63 | time: 54.28s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 157.40 | loss  3.70 |
| epoch  64 |   200/  475 batches | ms/batch 150.61 | loss  2.91 |
| epoch  64 |   300/  475 batches | ms/batch 146.09 | loss  3.44 |
| epoch  64 |   400/  475 batches | ms/batch 143.73 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 68.06s | training loss  3.14 |
    | end of validation epoch  64 | time: 54.80s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 165.97 | loss  3.37 |
| epoch  65 |   200/  475 batches | ms/batch 152.27 | loss  3.29 |
| epoch  65 |   300/  475 batches | ms/batch 146.76 | loss  3.17 |
| epoch  65 |   400/  475 batches | ms/batch 144.82 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 68.46s | training loss  3.13 |
    | end of validation epoch  65 | time: 55.50s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 159.53 | loss  3.38 |
| epoch  66 |   200/  475 batches | ms/batch 149.99 | loss  3.36 |
| epoch  66 |   300/  475 batches | ms/batch 144.94 | loss  2.94 |
| epoch  66 |   400/  475 batches | ms/batch 142.75 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 68.01s | training loss  3.13 |
    | end of validation epoch  66 | time: 54.97s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 167.46 | loss  2.70 |
| epoch  67 |   200/  475 batches | ms/batch 151.15 | loss  3.40 |
| epoch  67 |   300/  475 batches | ms/batch 146.10 | loss  3.46 |
| epoch  67 |   400/  475 batches | ms/batch 143.61 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 68.20s | training loss  3.11 |
    | end of validation epoch  67 | time: 55.31s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 162.48 | loss  3.25 |
| epoch  68 |   200/  475 batches | ms/batch 149.39 | loss  3.45 |
| epoch  68 |   300/  475 batches | ms/batch 144.35 | loss  3.06 |
| epoch  68 |   400/  475 batches | ms/batch 141.73 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 67.42s | training loss  3.11 |
    | end of validation epoch  68 | time: 55.67s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 164.81 | loss  2.99 |
| epoch  69 |   200/  475 batches | ms/batch 150.14 | loss  3.07 |
| epoch  69 |   300/  475 batches | ms/batch 146.35 | loss  3.31 |
| epoch  69 |   400/  475 batches | ms/batch 144.03 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 68.02s | training loss  3.10 |
    | end of validation epoch  69 | time: 55.19s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 163.81 | loss  3.35 |
| epoch  70 |   200/  475 batches | ms/batch 149.62 | loss  2.91 |
| epoch  70 |   300/  475 batches | ms/batch 146.01 | loss  3.36 |
| epoch  70 |   400/  475 batches | ms/batch 143.70 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 68.19s | training loss  3.09 |
    | end of validation epoch  70 | time: 54.53s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 161.02 | loss  3.82 |
| epoch  71 |   200/  475 batches | ms/batch 149.18 | loss  3.25 |
| epoch  71 |   300/  475 batches | ms/batch 145.40 | loss  3.40 |
| epoch  71 |   400/  475 batches | ms/batch 142.74 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 67.90s | training loss  3.09 |
    | end of validation epoch  71 | time: 53.90s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 166.39 | loss  3.24 |
| epoch  72 |   200/  475 batches | ms/batch 151.48 | loss  3.03 |
| epoch  72 |   300/  475 batches | ms/batch 146.09 | loss  3.42 |
| epoch  72 |   400/  475 batches | ms/batch 143.38 | loss  2.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 68.31s | training loss  3.09 |
    | end of validation epoch  72 | time: 53.58s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 160.88 | loss  3.29 |
| epoch  73 |   200/  475 batches | ms/batch 147.61 | loss  3.00 |
| epoch  73 |   300/  475 batches | ms/batch 144.10 | loss  2.61 |
| epoch  73 |   400/  475 batches | ms/batch 142.17 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 67.66s | training loss  3.07 |
    | end of validation epoch  73 | time: 55.31s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 154.14 | loss  2.23 |
| epoch  74 |   200/  475 batches | ms/batch 149.30 | loss  3.47 |
| epoch  74 |   300/  475 batches | ms/batch 145.32 | loss  3.27 |
| epoch  74 |   400/  475 batches | ms/batch 142.18 | loss  2.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 67.46s | training loss  3.05 |
    | end of validation epoch  74 | time: 53.66s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 166.03 | loss  3.15 |
| epoch  75 |   200/  475 batches | ms/batch 151.04 | loss  3.26 |
| epoch  75 |   300/  475 batches | ms/batch 146.46 | loss  3.57 |
| epoch  75 |   400/  475 batches | ms/batch 143.12 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 68.24s | training loss  3.05 |
    | end of validation epoch  75 | time: 52.86s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 166.90 | loss  3.10 |
| epoch  76 |   200/  475 batches | ms/batch 152.42 | loss  2.85 |
| epoch  76 |   300/  475 batches | ms/batch 147.64 | loss  3.30 |
| epoch  76 |   400/  475 batches | ms/batch 144.72 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 68.81s | training loss  3.07 |
    | end of validation epoch  76 | time: 55.15s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 164.23 | loss  3.51 |
| epoch  77 |   200/  475 batches | ms/batch 150.12 | loss  3.16 |
| epoch  77 |   300/  475 batches | ms/batch 146.39 | loss  2.81 |
| epoch  77 |   400/  475 batches | ms/batch 144.51 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 68.72s | training loss  3.04 |
    | end of validation epoch  77 | time: 55.28s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 165.32 | loss  3.62 |
| epoch  78 |   200/  475 batches | ms/batch 151.34 | loss  3.02 |
| epoch  78 |   300/  475 batches | ms/batch 145.35 | loss  3.03 |
| epoch  78 |   400/  475 batches | ms/batch 143.70 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 68.15s | training loss  3.05 |
    | end of validation epoch  78 | time: 54.83s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 158.08 | loss  3.34 |
| epoch  79 |   200/  475 batches | ms/batch 150.57 | loss  2.70 |
| epoch  79 |   300/  475 batches | ms/batch 145.90 | loss  2.88 |
| epoch  79 |   400/  475 batches | ms/batch 143.08 | loss  3.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 68.20s | training loss  3.03 |
    | end of validation epoch  79 | time: 54.52s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 161.47 | loss  3.10 |
| epoch  80 |   200/  475 batches | ms/batch 148.87 | loss  2.77 |
| epoch  80 |   300/  475 batches | ms/batch 144.65 | loss  2.36 |
| epoch  80 |   400/  475 batches | ms/batch 142.35 | loss  3.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 67.88s | training loss  3.04 |
    | end of validation epoch  80 | time: 54.75s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 160.45 | loss  2.63 |
| epoch  81 |   200/  475 batches | ms/batch 149.99 | loss  3.14 |
| epoch  81 |   300/  475 batches | ms/batch 145.57 | loss  2.42 |
| epoch  81 |   400/  475 batches | ms/batch 143.24 | loss  2.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 68.25s | training loss  3.03 |
    | end of validation epoch  81 | time: 55.15s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 166.27 | loss  2.93 |
| epoch  82 |   200/  475 batches | ms/batch 152.14 | loss  3.39 |
| epoch  82 |   300/  475 batches | ms/batch 146.66 | loss  3.14 |
| epoch  82 |   400/  475 batches | ms/batch 144.23 | loss  3.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 68.36s | training loss  3.04 |
    | end of validation epoch  82 | time: 55.38s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 151.87 | loss  2.20 |
| epoch  83 |   200/  475 batches | ms/batch 149.79 | loss  3.28 |
| epoch  83 |   300/  475 batches | ms/batch 145.56 | loss  2.91 |
| epoch  83 |   400/  475 batches | ms/batch 142.76 | loss  2.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 67.66s | training loss  3.02 |
    | end of validation epoch  83 | time: 53.89s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 158.45 | loss  3.20 |
| epoch  84 |   200/  475 batches | ms/batch 150.24 | loss  2.85 |
| epoch  84 |   300/  475 batches | ms/batch 144.80 | loss  3.06 |
| epoch  84 |   400/  475 batches | ms/batch 142.55 | loss  2.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 67.66s | training loss  3.01 |
    | end of validation epoch  84 | time: 55.59s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 165.98 | loss  2.78 |
| epoch  85 |   200/  475 batches | ms/batch 150.69 | loss  3.29 |
| epoch  85 |   300/  475 batches | ms/batch 145.44 | loss  3.25 |
| epoch  85 |   400/  475 batches | ms/batch 143.28 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 68.23s | training loss  3.00 |
    | end of validation epoch  85 | time: 55.10s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 163.54 | loss  2.73 |
| epoch  86 |   200/  475 batches | ms/batch 151.76 | loss  2.75 |
| epoch  86 |   300/  475 batches | ms/batch 147.73 | loss  3.20 |
| epoch  86 |   400/  475 batches | ms/batch 144.45 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 68.56s | training loss  2.99 |
    | end of validation epoch  86 | time: 54.92s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 164.71 | loss  3.36 |
| epoch  87 |   200/  475 batches | ms/batch 150.66 | loss  2.95 |
| epoch  87 |   300/  475 batches | ms/batch 145.82 | loss  2.97 |
| epoch  87 |   400/  475 batches | ms/batch 142.70 | loss  3.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 67.65s | training loss  2.98 |
    | end of validation epoch  87 | time: 53.76s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 168.17 | loss  2.91 |
| epoch  88 |   200/  475 batches | ms/batch 149.78 | loss  3.19 |
| epoch  88 |   300/  475 batches | ms/batch 144.81 | loss  3.17 |
| epoch  88 |   400/  475 batches | ms/batch 143.06 | loss  3.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 68.02s | training loss  2.98 |
    | end of validation epoch  88 | time: 54.35s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 166.69 | loss  3.01 |
| epoch  89 |   200/  475 batches | ms/batch 151.49 | loss  2.82 |
| epoch  89 |   300/  475 batches | ms/batch 145.16 | loss  2.89 |
| epoch  89 |   400/  475 batches | ms/batch 143.23 | loss  3.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 68.04s | training loss  2.98 |
    | end of validation epoch  89 | time: 55.29s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 161.23 | loss  3.04 |
| epoch  90 |   200/  475 batches | ms/batch 148.69 | loss  2.83 |
| epoch  90 |   300/  475 batches | ms/batch 144.26 | loss  3.04 |
| epoch  90 |   400/  475 batches | ms/batch 141.91 | loss  2.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 67.49s | training loss  2.97 |
    | end of validation epoch  90 | time: 55.29s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 165.91 | loss  3.13 |
| epoch  91 |   200/  475 batches | ms/batch 151.01 | loss  2.91 |
| epoch  91 |   300/  475 batches | ms/batch 145.55 | loss  3.44 |
| epoch  91 |   400/  475 batches | ms/batch 143.60 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 68.13s | training loss  2.96 |
    | end of validation epoch  91 | time: 55.57s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 160.96 | loss  2.97 |
| epoch  92 |   200/  475 batches | ms/batch 151.17 | loss  2.97 |
| epoch  92 |   300/  475 batches | ms/batch 144.25 | loss  3.21 |
| epoch  92 |   400/  475 batches | ms/batch 142.13 | loss  2.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 67.40s | training loss  2.96 |
    | end of validation epoch  92 | time: 55.42s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 161.07 | loss  2.99 |
| epoch  93 |   200/  475 batches | ms/batch 149.95 | loss  3.04 |
| epoch  93 |   300/  475 batches | ms/batch 145.64 | loss  2.09 |
| epoch  93 |   400/  475 batches | ms/batch 142.73 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 67.76s | training loss  2.95 |
    | end of validation epoch  93 | time: 54.11s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 165.99 | loss  2.91 |
| epoch  94 |   200/  475 batches | ms/batch 150.92 | loss  3.59 |
| epoch  94 |   300/  475 batches | ms/batch 146.33 | loss  2.73 |
| epoch  94 |   400/  475 batches | ms/batch 143.58 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 68.05s | training loss  2.94 |
    | end of validation epoch  94 | time: 55.26s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 155.47 | loss  2.79 |
| epoch  95 |   200/  475 batches | ms/batch 149.11 | loss  2.95 |
| epoch  95 |   300/  475 batches | ms/batch 146.03 | loss  3.01 |
| epoch  95 |   400/  475 batches | ms/batch 142.98 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 67.65s | training loss  2.95 |
    | end of validation epoch  95 | time: 54.86s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734, 2.9502757740020753] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194, 2.5365526345597598]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 164.07 | loss  3.55 |
| epoch  96 |   200/  475 batches | ms/batch 151.40 | loss  3.15 |
| epoch  96 |   300/  475 batches | ms/batch 147.46 | loss  3.23 |
| epoch  96 |   400/  475 batches | ms/batch 144.81 | loss  2.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 68.27s | training loss  2.93 |
    | end of validation epoch  96 | time: 54.73s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734, 2.9502757740020753, 2.929102588452791] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194, 2.5365526345597598, 2.4989059712706494]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 163.46 | loss  2.65 |
| epoch  97 |   200/  475 batches | ms/batch 150.61 | loss  2.94 |
| epoch  97 |   300/  475 batches | ms/batch 145.69 | loss  3.08 |
| epoch  97 |   400/  475 batches | ms/batch 143.93 | loss  3.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 68.24s | training loss  2.93 |
    | end of validation epoch  97 | time: 56.07s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734, 2.9502757740020753, 2.929102588452791, 2.9296277758949683] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194, 2.5365526345597598, 2.4989059712706494, 2.473085573741368]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 165.27 | loss  2.66 |
| epoch  98 |   200/  475 batches | ms/batch 150.54 | loss  2.71 |
| epoch  98 |   300/  475 batches | ms/batch 147.02 | loss  3.07 |
| epoch  98 |   400/  475 batches | ms/batch 143.99 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 68.47s | training loss  2.91 |
    | end of validation epoch  98 | time: 54.36s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734, 2.9502757740020753, 2.929102588452791, 2.9296277758949683, 2.9083974742889405] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194, 2.5365526345597598, 2.4989059712706494, 2.473085573741368, 2.4883326091686215]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 154.30 | loss  2.91 |
| epoch  99 |   200/  475 batches | ms/batch 150.59 | loss  2.53 |
| epoch  99 |   300/  475 batches | ms/batch 145.48 | loss  3.11 |
| epoch  99 |   400/  475 batches | ms/batch 143.68 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 68.00s | training loss  2.92 |
    | end of validation epoch  99 | time: 55.32s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.704766763385973, 5.138340708080091, 4.832984428405762, 4.637877460278963, 4.494107621343512, 4.380822641975001, 4.281275984613519, 4.195815588298597, 4.116303967927632, 4.065139359925922, 4.022628865994905, 3.974079747450979, 3.9282528932471026, 3.8844620609283447, 3.8396796351984928, 3.8069492591054814, 3.789716517799779, 3.737594564839413, 3.7211866117778576, 3.690979623292622, 3.67925452382941, 3.6463832333213406, 3.6114331094842207, 3.6097484819512617, 3.58340110829002, 3.5545826515398526, 3.5560138175362033, 3.545117342095626, 3.500788964221352, 3.498891506697002, 3.4924339851580166, 3.452300766894692, 3.4524956577702572, 3.42903151512146, 3.4189969002573113, 3.405499610900879, 3.3947958173249897, 3.383650611576281, 3.37292885981108, 3.342033814380043, 3.351612820876272, 3.328953016682675, 3.325208989193565, 3.3178945335588956, 3.2931429837879382, 3.298730606279875, 3.288314659720973, 3.2773429007279247, 3.2749078108135024, 3.258245163967735, 3.2567310498890123, 3.230803400842767, 3.225947598407143, 3.2139862763254268, 3.2150655967310855, 3.1870023190347774, 3.186273374055561, 3.1906090078855818, 3.1774590954027677, 3.194858060134085, 3.157351386421605, 3.1562444466038753, 3.148040613877146, 3.143332311228702, 3.133848829269409, 3.132982100436562, 3.113320099680047, 3.1111039944698935, 3.1028695974851908, 3.09166809182418, 3.087708537955033, 3.0874250908901817, 3.067671503769724, 3.05481908045317, 3.049197884107891, 3.0695083432448538, 3.0367941831287584, 3.050225311078523, 3.0313363406532687, 3.042821693922344, 3.027039471676475, 3.035936006746794, 3.0209493963341965, 3.0087011568169846, 3.0049303943232486, 2.991054697036743, 2.9818444211859454, 2.9791717579490262, 2.9784883499145507, 2.968276946419164, 2.962635122098421, 2.957415646502846, 2.951868924090737, 2.9401346231761734, 2.9502757740020753, 2.929102588452791, 2.9296277758949683, 2.9083974742889405, 2.9246387647327623] validation loss is  [5.08311832251669, 4.542349663101325, 4.350232234522074, 3.9823808269340453, 3.7739394452391553, 3.6593402634147836, 3.5558976065210937, 3.4256142367835807, 3.383131508065873, 3.3117779543419847, 3.2918909517656854, 3.262906763733936, 3.243973940360446, 3.1755377945779752, 3.129832808710948, 3.0956042994972037, 3.103475797076185, 3.0635582799671077, 2.988784192990856, 2.9647521832410026, 2.9762517684648016, 2.927585802158388, 2.97511572196704, 2.984658183169966, 2.9244014154963134, 2.8389221740370036, 2.889728281678272, 2.9172374721334764, 2.850961999732907, 2.8310894064542627, 2.796086034854921, 2.8891217588376596, 2.7511470317840576, 2.807722332096901, 2.770591533484579, 2.770967221059719, 2.718525816412533, 2.7157964986913345, 2.7753147257476294, 2.7252238778507007, 2.7010605064760735, 2.6985455711348716, 2.756201967471788, 2.677911257543484, 2.6930114862297763, 2.660532362320844, 2.607806514291202, 2.6426535824767683, 2.693504700139791, 2.593592052700139, 2.6237293622073006, 2.600073566957682, 2.6175629661864592, 2.5999719896236386, 2.603683427602303, 2.583985126319052, 2.6120771480207683, 2.6131625556144393, 2.6873205058714924, 2.5891070936908243, 2.6043388282551483, 2.6589329182600774, 2.5873596067188167, 2.527326673018832, 2.602835196406901, 2.560369394406551, 2.5707674487298275, 2.5732271521031356, 2.538011556913873, 2.524373011428769, 2.5496645755126695, 2.5422682561794248, 2.566058021633565, 2.5047933864994207, 2.529256895810616, 2.5308115502365496, 2.593282392045029, 2.5240096064174877, 2.4949868246286857, 2.499190690136757, 2.5430337110487353, 2.552536714978579, 2.5669292091321543, 2.4874407694119367, 2.5553331315016545, 2.5217828540241016, 2.502001671230092, 2.5307648302126333, 2.5398166961028794, 2.4888204105761873, 2.504151853192754, 2.5194026652504418, 2.505077725698968, 2.543638897543194, 2.5365526345597598, 2.4989059712706494, 2.473085573741368, 2.4883326091686215, 2.511787196167377]
