/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 958.60 | loss  2.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 102.77s | training loss  2.94 |
    | end of validation epoch   1 | time: 64.05s | validation loss  2.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.9427650468843476] validation loss is  [2.058871328830719]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1000.97 | loss  2.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 106.96s | training loss  2.65 |
    | end of validation epoch   2 | time: 63.20s | validation loss  1.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.9427650468843476, 2.6502653865126877] validation loss is  [2.058871328830719, 1.8318764666716258]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 927.53 | loss  2.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 99.79s | training loss  2.44 |
    | end of validation epoch   3 | time: 62.55s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 923.30 | loss  2.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 99.46s | training loss  2.28 |
    | end of validation epoch   4 | time: 62.70s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 928.96 | loss  1.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 99.91s | training loss  2.16 |
    | end of validation epoch   5 | time: 63.09s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 918.20 | loss  1.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 99.28s | training loss  2.08 |
    | end of validation epoch   6 | time: 62.80s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 914.07 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 98.20s | training loss  2.01 |
    | end of validation epoch   7 | time: 62.86s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 917.17 | loss  2.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 98.69s | training loss  1.97 |
    | end of validation epoch   8 | time: 62.43s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 914.75 | loss  1.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 98.25s | training loss  1.86 |
    | end of validation epoch   9 | time: 63.08s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 930.22 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 100.18s | training loss  1.83 |
    | end of validation epoch  10 | time: 63.60s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 927.23 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 100.56s | training loss  1.79 |
    | end of validation epoch  11 | time: 64.14s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 934.07 | loss  1.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 100.40s | training loss  1.77 |
    | end of validation epoch  12 | time: 63.60s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 928.98 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 99.89s | training loss  1.74 |
    | end of validation epoch  13 | time: 63.75s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 923.16 | loss  1.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 99.80s | training loss  1.69 |
    | end of validation epoch  14 | time: 63.78s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 925.60 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 99.52s | training loss  1.69 |
    | end of validation epoch  15 | time: 63.28s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 929.91 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 99.91s | training loss  1.65 |
    | end of validation epoch  16 | time: 67.28s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 925.90 | loss  1.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 99.48s | training loss  1.63 |
    | end of validation epoch  17 | time: 63.42s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 922.94 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 99.27s | training loss  1.59 |
    | end of validation epoch  18 | time: 64.20s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 932.84 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 100.01s | training loss  1.58 |
    | end of validation epoch  19 | time: 62.60s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 915.76 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 98.37s | training loss  1.55 |
    | end of validation epoch  20 | time: 62.28s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 915.36 | loss  1.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 98.23s | training loss  1.57 |
    | end of validation epoch  21 | time: 62.48s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 911.85 | loss  1.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 98.05s | training loss  1.54 |
    | end of validation epoch  22 | time: 62.22s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 910.72 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 97.98s | training loss  1.52 |
    | end of validation epoch  23 | time: 61.80s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 912.56 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 98.04s | training loss  1.51 |
    | end of validation epoch  24 | time: 61.92s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 917.40 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 98.61s | training loss  1.53 |
    | end of validation epoch  25 | time: 62.85s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418]
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 925.44 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 99.89s | training loss  1.51 |
    | end of validation epoch  26 | time: 63.96s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 919.87 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 99.32s | training loss  1.49 |
    | end of validation epoch  27 | time: 63.63s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 921.42 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 99.57s | training loss  1.48 |
    | end of validation epoch  28 | time: 63.60s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 927.03 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 99.68s | training loss  1.48 |
    | end of validation epoch  29 | time: 63.55s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268]
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 920.40 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 99.42s | training loss  1.47 |
    | end of validation epoch  30 | time: 63.29s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 924.60 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 99.43s | training loss  1.46 |
    | end of validation epoch  31 | time: 63.70s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 922.26 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 99.75s | training loss  1.45 |
    | end of validation epoch  32 | time: 63.12s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 928.63 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 100.00s | training loss  1.46 |
    | end of validation epoch  33 | time: 63.51s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 926.61 | loss  1.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 99.44s | training loss  1.46 |
    | end of validation epoch  34 | time: 62.69s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 917.42 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 98.73s | training loss  1.45 |
    | end of validation epoch  35 | time: 62.04s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 914.42 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 98.43s | training loss  1.44 |
    | end of validation epoch  36 | time: 62.81s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 916.04 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 98.25s | training loss  1.43 |
    | end of validation epoch  37 | time: 63.22s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 913.92 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 98.12s | training loss  1.43 |
    | end of validation epoch  38 | time: 62.29s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457]
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 911.51 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 98.10s | training loss  1.43 |
    | end of validation epoch  39 | time: 62.25s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 932.47 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 100.41s | training loss  1.42 |
    | end of validation epoch  40 | time: 62.70s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 918.78 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 98.62s | training loss  1.42 |
    | end of validation epoch  41 | time: 62.32s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 944.14 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 102.14s | training loss  1.42 |
    | end of validation epoch  42 | time: 66.00s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 931.25 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 100.14s | training loss  1.42 |
    | end of validation epoch  43 | time: 62.93s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 924.00 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 99.16s | training loss  1.41 |
    | end of validation epoch  44 | time: 63.12s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 921.21 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 98.99s | training loss  1.43 |
    | end of validation epoch  45 | time: 69.33s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 915.40 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 98.94s | training loss  1.42 |
    | end of validation epoch  46 | time: 63.11s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 919.65 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 98.82s | training loss  1.41 |
    | end of validation epoch  47 | time: 62.45s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 919.48 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 98.81s | training loss  1.40 |
    | end of validation epoch  48 | time: 62.71s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 920.12 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 98.78s | training loss  1.40 |
    | end of validation epoch  49 | time: 62.98s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 915.62 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 98.41s | training loss  1.42 |
    | end of validation epoch  50 | time: 62.93s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 918.12 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 98.49s | training loss  1.41 |
    | end of validation epoch  51 | time: 62.41s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 919.84 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 98.89s | training loss  1.42 |
    | end of validation epoch  52 | time: 62.71s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 919.22 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 98.61s | training loss  1.40 |
    | end of validation epoch  53 | time: 62.98s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 919.59 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 98.71s | training loss  1.40 |
    | end of validation epoch  54 | time: 62.56s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 916.16 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 98.38s | training loss  1.39 |
    | end of validation epoch  55 | time: 62.30s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 916.33 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 99.53s | training loss  1.40 |
    | end of validation epoch  56 | time: 62.63s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 916.43 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 98.65s | training loss  1.41 |
    | end of validation epoch  57 | time: 62.00s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 922.00 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 98.85s | training loss  1.41 |
    | end of validation epoch  58 | time: 62.08s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 910.01 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 98.69s | training loss  1.40 |
    | end of validation epoch  59 | time: 62.82s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 917.61 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 98.68s | training loss  1.39 |
    | end of validation epoch  60 | time: 62.40s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 915.20 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 98.95s | training loss  1.41 |
    | end of validation epoch  61 | time: 62.89s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 921.86 | loss  1.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 98.88s | training loss  1.39 |
    | end of validation epoch  62 | time: 62.66s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 917.67 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 98.91s | training loss  1.38 |
    | end of validation epoch  63 | time: 62.86s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 923.08 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 99.13s | training loss  1.38 |
    | end of validation epoch  64 | time: 62.08s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 931.90 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 100.20s | training loss  1.39 |
    | end of validation epoch  65 | time: 61.73s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 919.98 | loss  1.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 98.78s | training loss  1.38 |
    | end of validation epoch  66 | time: 62.41s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 918.46 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 99.93s | training loss  1.40 |
    | end of validation epoch  67 | time: 62.61s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 921.44 | loss  1.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 99.01s | training loss  1.38 |
    | end of validation epoch  68 | time: 64.22s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 918.59 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 99.02s | training loss  1.39 |
    | end of validation epoch  69 | time: 62.65s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 923.25 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 99.16s | training loss  1.41 |
    | end of validation epoch  70 | time: 62.68s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 919.93 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 99.26s | training loss  1.39 |
    | end of validation epoch  71 | time: 62.55s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 920.37 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 98.98s | training loss  1.37 |
    | end of validation epoch  72 | time: 63.28s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 923.95 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 99.45s | training loss  1.38 |
    | end of validation epoch  73 | time: 62.60s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 919.92 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 99.14s | training loss  1.39 |
    | end of validation epoch  74 | time: 62.68s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 918.60 | loss  1.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 98.68s | training loss  1.40 |
    | end of validation epoch  75 | time: 62.68s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 922.83 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 99.01s | training loss  1.40 |
    | end of validation epoch  76 | time: 62.26s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 927.78 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 99.71s | training loss  1.40 |
    | end of validation epoch  77 | time: 63.43s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 918.98 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 99.00s | training loss  1.38 |
    | end of validation epoch  78 | time: 62.41s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 916.48 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 98.41s | training loss  1.40 |
    | end of validation epoch  79 | time: 62.63s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 915.90 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 98.82s | training loss  1.41 |
    | end of validation epoch  80 | time: 62.40s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 919.84 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 98.90s | training loss  1.40 |
    | end of validation epoch  81 | time: 62.74s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 914.08 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 98.80s | training loss  1.39 |
    | end of validation epoch  82 | time: 62.87s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 914.71 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 98.74s | training loss  1.40 |
    | end of validation epoch  83 | time: 62.44s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 924.50 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 99.15s | training loss  1.39 |
    | end of validation epoch  84 | time: 62.53s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 918.67 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 99.09s | training loss  1.38 |
    | end of validation epoch  85 | time: 63.20s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 988.47 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 105.89s | training loss  1.41 |
    | end of validation epoch  86 | time: 63.96s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 926.08 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 99.29s | training loss  1.40 |
    | end of validation epoch  87 | time: 62.94s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 923.42 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 99.52s | training loss  1.40 |
    | end of validation epoch  88 | time: 62.90s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 923.30 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 99.19s | training loss  1.38 |
    | end of validation epoch  89 | time: 62.68s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 917.37 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 98.38s | training loss  1.40 |
    | end of validation epoch  90 | time: 62.59s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 921.31 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 99.03s | training loss  1.38 |
    | end of validation epoch  91 | time: 62.63s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 916.94 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 98.86s | training loss  1.38 |
    | end of validation epoch  92 | time: 62.48s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 921.41 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 99.04s | training loss  1.40 |
    | end of validation epoch  93 | time: 62.48s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 918.75 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 98.45s | training loss  1.39 |
    | end of validation epoch  94 | time: 62.64s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 918.36 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 98.81s | training loss  1.40 |
    | end of validation epoch  95 | time: 62.57s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315, 1.398182520995269] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616, 1.1105636805295944]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 915.77 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 99.25s | training loss  1.38 |
    | end of validation epoch  96 | time: 62.56s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315, 1.398182520995269, 1.376554940197919] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616, 1.1105636805295944, 1.1096862492461999]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 916.05 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 98.69s | training loss  1.39 |
    | end of validation epoch  97 | time: 62.94s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315, 1.398182520995269, 1.376554940197919, 1.3879112509993818] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616, 1.1105636805295944, 1.1096862492461999, 1.1085178622355063]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 916.03 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 98.58s | training loss  1.40 |
    | end of validation epoch  98 | time: 64.99s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315, 1.398182520995269, 1.376554940197919, 1.3879112509993818, 1.3962978489764102] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616, 1.1105636805295944, 1.1096862492461999, 1.1085178622355063, 1.103465657370786]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 989.10 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 106.45s | training loss  1.40 |
    | end of validation epoch  99 | time: 64.26s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.9427650468843476, 2.6502653865126877, 2.437672522690919, 2.282025992333352, 2.157313234097249, 2.075500574197855, 2.005149986292865, 1.9664857763427872, 1.86229208997778, 1.8332187205821544, 1.7907311465289142, 1.7691559566033852, 1.7404078309600417, 1.6948879420220315, 1.6908231726637832, 1.6462424263223872, 1.6337073794356338, 1.5879229350132986, 1.5807893823932957, 1.5497090859456106, 1.5657891174694438, 1.5391737340806841, 1.5211903436763867, 1.5122202301884557, 1.527930524972108, 1.5102694850784164, 1.4917150606980194, 1.4773747179959271, 1.4779858148849763, 1.4742349665444177, 1.4551730467392519, 1.454565100841694, 1.4565396856617283, 1.4590654963845606, 1.4450931720905476, 1.4404184399424373, 1.4273312499931268, 1.4305037034524453, 1.4331411821348172, 1.4211654985273205, 1.4200083683202933, 1.4245747870153136, 1.423880139986674, 1.4147102564304799, 1.4344758675979064, 1.4163345538818084, 1.4110543341250033, 1.4010014115153133, 1.3999960358078416, 1.4208940761583346, 1.4130520283638894, 1.421758202819137, 1.3982462421193853, 1.396904970074559, 1.3938905870592273, 1.3997171816525158, 1.4115859130481343, 1.4118900137978632, 1.3978441878482029, 1.3937636044648316, 1.411048156721098, 1.3894680652532492, 1.3813595385164827, 1.3772571387591663, 1.3881686184857343, 1.3842223453092146, 1.3995063149177276, 1.379077385137747, 1.3898856360633094, 1.41097287014798, 1.387957739400434, 1.3745306085895848, 1.3844429415625494, 1.3891947086866911, 1.3977200050611753, 1.3950689880697578, 1.4019351123689532, 1.3844152270136654, 1.396228502462576, 1.4091925030356054, 1.4030764468081363, 1.3925450900653462, 1.4005551370414528, 1.3937681365657497, 1.3823745325878933, 1.4063317754247167, 1.3957013847591642, 1.3965591278162088, 1.3791714509328206, 1.402232891804463, 1.378367491670557, 1.3820170346681062, 1.401839911400735, 1.3921591473055315, 1.398182520995269, 1.376554940197919, 1.3879112509993818, 1.3962978489764102, 1.3981224148123115] validation loss is  [2.058871328830719, 1.8318764666716258, 1.6786836410562198, 1.58327699949344, 1.5065198180576165, 1.4519841969013214, 1.4449032470583916, 1.370622742921114, 1.3535219331582387, 1.339154868076245, 1.330602388208111, 1.2921290112038453, 1.2816136479377747, 1.2562897118429344, 1.2412790016581614, 1.2359096202999353, 1.2234038629879553, 1.2183248586952686, 1.2211284457395475, 1.210454850147168, 1.197725446894765, 1.1920775535206, 1.2055007833987474, 1.1929035391658545, 1.1954922166963418, 1.1809829091653228, 1.1661040146524708, 1.1730658908685048, 1.1841204135368268, 1.1753483085582654, 1.1635338322569926, 1.1631845152005553, 1.1721176064262788, 1.1583113151912887, 1.1514394224310915, 1.1427686559036374, 1.1615674473966162, 1.153550351659457, 1.1424302477389574, 1.1477584916477401, 1.1283558057621121, 1.1407140629986923, 1.135425833053887, 1.133283423880736, 1.131194615115722, 1.1279313353200753, 1.121906709857285, 1.125023274992903, 1.125901283385853, 1.112555927907427, 1.1169942924752831, 1.11834233161062, 1.1175844455137849, 1.1267386473094423, 1.129918220763405, 1.1211678584416707, 1.1203616388762991, 1.1155084272225697, 1.140661536405484, 1.1087636699279149, 1.1262892975161474, 1.1281314340109627, 1.1040673082073529, 1.1186407152563334, 1.1024511555830638, 1.1151969116181135, 1.1136466848353546, 1.1063684566567342, 1.1103356396779418, 1.116211178402106, 1.1135127016653616, 1.10635747294873, 1.105627338712414, 1.1083099357783794, 1.1125156624863546, 1.09988742445906, 1.1244641784578562, 1.1237524741639693, 1.123427187403043, 1.098823935414354, 1.0986170436566074, 1.117524861668547, 1.1214710328107078, 1.0954596965263288, 1.104169371848305, 1.11350226867944, 1.1200985743974645, 1.1023733684172232, 1.1034650340055425, 1.103899775383373, 1.101926850154996, 1.0948548813660939, 1.1134674223139882, 1.1074754626800616, 1.1105636805295944, 1.1096862492461999, 1.1085178622355063, 1.103465657370786, 1.1083403717105587]
