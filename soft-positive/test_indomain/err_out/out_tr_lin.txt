/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_soumya_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 210.31 | loss  6.94 |
| epoch   1 |   200/  475 batches | ms/batch 181.44 | loss  6.65 |
| epoch   1 |   300/  475 batches | ms/batch 174.41 | loss  6.51 |
| epoch   1 |   400/  475 batches | ms/batch 169.88 | loss  6.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 80.06s | training loss  6.76 |
    | end of validation epoch   1 | time: 55.96s | validation loss  5.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.758421798505281] validation loss is  [5.460700840509238]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 183.57 | loss  6.45 |
| epoch   2 |   200/  475 batches | ms/batch 166.84 | loss  6.46 |
| epoch   2 |   300/  475 batches | ms/batch 165.43 | loss  6.10 |
| epoch   2 |   400/  475 batches | ms/batch 161.01 | loss  6.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 75.97s | training loss  6.26 |
    | end of validation epoch   2 | time: 55.47s | validation loss  5.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.758421798505281, 6.261438448052657] validation loss is  [5.460700840509238, 5.098739219312908]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 188.94 | loss  6.23 |
| epoch   3 |   200/  475 batches | ms/batch 168.69 | loss  6.13 |
| epoch   3 |   300/  475 batches | ms/batch 164.50 | loss  5.66 |
| epoch   3 |   400/  475 batches | ms/batch 162.94 | loss  5.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 77.20s | training loss  5.92 |
    | end of validation epoch   3 | time: 56.97s | validation loss  4.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 177.70 | loss  5.69 |
| epoch   4 |   200/  475 batches | ms/batch 167.05 | loss  5.70 |
| epoch   4 |   300/  475 batches | ms/batch 160.99 | loss  5.65 |
| epoch   4 |   400/  475 batches | ms/batch 160.32 | loss  5.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 75.64s | training loss  5.68 |
    | end of validation epoch   4 | time: 54.55s | validation loss  4.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 174.04 | loss  5.51 |
| epoch   5 |   200/  475 batches | ms/batch 170.07 | loss  5.55 |
| epoch   5 |   300/  475 batches | ms/batch 166.65 | loss  5.44 |
| epoch   5 |   400/  475 batches | ms/batch 164.16 | loss  5.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 77.84s | training loss  5.48 |
    | end of validation epoch   5 | time: 54.83s | validation loss  4.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 176.32 | loss  5.08 |
| epoch   6 |   200/  475 batches | ms/batch 164.36 | loss  5.46 |
| epoch   6 |   300/  475 batches | ms/batch 161.82 | loss  5.34 |
| epoch   6 |   400/  475 batches | ms/batch 159.55 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 76.15s | training loss  5.31 |
    | end of validation epoch   6 | time: 53.64s | validation loss  4.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 179.75 | loss  5.07 |
| epoch   7 |   200/  475 batches | ms/batch 168.53 | loss  5.24 |
| epoch   7 |   300/  475 batches | ms/batch 164.80 | loss  5.41 |
| epoch   7 |   400/  475 batches | ms/batch 162.40 | loss  5.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 77.66s | training loss  5.18 |
    | end of validation epoch   7 | time: 53.88s | validation loss  4.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 172.99 | loss  5.46 |
| epoch   8 |   200/  475 batches | ms/batch 169.14 | loss  5.04 |
| epoch   8 |   300/  475 batches | ms/batch 164.57 | loss  5.21 |
| epoch   8 |   400/  475 batches | ms/batch 161.88 | loss  4.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 76.78s | training loss  5.08 |
    | end of validation epoch   8 | time: 55.30s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 180.32 | loss  5.10 |
| epoch   9 |   200/  475 batches | ms/batch 167.35 | loss  4.92 |
| epoch   9 |   300/  475 batches | ms/batch 162.54 | loss  4.96 |
| epoch   9 |   400/  475 batches | ms/batch 161.01 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 76.62s | training loss  4.99 |
    | end of validation epoch   9 | time: 56.90s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 188.98 | loss  4.56 |
| epoch  10 |   200/  475 batches | ms/batch 170.76 | loss  5.06 |
| epoch  10 |   300/  475 batches | ms/batch 166.88 | loss  4.50 |
| epoch  10 |   400/  475 batches | ms/batch 164.66 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 77.78s | training loss  4.89 |
    | end of validation epoch  10 | time: 55.31s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 192.83 | loss  4.89 |
| epoch  11 |   200/  475 batches | ms/batch 171.66 | loss  4.74 |
| epoch  11 |   300/  475 batches | ms/batch 166.90 | loss  4.89 |
| epoch  11 |   400/  475 batches | ms/batch 165.20 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 78.26s | training loss  4.82 |
    | end of validation epoch  11 | time: 55.32s | validation loss  3.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 182.73 | loss  4.75 |
| epoch  12 |   200/  475 batches | ms/batch 164.61 | loss  5.09 |
| epoch  12 |   300/  475 batches | ms/batch 163.18 | loss  4.89 |
| epoch  12 |   400/  475 batches | ms/batch 162.53 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 77.07s | training loss  4.76 |
    | end of validation epoch  12 | time: 54.19s | validation loss  3.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 191.68 | loss  4.95 |
| epoch  13 |   200/  475 batches | ms/batch 170.99 | loss  4.96 |
| epoch  13 |   300/  475 batches | ms/batch 165.13 | loss  4.39 |
| epoch  13 |   400/  475 batches | ms/batch 163.20 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 77.32s | training loss  4.72 |
    | end of validation epoch  13 | time: 52.46s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 185.34 | loss  4.66 |
| epoch  14 |   200/  475 batches | ms/batch 169.43 | loss  4.90 |
| epoch  14 |   300/  475 batches | ms/batch 165.86 | loss  4.47 |
| epoch  14 |   400/  475 batches | ms/batch 163.19 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 77.26s | training loss  4.66 |
    | end of validation epoch  14 | time: 52.36s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 192.90 | loss  4.33 |
| epoch  15 |   200/  475 batches | ms/batch 172.83 | loss  4.93 |
| epoch  15 |   300/  475 batches | ms/batch 167.79 | loss  4.70 |
| epoch  15 |   400/  475 batches | ms/batch 166.06 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 78.58s | training loss  4.61 |
    | end of validation epoch  15 | time: 55.98s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 185.64 | loss  4.83 |
| epoch  16 |   200/  475 batches | ms/batch 168.55 | loss  4.86 |
| epoch  16 |   300/  475 batches | ms/batch 161.10 | loss  4.43 |
| epoch  16 |   400/  475 batches | ms/batch 161.14 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 75.90s | training loss  4.57 |
    | end of validation epoch  16 | time: 54.06s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 187.89 | loss  4.90 |
| epoch  17 |   200/  475 batches | ms/batch 170.53 | loss  4.67 |
| epoch  17 |   300/  475 batches | ms/batch 166.02 | loss  4.76 |
| epoch  17 |   400/  475 batches | ms/batch 164.71 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 77.96s | training loss  4.54 |
    | end of validation epoch  17 | time: 55.33s | validation loss  3.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 172.01 | loss  4.63 |
| epoch  18 |   200/  475 batches | ms/batch 170.75 | loss  4.50 |
| epoch  18 |   300/  475 batches | ms/batch 165.29 | loss  4.64 |
| epoch  18 |   400/  475 batches | ms/batch 163.82 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 77.76s | training loss  4.52 |
    | end of validation epoch  18 | time: 53.68s | validation loss  3.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 175.06 | loss  4.81 |
| epoch  19 |   200/  475 batches | ms/batch 167.38 | loss  4.40 |
| epoch  19 |   300/  475 batches | ms/batch 166.04 | loss  4.24 |
| epoch  19 |   400/  475 batches | ms/batch 163.83 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 77.79s | training loss  4.48 |
    | end of validation epoch  19 | time: 54.31s | validation loss  3.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 171.95 | loss  4.35 |
| epoch  20 |   200/  475 batches | ms/batch 156.94 | loss  4.06 |
| epoch  20 |   300/  475 batches | ms/batch 161.01 | loss  4.54 |
| epoch  20 |   400/  475 batches | ms/batch 160.79 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 76.25s | training loss  4.46 |
    | end of validation epoch  20 | time: 54.95s | validation loss  3.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 176.97 | loss  4.16 |
| epoch  21 |   200/  475 batches | ms/batch 165.91 | loss  4.58 |
| epoch  21 |   300/  475 batches | ms/batch 161.33 | loss  4.27 |
| epoch  21 |   400/  475 batches | ms/batch 160.39 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 76.00s | training loss  4.45 |
    | end of validation epoch  21 | time: 53.47s | validation loss  3.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 185.21 | loss  4.17 |
| epoch  22 |   200/  475 batches | ms/batch 167.65 | loss  4.39 |
| epoch  22 |   300/  475 batches | ms/batch 164.80 | loss  4.80 |
| epoch  22 |   400/  475 batches | ms/batch 163.34 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 77.58s | training loss  4.42 |
    | end of validation epoch  22 | time: 55.87s | validation loss  3.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 172.07 | loss  4.39 |
| epoch  23 |   200/  475 batches | ms/batch 165.62 | loss  4.64 |
| epoch  23 |   300/  475 batches | ms/batch 162.58 | loss  4.13 |
| epoch  23 |   400/  475 batches | ms/batch 161.04 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 76.64s | training loss  4.40 |
    | end of validation epoch  23 | time: 53.31s | validation loss  3.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 187.85 | loss  4.33 |
| epoch  24 |   200/  475 batches | ms/batch 169.59 | loss  4.42 |
| epoch  24 |   300/  475 batches | ms/batch 164.41 | loss  4.25 |
| epoch  24 |   400/  475 batches | ms/batch 162.56 | loss  4.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 77.17s | training loss  4.38 |
    | end of validation epoch  24 | time: 55.46s | validation loss  3.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 181.12 | loss  4.49 |
| epoch  25 |   200/  475 batches | ms/batch 167.80 | loss  4.25 |
| epoch  25 |   300/  475 batches | ms/batch 162.72 | loss  4.17 |
| epoch  25 |   400/  475 batches | ms/batch 161.38 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 76.58s | training loss  4.38 |
    | end of validation epoch  25 | time: 55.55s | validation loss  3.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 166.59 | loss  4.51 |
| epoch  26 |   200/  475 batches | ms/batch 164.06 | loss  4.43 |
| epoch  26 |   300/  475 batches | ms/batch 162.45 | loss  4.57 |
| epoch  26 |   400/  475 batches | ms/batch 161.41 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 76.57s | training loss  4.36 |
    | end of validation epoch  26 | time: 56.97s | validation loss  3.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 183.59 | loss  3.86 |
| epoch  27 |   200/  475 batches | ms/batch 171.48 | loss  4.34 |
| epoch  27 |   300/  475 batches | ms/batch 164.25 | loss  4.11 |
| epoch  27 |   400/  475 batches | ms/batch 162.99 | loss  4.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 77.02s | training loss  4.33 |
    | end of validation epoch  27 | time: 53.35s | validation loss  3.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 191.95 | loss  4.60 |
| epoch  28 |   200/  475 batches | ms/batch 170.72 | loss  4.55 |
| epoch  28 |   300/  475 batches | ms/batch 166.28 | loss  4.25 |
| epoch  28 |   400/  475 batches | ms/batch 164.86 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 78.63s | training loss  4.31 |
    | end of validation epoch  28 | time: 55.13s | validation loss  3.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 173.51 | loss  4.45 |
| epoch  29 |   200/  475 batches | ms/batch 168.55 | loss  4.23 |
| epoch  29 |   300/  475 batches | ms/batch 165.26 | loss  4.20 |
| epoch  29 |   400/  475 batches | ms/batch 163.28 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 77.41s | training loss  4.30 |
    | end of validation epoch  29 | time: 54.78s | validation loss  3.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 185.90 | loss  4.36 |
| epoch  30 |   200/  475 batches | ms/batch 168.81 | loss  4.35 |
| epoch  30 |   300/  475 batches | ms/batch 165.56 | loss  3.97 |
| epoch  30 |   400/  475 batches | ms/batch 164.07 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 78.44s | training loss  4.31 |
    | end of validation epoch  30 | time: 54.95s | validation loss  3.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 179.84 | loss  4.55 |
| epoch  31 |   200/  475 batches | ms/batch 165.63 | loss  4.34 |
| epoch  31 |   300/  475 batches | ms/batch 162.23 | loss  4.41 |
| epoch  31 |   400/  475 batches | ms/batch 161.33 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 76.72s | training loss  4.29 |
    | end of validation epoch  31 | time: 52.47s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 175.86 | loss  4.11 |
| epoch  32 |   200/  475 batches | ms/batch 169.19 | loss  4.65 |
| epoch  32 |   300/  475 batches | ms/batch 164.66 | loss  4.10 |
| epoch  32 |   400/  475 batches | ms/batch 161.48 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 76.56s | training loss  4.27 |
    | end of validation epoch  32 | time: 54.68s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 182.67 | loss  4.34 |
| epoch  33 |   200/  475 batches | ms/batch 164.34 | loss  4.24 |
| epoch  33 |   300/  475 batches | ms/batch 162.43 | loss  4.24 |
| epoch  33 |   400/  475 batches | ms/batch 160.08 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 76.17s | training loss  4.27 |
    | end of validation epoch  33 | time: 53.72s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 169.10 | loss  4.17 |
| epoch  34 |   200/  475 batches | ms/batch 157.77 | loss  4.31 |
| epoch  34 |   300/  475 batches | ms/batch 156.65 | loss  4.07 |
| epoch  34 |   400/  475 batches | ms/batch 156.26 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 73.99s | training loss  4.27 |
    | end of validation epoch  34 | time: 54.22s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 189.58 | loss  4.26 |
| epoch  35 |   200/  475 batches | ms/batch 170.21 | loss  4.43 |
| epoch  35 |   300/  475 batches | ms/batch 163.38 | loss  4.28 |
| epoch  35 |   400/  475 batches | ms/batch 160.83 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 76.41s | training loss  4.27 |
    | end of validation epoch  35 | time: 55.07s | validation loss  3.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316]
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 183.16 | loss  4.73 |
| epoch  36 |   200/  475 batches | ms/batch 165.84 | loss  4.68 |
| epoch  36 |   300/  475 batches | ms/batch 162.23 | loss  4.23 |
| epoch  36 |   400/  475 batches | ms/batch 160.27 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 76.17s | training loss  4.27 |
    | end of validation epoch  36 | time: 55.12s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 184.96 | loss  4.18 |
| epoch  37 |   200/  475 batches | ms/batch 168.97 | loss  3.93 |
| epoch  37 |   300/  475 batches | ms/batch 162.98 | loss  3.87 |
| epoch  37 |   400/  475 batches | ms/batch 159.21 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 75.78s | training loss  4.25 |
    | end of validation epoch  37 | time: 52.24s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 182.50 | loss  4.36 |
| epoch  38 |   200/  475 batches | ms/batch 170.25 | loss  4.50 |
| epoch  38 |   300/  475 batches | ms/batch 164.09 | loss  4.44 |
| epoch  38 |   400/  475 batches | ms/batch 162.93 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 77.48s | training loss  4.24 |
    | end of validation epoch  38 | time: 53.44s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 176.57 | loss  4.36 |
| epoch  39 |   200/  475 batches | ms/batch 166.06 | loss  4.11 |
| epoch  39 |   300/  475 batches | ms/batch 161.88 | loss  4.14 |
| epoch  39 |   400/  475 batches | ms/batch 160.56 | loss  3.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 76.27s | training loss  4.24 |
    | end of validation epoch  39 | time: 53.56s | validation loss  3.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 186.16 | loss  4.20 |
| epoch  40 |   200/  475 batches | ms/batch 169.11 | loss  4.06 |
| epoch  40 |   300/  475 batches | ms/batch 163.94 | loss  4.67 |
| epoch  40 |   400/  475 batches | ms/batch 161.87 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 77.00s | training loss  4.23 |
    | end of validation epoch  40 | time: 55.21s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 184.02 | loss  4.01 |
| epoch  41 |   200/  475 batches | ms/batch 165.42 | loss  4.28 |
| epoch  41 |   300/  475 batches | ms/batch 159.74 | loss  3.95 |
| epoch  41 |   400/  475 batches | ms/batch 158.78 | loss  4.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 75.64s | training loss  4.23 |
    | end of validation epoch  41 | time: 55.09s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 173.90 | loss  4.36 |
| epoch  42 |   200/  475 batches | ms/batch 164.67 | loss  4.43 |
| epoch  42 |   300/  475 batches | ms/batch 160.61 | loss  3.75 |
| epoch  42 |   400/  475 batches | ms/batch 160.13 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 75.63s | training loss  4.22 |
    | end of validation epoch  42 | time: 55.91s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 180.42 | loss  3.88 |
| epoch  43 |   200/  475 batches | ms/batch 164.17 | loss  4.43 |
| epoch  43 |   300/  475 batches | ms/batch 160.31 | loss  4.11 |
| epoch  43 |   400/  475 batches | ms/batch 158.97 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 75.94s | training loss  4.23 |
    | end of validation epoch  43 | time: 55.29s | validation loss  3.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114]
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 177.85 | loss  4.30 |
| epoch  44 |   200/  475 batches | ms/batch 167.15 | loss  4.32 |
| epoch  44 |   300/  475 batches | ms/batch 161.49 | loss  3.98 |
| epoch  44 |   400/  475 batches | ms/batch 160.15 | loss  3.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 76.30s | training loss  4.23 |
    | end of validation epoch  44 | time: 54.07s | validation loss  3.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235]
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 183.80 | loss  4.24 |
| epoch  45 |   200/  475 batches | ms/batch 167.83 | loss  4.49 |
| epoch  45 |   300/  475 batches | ms/batch 161.35 | loss  4.28 |
| epoch  45 |   400/  475 batches | ms/batch 159.52 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 76.38s | training loss  4.22 |
    | end of validation epoch  45 | time: 55.67s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 163.17 | loss  4.18 |
| epoch  46 |   200/  475 batches | ms/batch 163.90 | loss  3.98 |
| epoch  46 |   300/  475 batches | ms/batch 159.14 | loss  4.39 |
| epoch  46 |   400/  475 batches | ms/batch 158.41 | loss  4.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 75.56s | training loss  4.22 |
    | end of validation epoch  46 | time: 54.99s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 174.93 | loss  4.24 |
| epoch  47 |   200/  475 batches | ms/batch 163.99 | loss  4.30 |
| epoch  47 |   300/  475 batches | ms/batch 158.89 | loss  4.21 |
| epoch  47 |   400/  475 batches | ms/batch 157.13 | loss  3.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 74.86s | training loss  4.20 |
    | end of validation epoch  47 | time: 54.99s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 182.92 | loss  4.52 |
| epoch  48 |   200/  475 batches | ms/batch 168.79 | loss  4.31 |
| epoch  48 |   300/  475 batches | ms/batch 163.94 | loss  4.02 |
| epoch  48 |   400/  475 batches | ms/batch 159.71 | loss  4.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 76.15s | training loss  4.21 |
    | end of validation epoch  48 | time: 55.59s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 174.48 | loss  4.30 |
| epoch  49 |   200/  475 batches | ms/batch 168.26 | loss  4.13 |
| epoch  49 |   300/  475 batches | ms/batch 162.34 | loss  4.28 |
| epoch  49 |   400/  475 batches | ms/batch 160.69 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 76.20s | training loss  4.21 |
    | end of validation epoch  49 | time: 56.06s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 179.38 | loss  4.23 |
| epoch  50 |   200/  475 batches | ms/batch 164.48 | loss  4.30 |
| epoch  50 |   300/  475 batches | ms/batch 160.31 | loss  4.09 |
| epoch  50 |   400/  475 batches | ms/batch 158.09 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 75.65s | training loss  4.21 |
    | end of validation epoch  50 | time: 54.40s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464]
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 185.15 | loss  4.28 |
| epoch  51 |   200/  475 batches | ms/batch 166.40 | loss  4.04 |
| epoch  51 |   300/  475 batches | ms/batch 161.16 | loss  4.40 |
| epoch  51 |   400/  475 batches | ms/batch 159.38 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 76.05s | training loss  4.21 |
    | end of validation epoch  51 | time: 56.81s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014]
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 173.00 | loss  4.14 |
| epoch  52 |   200/  475 batches | ms/batch 164.81 | loss  4.34 |
| epoch  52 |   300/  475 batches | ms/batch 161.90 | loss  4.13 |
| epoch  52 |   400/  475 batches | ms/batch 159.35 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 76.44s | training loss  4.20 |
    | end of validation epoch  52 | time: 55.16s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701]
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 182.44 | loss  4.07 |
| epoch  53 |   200/  475 batches | ms/batch 164.85 | loss  4.52 |
| epoch  53 |   300/  475 batches | ms/batch 160.43 | loss  4.57 |
| epoch  53 |   400/  475 batches | ms/batch 158.67 | loss  4.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 75.67s | training loss  4.21 |
    | end of validation epoch  53 | time: 54.87s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632]
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 182.36 | loss  3.80 |
| epoch  54 |   200/  475 batches | ms/batch 166.65 | loss  4.16 |
| epoch  54 |   300/  475 batches | ms/batch 162.04 | loss  4.23 |
| epoch  54 |   400/  475 batches | ms/batch 160.22 | loss  4.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 76.35s | training loss  4.20 |
    | end of validation epoch  54 | time: 55.49s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 180.76 | loss  3.83 |
| epoch  55 |   200/  475 batches | ms/batch 166.72 | loss  4.31 |
| epoch  55 |   300/  475 batches | ms/batch 160.27 | loss  3.78 |
| epoch  55 |   400/  475 batches | ms/batch 158.10 | loss  4.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 75.20s | training loss  4.20 |
    | end of validation epoch  55 | time: 54.49s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036]
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 184.32 | loss  4.25 |
| epoch  56 |   200/  475 batches | ms/batch 168.91 | loss  4.22 |
| epoch  56 |   300/  475 batches | ms/batch 163.06 | loss  4.01 |
| epoch  56 |   400/  475 batches | ms/batch 161.44 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 76.62s | training loss  4.20 |
    | end of validation epoch  56 | time: 54.23s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 178.39 | loss  4.05 |
| epoch  57 |   200/  475 batches | ms/batch 164.30 | loss  4.07 |
| epoch  57 |   300/  475 batches | ms/batch 159.95 | loss  4.33 |
| epoch  57 |   400/  475 batches | ms/batch 158.04 | loss  4.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 76.04s | training loss  4.20 |
    | end of validation epoch  57 | time: 54.11s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 168.48 | loss  4.26 |
| epoch  58 |   200/  475 batches | ms/batch 155.16 | loss  4.22 |
| epoch  58 |   300/  475 batches | ms/batch 153.12 | loss  3.73 |
| epoch  58 |   400/  475 batches | ms/batch 157.55 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 73.81s | training loss  4.18 |
    | end of validation epoch  58 | time: 53.71s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 176.16 | loss  4.45 |
| epoch  59 |   200/  475 batches | ms/batch 161.74 | loss  4.32 |
| epoch  59 |   300/  475 batches | ms/batch 158.26 | loss  4.12 |
| epoch  59 |   400/  475 batches | ms/batch 157.43 | loss  4.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 75.48s | training loss  4.20 |
    | end of validation epoch  59 | time: 54.14s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 180.45 | loss  4.29 |
| epoch  60 |   200/  475 batches | ms/batch 164.60 | loss  4.13 |
| epoch  60 |   300/  475 batches | ms/batch 160.78 | loss  4.15 |
| epoch  60 |   400/  475 batches | ms/batch 157.88 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 75.53s | training loss  4.21 |
    | end of validation epoch  60 | time: 56.48s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587]
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 178.23 | loss  4.40 |
| epoch  61 |   200/  475 batches | ms/batch 163.25 | loss  4.40 |
| epoch  61 |   300/  475 batches | ms/batch 158.49 | loss  3.93 |
| epoch  61 |   400/  475 batches | ms/batch 157.72 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 74.97s | training loss  4.19 |
    | end of validation epoch  61 | time: 52.40s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032]
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 176.42 | loss  4.02 |
| epoch  62 |   200/  475 batches | ms/batch 157.27 | loss  4.07 |
| epoch  62 |   300/  475 batches | ms/batch 160.85 | loss  4.08 |
| epoch  62 |   400/  475 batches | ms/batch 159.03 | loss  3.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 75.99s | training loss  4.18 |
    | end of validation epoch  62 | time: 55.44s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 172.45 | loss  3.96 |
| epoch  63 |   200/  475 batches | ms/batch 163.55 | loss  4.40 |
| epoch  63 |   300/  475 batches | ms/batch 159.79 | loss  4.03 |
| epoch  63 |   400/  475 batches | ms/batch 156.88 | loss  3.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 75.02s | training loss  4.18 |
    | end of validation epoch  63 | time: 55.48s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 179.74 | loss  4.59 |
| epoch  64 |   200/  475 batches | ms/batch 163.79 | loss  4.55 |
| epoch  64 |   300/  475 batches | ms/batch 156.97 | loss  4.12 |
| epoch  64 |   400/  475 batches | ms/batch 156.03 | loss  4.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 74.63s | training loss  4.18 |
    | end of validation epoch  64 | time: 55.84s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029]
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 169.67 | loss  4.06 |
| epoch  65 |   200/  475 batches | ms/batch 162.10 | loss  4.04 |
| epoch  65 |   300/  475 batches | ms/batch 159.49 | loss  4.25 |
| epoch  65 |   400/  475 batches | ms/batch 156.96 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 74.76s | training loss  4.19 |
    | end of validation epoch  65 | time: 55.02s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 182.85 | loss  4.51 |
| epoch  66 |   200/  475 batches | ms/batch 166.24 | loss  4.60 |
| epoch  66 |   300/  475 batches | ms/batch 163.09 | loss  4.30 |
| epoch  66 |   400/  475 batches | ms/batch 160.50 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 76.38s | training loss  4.18 |
    | end of validation epoch  66 | time: 55.94s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 172.13 | loss  4.35 |
| epoch  67 |   200/  475 batches | ms/batch 160.52 | loss  3.98 |
| epoch  67 |   300/  475 batches | ms/batch 159.18 | loss  4.15 |
| epoch  67 |   400/  475 batches | ms/batch 157.69 | loss  4.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 75.14s | training loss  4.18 |
    | end of validation epoch  67 | time: 53.53s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966]
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 169.85 | loss  4.03 |
| epoch  68 |   200/  475 batches | ms/batch 166.13 | loss  4.11 |
| epoch  68 |   300/  475 batches | ms/batch 159.91 | loss  4.49 |
| epoch  68 |   400/  475 batches | ms/batch 160.98 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 76.18s | training loss  4.19 |
    | end of validation epoch  68 | time: 55.38s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 180.87 | loss  4.34 |
| epoch  69 |   200/  475 batches | ms/batch 166.45 | loss  4.10 |
| epoch  69 |   300/  475 batches | ms/batch 160.51 | loss  4.52 |
| epoch  69 |   400/  475 batches | ms/batch 159.68 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 76.14s | training loss  4.19 |
    | end of validation epoch  69 | time: 55.97s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 180.79 | loss  3.83 |
| epoch  70 |   200/  475 batches | ms/batch 165.66 | loss  3.91 |
| epoch  70 |   300/  475 batches | ms/batch 161.04 | loss  3.96 |
| epoch  70 |   400/  475 batches | ms/batch 158.87 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 75.84s | training loss  4.18 |
    | end of validation epoch  70 | time: 56.01s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 170.53 | loss  4.22 |
| epoch  71 |   200/  475 batches | ms/batch 165.28 | loss  3.97 |
| epoch  71 |   300/  475 batches | ms/batch 159.22 | loss  3.91 |
| epoch  71 |   400/  475 batches | ms/batch 158.31 | loss  4.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 75.39s | training loss  4.19 |
    | end of validation epoch  71 | time: 52.05s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 169.34 | loss  4.41 |
| epoch  72 |   200/  475 batches | ms/batch 167.50 | loss  4.63 |
| epoch  72 |   300/  475 batches | ms/batch 160.35 | loss  4.20 |
| epoch  72 |   400/  475 batches | ms/batch 158.00 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 75.48s | training loss  4.18 |
    | end of validation epoch  72 | time: 55.14s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 180.38 | loss  4.00 |
| epoch  73 |   200/  475 batches | ms/batch 164.66 | loss  4.08 |
| epoch  73 |   300/  475 batches | ms/batch 160.53 | loss  4.38 |
| epoch  73 |   400/  475 batches | ms/batch 158.38 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 75.80s | training loss  4.20 |
    | end of validation epoch  73 | time: 54.79s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 180.23 | loss  4.22 |
| epoch  74 |   200/  475 batches | ms/batch 165.75 | loss  4.37 |
| epoch  74 |   300/  475 batches | ms/batch 159.65 | loss  4.34 |
| epoch  74 |   400/  475 batches | ms/batch 157.71 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 75.08s | training loss  4.18 |
    | end of validation epoch  74 | time: 54.85s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 169.25 | loss  4.10 |
| epoch  75 |   200/  475 batches | ms/batch 167.16 | loss  4.01 |
| epoch  75 |   300/  475 batches | ms/batch 161.76 | loss  4.06 |
| epoch  75 |   400/  475 batches | ms/batch 159.35 | loss  4.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 75.87s | training loss  4.18 |
    | end of validation epoch  75 | time: 56.04s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 178.88 | loss  4.31 |
| epoch  76 |   200/  475 batches | ms/batch 163.69 | loss  3.78 |
| epoch  76 |   300/  475 batches | ms/batch 160.06 | loss  3.80 |
| epoch  76 |   400/  475 batches | ms/batch 158.13 | loss  4.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 75.82s | training loss  4.19 |
    | end of validation epoch  76 | time: 56.03s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 169.74 | loss  3.92 |
| epoch  77 |   200/  475 batches | ms/batch 164.91 | loss  3.96 |
| epoch  77 |   300/  475 batches | ms/batch 160.51 | loss  4.48 |
| epoch  77 |   400/  475 batches | ms/batch 158.77 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 75.53s | training loss  4.20 |
    | end of validation epoch  77 | time: 54.51s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 183.98 | loss  4.21 |
| epoch  78 |   200/  475 batches | ms/batch 168.22 | loss  3.85 |
| epoch  78 |   300/  475 batches | ms/batch 162.52 | loss  4.55 |
| epoch  78 |   400/  475 batches | ms/batch 160.23 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 76.46s | training loss  4.17 |
    | end of validation epoch  78 | time: 55.45s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 177.32 | loss  4.17 |
| epoch  79 |   200/  475 batches | ms/batch 163.04 | loss  3.95 |
| epoch  79 |   300/  475 batches | ms/batch 161.45 | loss  4.40 |
| epoch  79 |   400/  475 batches | ms/batch 160.86 | loss  3.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 78.20s | training loss  4.17 |
    | end of validation epoch  79 | time: 53.19s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 184.87 | loss  4.19 |
| epoch  80 |   200/  475 batches | ms/batch 167.19 | loss  4.10 |
| epoch  80 |   300/  475 batches | ms/batch 162.65 | loss  4.00 |
| epoch  80 |   400/  475 batches | ms/batch 161.59 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 76.53s | training loss  4.19 |
    | end of validation epoch  80 | time: 56.83s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 179.37 | loss  4.35 |
| epoch  81 |   200/  475 batches | ms/batch 165.02 | loss  4.09 |
| epoch  81 |   300/  475 batches | ms/batch 162.04 | loss  4.03 |
| epoch  81 |   400/  475 batches | ms/batch 159.10 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 75.83s | training loss  4.19 |
    | end of validation epoch  81 | time: 54.91s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 166.86 | loss  4.24 |
| epoch  82 |   200/  475 batches | ms/batch 154.86 | loss  4.61 |
| epoch  82 |   300/  475 batches | ms/batch 159.93 | loss  4.33 |
| epoch  82 |   400/  475 batches | ms/batch 160.21 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 75.53s | training loss  4.19 |
    | end of validation epoch  82 | time: 55.18s | validation loss  3.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 176.21 | loss  3.85 |
| epoch  83 |   200/  475 batches | ms/batch 164.50 | loss  3.87 |
| epoch  83 |   300/  475 batches | ms/batch 161.10 | loss  4.05 |
| epoch  83 |   400/  475 batches | ms/batch 158.57 | loss  4.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 75.54s | training loss  4.19 |
    | end of validation epoch  83 | time: 55.66s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 180.37 | loss  4.18 |
| epoch  84 |   200/  475 batches | ms/batch 165.21 | loss  4.05 |
| epoch  84 |   300/  475 batches | ms/batch 160.24 | loss  4.16 |
| epoch  84 |   400/  475 batches | ms/batch 158.81 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 76.17s | training loss  4.18 |
    | end of validation epoch  84 | time: 54.86s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 181.44 | loss  4.13 |
| epoch  85 |   200/  475 batches | ms/batch 165.52 | loss  4.45 |
| epoch  85 |   300/  475 batches | ms/batch 158.09 | loss  4.15 |
| epoch  85 |   400/  475 batches | ms/batch 155.34 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 75.01s | training loss  4.18 |
    | end of validation epoch  85 | time: 55.64s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 180.65 | loss  4.05 |
| epoch  86 |   200/  475 batches | ms/batch 166.51 | loss  3.90 |
| epoch  86 |   300/  475 batches | ms/batch 162.47 | loss  3.88 |
| epoch  86 |   400/  475 batches | ms/batch 160.23 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 76.28s | training loss  4.18 |
    | end of validation epoch  86 | time: 55.33s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 171.89 | loss  4.38 |
| epoch  87 |   200/  475 batches | ms/batch 165.82 | loss  4.18 |
| epoch  87 |   300/  475 batches | ms/batch 157.74 | loss  4.33 |
| epoch  87 |   400/  475 batches | ms/batch 157.90 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 75.03s | training loss  4.19 |
    | end of validation epoch  87 | time: 56.00s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 170.87 | loss  4.38 |
| epoch  88 |   200/  475 batches | ms/batch 164.91 | loss  4.70 |
| epoch  88 |   300/  475 batches | ms/batch 163.25 | loss  4.29 |
| epoch  88 |   400/  475 batches | ms/batch 159.72 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 75.99s | training loss  4.18 |
    | end of validation epoch  88 | time: 52.51s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 178.39 | loss  4.43 |
| epoch  89 |   200/  475 batches | ms/batch 164.53 | loss  4.14 |
| epoch  89 |   300/  475 batches | ms/batch 159.80 | loss  4.39 |
| epoch  89 |   400/  475 batches | ms/batch 158.65 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 75.60s | training loss  4.18 |
    | end of validation epoch  89 | time: 54.07s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 168.67 | loss  4.13 |
| epoch  90 |   200/  475 batches | ms/batch 167.59 | loss  4.59 |
| epoch  90 |   300/  475 batches | ms/batch 162.14 | loss  4.41 |
| epoch  90 |   400/  475 batches | ms/batch 159.96 | loss  3.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 76.06s | training loss  4.17 |
    | end of validation epoch  90 | time: 54.06s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 181.08 | loss  4.34 |
| epoch  91 |   200/  475 batches | ms/batch 166.91 | loss  4.04 |
| epoch  91 |   300/  475 batches | ms/batch 163.70 | loss  4.47 |
| epoch  91 |   400/  475 batches | ms/batch 162.40 | loss  4.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 76.69s | training loss  4.18 |
    | end of validation epoch  91 | time: 53.40s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 171.71 | loss  4.08 |
| epoch  92 |   200/  475 batches | ms/batch 160.61 | loss  4.11 |
| epoch  92 |   300/  475 batches | ms/batch 158.84 | loss  4.44 |
| epoch  92 |   400/  475 batches | ms/batch 157.14 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 75.19s | training loss  4.18 |
    | end of validation epoch  92 | time: 57.24s | validation loss  3.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 178.37 | loss  3.99 |
| epoch  93 |   200/  475 batches | ms/batch 162.96 | loss  4.07 |
| epoch  93 |   300/  475 batches | ms/batch 157.10 | loss  4.26 |
| epoch  93 |   400/  475 batches | ms/batch 156.74 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 74.96s | training loss  4.18 |
    | end of validation epoch  93 | time: 54.56s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 176.24 | loss  4.31 |
| epoch  94 |   200/  475 batches | ms/batch 164.83 | loss  4.33 |
| epoch  94 |   300/  475 batches | ms/batch 162.10 | loss  4.07 |
| epoch  94 |   400/  475 batches | ms/batch 160.05 | loss  3.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 75.77s | training loss  4.19 |
    | end of validation epoch  94 | time: 56.05s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 172.53 | loss  3.82 |
| epoch  95 |   200/  475 batches | ms/batch 163.34 | loss  3.94 |
| epoch  95 |   300/  475 batches | ms/batch 161.31 | loss  4.03 |
| epoch  95 |   400/  475 batches | ms/batch 159.13 | loss  4.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 75.81s | training loss  4.18 |
    | end of validation epoch  95 | time: 54.58s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713, 4.182604885101318] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141, 3.4361653247801196]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 178.83 | loss  4.51 |
| epoch  96 |   200/  475 batches | ms/batch 163.08 | loss  4.59 |
| epoch  96 |   300/  475 batches | ms/batch 162.63 | loss  4.05 |
| epoch  96 |   400/  475 batches | ms/batch 160.42 | loss  4.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 75.48s | training loss  4.19 |
    | end of validation epoch  96 | time: 54.20s | validation loss  3.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713, 4.182604885101318, 4.187384938691792] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141, 3.4361653247801196, 3.4254465403677035]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 168.29 | loss  3.98 |
| epoch  97 |   200/  475 batches | ms/batch 168.73 | loss  4.06 |
| epoch  97 |   300/  475 batches | ms/batch 163.12 | loss  4.26 |
| epoch  97 |   400/  475 batches | ms/batch 161.15 | loss  4.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 76.84s | training loss  4.18 |
    | end of validation epoch  97 | time: 54.52s | validation loss  3.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713, 4.182604885101318, 4.187384938691792, 4.175676681618942] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141, 3.4361653247801196, 3.4254465403677035, 3.4117279393332347]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 170.68 | loss  3.99 |
| epoch  98 |   200/  475 batches | ms/batch 163.91 | loss  3.99 |
| epoch  98 |   300/  475 batches | ms/batch 161.47 | loss  4.26 |
| epoch  98 |   400/  475 batches | ms/batch 160.08 | loss  4.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 75.73s | training loss  4.17 |
    | end of validation epoch  98 | time: 53.83s | validation loss  3.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713, 4.182604885101318, 4.187384938691792, 4.175676681618942, 4.172917482978419] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141, 3.4361653247801196, 3.4254465403677035, 3.4117279393332347, 3.4126096933829686]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 171.79 | loss  4.03 |
| epoch  99 |   200/  475 batches | ms/batch 167.70 | loss  3.84 |
| epoch  99 |   300/  475 batches | ms/batch 160.96 | loss  4.13 |
| epoch  99 |   400/  475 batches | ms/batch 159.03 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 75.53s | training loss  4.17 |
    | end of validation epoch  99 | time: 54.38s | validation loss  3.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.758421798505281, 6.261438448052657, 5.919581813812256, 5.682910651156777, 5.475685277236136, 5.308390257985969, 5.179303407167134, 5.077349161850779, 4.993316066139623, 4.887346212487472, 4.8174902474252805, 4.758699710243627, 4.720656837664152, 4.659417370244077, 4.612684843665675, 4.5745538239730035, 4.5382578227394506, 4.520671350579513, 4.481773429669832, 4.459684114456177, 4.451821026049163, 4.422608143153943, 4.396709964149877, 4.37669845229701, 4.377417345548931, 4.357296612890143, 4.329738615939491, 4.313105692612497, 4.301124335841129, 4.306397031482898, 4.2920996555529145, 4.273025580958317, 4.269742540058337, 4.272714197761134, 4.27060211231834, 4.267319593931499, 4.249058165801199, 4.244850581319708, 4.242353894083124, 4.230044966245952, 4.230575227737427, 4.217299602910092, 4.232148379777607, 4.228203564694053, 4.222402094288876, 4.22007147989775, 4.199568760018599, 4.213380925529882, 4.21091086086474, 4.205996191125167, 4.212087908795006, 4.203953901592054, 4.206204853057861, 4.195751322696084, 4.198973955857126, 4.201223227852269, 4.201084428586458, 4.1849612933711, 4.19593268494857, 4.20639776179665, 4.191202338369269, 4.1780675571843195, 4.18125561663979, 4.179327505513242, 4.189286170256765, 4.179586595736052, 4.1786025222979095, 4.192242325230649, 4.193575761192723, 4.183927880337364, 4.187110350759406, 4.1769201715368975, 4.197917411201878, 4.184129821877731, 4.1807608318328855, 4.185093105717709, 4.203511472501253, 4.172596267399035, 4.174088477586445, 4.192593264328806, 4.189752961208946, 4.18999513927259, 4.188833686929, 4.175766533299496, 4.1816472184030635, 4.184872168490761, 4.193221005389565, 4.17689962387085, 4.178219982448377, 4.1719073827643145, 4.177843277077926, 4.1766026627390005, 4.181375451840853, 4.188455265446713, 4.182604885101318, 4.187384938691792, 4.175676681618942, 4.172917482978419, 4.174291151950234] validation loss is  [5.460700840509238, 5.098739219312908, 4.848427660324994, 4.63375295510813, 4.4746708829863735, 4.363112609927394, 4.247841494424002, 4.165284872055054, 4.073249249899087, 4.012056104275358, 3.9725441572045077, 3.9389527405009552, 3.8878683142301416, 3.8649890683278314, 3.810871390735402, 3.7962219474696313, 3.771921412283633, 3.724201012058418, 3.70767439713999, 3.6854045230801367, 3.684007294037763, 3.6590742223403034, 3.65127967185333, 3.626264001141075, 3.62266663142613, 3.5931716466150365, 3.598738181490858, 3.5816430444476985, 3.5798714180954363, 3.5731735890652954, 3.5411713043180835, 3.5313989054255126, 3.5385814554551067, 3.5243765746845916, 3.541799743636316, 3.528752998143685, 3.5219552176339284, 3.5250234403530087, 3.530149415761483, 3.5156581842598795, 3.501566468166704, 3.4846643139334286, 3.5102222346458114, 3.5018101179299235, 3.4830849771740056, 3.477560570260056, 3.4805393198958967, 3.469586188051881, 3.453035643120774, 3.46939700591464, 3.4723365968015014, 3.453459928015701, 3.478413924449632, 3.456300491044501, 3.4589704425395036, 3.4485589896931366, 3.44757396233182, 3.4759989065282486, 3.430005556395074, 3.459934515111587, 3.448625947247032, 3.4412520252356007, 3.445824661174742, 3.438615550514029, 3.439380773977071, 3.4390653522074723, 3.4469908545998966, 3.4528974444926286, 3.4385983543235716, 3.4276022209840664, 3.4471396338038085, 3.4352826851756633, 3.4720987792776414, 3.446691749476585, 3.4266918827505672, 3.455346544249719, 3.416320740675726, 3.4384074992492417, 3.428086679522731, 3.4418155165279614, 3.4226096117195963, 3.457490696626551, 3.4170752012429118, 3.4377171873044565, 3.424701247896467, 3.452721000719471, 3.431581421058719, 3.4292973430216813, 3.4315082325654873, 3.4210067276193312, 3.418675582949855, 3.4207913835509483, 3.4323498761954427, 3.431776988406141, 3.4361653247801196, 3.4254465403677035, 3.4117279393332347, 3.4126096933829686, 3.411874881311625]
