/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_soumya_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 150.77 | loss  5.77 |
| epoch   1 |   200/  475 batches | ms/batch 139.57 | loss  5.63 |
| epoch   1 |   300/  475 batches | ms/batch 134.91 | loss  5.49 |
| epoch   1 |   400/  475 batches | ms/batch 132.75 | loss  5.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 63.25s | training loss  5.74 |
    | end of validation epoch   1 | time: 53.09s | validation loss  5.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.741060898429469] validation loss is  [5.0762160164969305]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 145.22 | loss  5.16 |
| epoch   2 |   200/  475 batches | ms/batch 134.28 | loss  5.06 |
| epoch   2 |   300/  475 batches | ms/batch 129.79 | loss  5.12 |
| epoch   2 |   400/  475 batches | ms/batch 127.81 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 60.58s | training loss  5.16 |
    | end of validation epoch   2 | time: 52.38s | validation loss  4.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.741060898429469, 5.160058138997932] validation loss is  [5.0762160164969305, 4.583180119009579]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 144.08 | loss  4.87 |
| epoch   3 |   200/  475 batches | ms/batch 132.99 | loss  4.84 |
| epoch   3 |   300/  475 batches | ms/batch 128.07 | loss  5.31 |
| epoch   3 |   400/  475 batches | ms/batch 126.33 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 59.74s | training loss  4.84 |
    | end of validation epoch   3 | time: 48.72s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 150.98 | loss  4.97 |
| epoch   4 |   200/  475 batches | ms/batch 134.90 | loss  4.62 |
| epoch   4 |   300/  475 batches | ms/batch 129.48 | loss  4.46 |
| epoch   4 |   400/  475 batches | ms/batch 126.37 | loss  4.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 59.68s | training loss  4.63 |
    | end of validation epoch   4 | time: 49.94s | validation loss  3.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 146.09 | loss  4.55 |
| epoch   5 |   200/  475 batches | ms/batch 133.16 | loss  4.57 |
| epoch   5 |   300/  475 batches | ms/batch 129.01 | loss  4.32 |
| epoch   5 |   400/  475 batches | ms/batch 126.88 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 60.23s | training loss  4.48 |
    | end of validation epoch   5 | time: 51.71s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 145.33 | loss  4.27 |
| epoch   6 |   200/  475 batches | ms/batch 133.84 | loss  4.44 |
| epoch   6 |   300/  475 batches | ms/batch 129.07 | loss  4.31 |
| epoch   6 |   400/  475 batches | ms/batch 126.58 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 60.15s | training loss  4.37 |
    | end of validation epoch   6 | time: 51.05s | validation loss  3.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 144.83 | loss  4.55 |
| epoch   7 |   200/  475 batches | ms/batch 134.43 | loss  4.57 |
| epoch   7 |   300/  475 batches | ms/batch 128.72 | loss  4.75 |
| epoch   7 |   400/  475 batches | ms/batch 127.47 | loss  4.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 60.58s | training loss  4.27 |
    | end of validation epoch   7 | time: 50.36s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 147.82 | loss  4.21 |
| epoch   8 |   200/  475 batches | ms/batch 134.51 | loss  4.35 |
| epoch   8 |   300/  475 batches | ms/batch 129.99 | loss  4.43 |
| epoch   8 |   400/  475 batches | ms/batch 128.09 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 60.83s | training loss  4.19 |
    | end of validation epoch   8 | time: 51.50s | validation loss  3.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 149.56 | loss  4.13 |
| epoch   9 |   200/  475 batches | ms/batch 135.47 | loss  3.85 |
| epoch   9 |   300/  475 batches | ms/batch 130.44 | loss  3.92 |
| epoch   9 |   400/  475 batches | ms/batch 128.43 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 60.92s | training loss  4.11 |
    | end of validation epoch   9 | time: 53.04s | validation loss  3.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 149.87 | loss  3.60 |
| epoch  10 |   200/  475 batches | ms/batch 136.80 | loss  4.14 |
| epoch  10 |   300/  475 batches | ms/batch 131.62 | loss  4.12 |
| epoch  10 |   400/  475 batches | ms/batch 128.75 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 60.79s | training loss  4.06 |
    | end of validation epoch  10 | time: 51.07s | validation loss  3.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 148.50 | loss  3.58 |
| epoch  11 |   200/  475 batches | ms/batch 136.04 | loss  3.87 |
| epoch  11 |   300/  475 batches | ms/batch 130.96 | loss  3.95 |
| epoch  11 |   400/  475 batches | ms/batch 128.64 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 61.24s | training loss  4.01 |
    | end of validation epoch  11 | time: 49.77s | validation loss  3.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 146.99 | loss  3.95 |
| epoch  12 |   200/  475 batches | ms/batch 134.40 | loss  3.91 |
| epoch  12 |   300/  475 batches | ms/batch 130.00 | loss  3.95 |
| epoch  12 |   400/  475 batches | ms/batch 128.43 | loss  4.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 60.69s | training loss  3.95 |
    | end of validation epoch  12 | time: 51.99s | validation loss  3.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 147.61 | loss  3.67 |
| epoch  13 |   200/  475 batches | ms/batch 133.77 | loss  3.46 |
| epoch  13 |   300/  475 batches | ms/batch 129.65 | loss  4.20 |
| epoch  13 |   400/  475 batches | ms/batch 127.09 | loss  4.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 60.23s | training loss  3.91 |
    | end of validation epoch  13 | time: 52.31s | validation loss  3.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 143.71 | loss  4.53 |
| epoch  14 |   200/  475 batches | ms/batch 131.90 | loss  3.77 |
| epoch  14 |   300/  475 batches | ms/batch 128.97 | loss  3.50 |
| epoch  14 |   400/  475 batches | ms/batch 127.33 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 60.37s | training loss  3.86 |
    | end of validation epoch  14 | time: 51.86s | validation loss  3.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 148.69 | loss  3.60 |
| epoch  15 |   200/  475 batches | ms/batch 134.93 | loss  3.70 |
| epoch  15 |   300/  475 batches | ms/batch 130.69 | loss  3.95 |
| epoch  15 |   400/  475 batches | ms/batch 128.26 | loss  3.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 60.71s | training loss  3.84 |
    | end of validation epoch  15 | time: 51.35s | validation loss  3.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 147.43 | loss  3.74 |
| epoch  16 |   200/  475 batches | ms/batch 135.49 | loss  3.89 |
| epoch  16 |   300/  475 batches | ms/batch 131.71 | loss  3.98 |
| epoch  16 |   400/  475 batches | ms/batch 128.98 | loss  4.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 61.19s | training loss  3.79 |
    | end of validation epoch  16 | time: 50.29s | validation loss  3.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 147.73 | loss  3.39 |
| epoch  17 |   200/  475 batches | ms/batch 135.90 | loss  3.52 |
| epoch  17 |   300/  475 batches | ms/batch 132.18 | loss  3.76 |
| epoch  17 |   400/  475 batches | ms/batch 129.99 | loss  3.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 61.90s | training loss  3.77 |
    | end of validation epoch  17 | time: 50.65s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 151.45 | loss  3.32 |
| epoch  18 |   200/  475 batches | ms/batch 136.17 | loss  4.04 |
| epoch  18 |   300/  475 batches | ms/batch 131.56 | loss  3.76 |
| epoch  18 |   400/  475 batches | ms/batch 129.34 | loss  3.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 61.24s | training loss  3.74 |
    | end of validation epoch  18 | time: 51.59s | validation loss  3.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 150.50 | loss  3.62 |
| epoch  19 |   200/  475 batches | ms/batch 136.90 | loss  3.44 |
| epoch  19 |   300/  475 batches | ms/batch 131.61 | loss  3.70 |
| epoch  19 |   400/  475 batches | ms/batch 128.81 | loss  4.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 61.17s | training loss  3.71 |
    | end of validation epoch  19 | time: 51.50s | validation loss  3.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 149.18 | loss  3.84 |
| epoch  20 |   200/  475 batches | ms/batch 136.10 | loss  3.52 |
| epoch  20 |   300/  475 batches | ms/batch 131.69 | loss  3.39 |
| epoch  20 |   400/  475 batches | ms/batch 129.31 | loss  3.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 61.33s | training loss  3.68 |
    | end of validation epoch  20 | time: 51.28s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 151.49 | loss  3.84 |
| epoch  21 |   200/  475 batches | ms/batch 135.72 | loss  3.16 |
| epoch  21 |   300/  475 batches | ms/batch 130.98 | loss  3.30 |
| epoch  21 |   400/  475 batches | ms/batch 129.36 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 61.26s | training loss  3.65 |
    | end of validation epoch  21 | time: 51.11s | validation loss  2.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 151.53 | loss  3.77 |
| epoch  22 |   200/  475 batches | ms/batch 138.43 | loss  3.44 |
| epoch  22 |   300/  475 batches | ms/batch 133.98 | loss  3.44 |
| epoch  22 |   400/  475 batches | ms/batch 131.04 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 62.05s | training loss  3.64 |
    | end of validation epoch  22 | time: 50.54s | validation loss  2.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 150.90 | loss  3.31 |
| epoch  23 |   200/  475 batches | ms/batch 137.29 | loss  3.56 |
| epoch  23 |   300/  475 batches | ms/batch 132.09 | loss  3.27 |
| epoch  23 |   400/  475 batches | ms/batch 130.45 | loss  3.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 62.20s | training loss  3.62 |
    | end of validation epoch  23 | time: 51.34s | validation loss  3.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 147.89 | loss  3.59 |
| epoch  24 |   200/  475 batches | ms/batch 134.71 | loss  3.02 |
| epoch  24 |   300/  475 batches | ms/batch 130.50 | loss  3.63 |
| epoch  24 |   400/  475 batches | ms/batch 129.10 | loss  3.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 61.23s | training loss  3.58 |
    | end of validation epoch  24 | time: 51.27s | validation loss  2.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 148.26 | loss  3.87 |
| epoch  25 |   200/  475 batches | ms/batch 135.64 | loss  3.49 |
| epoch  25 |   300/  475 batches | ms/batch 131.31 | loss  3.27 |
| epoch  25 |   400/  475 batches | ms/batch 129.08 | loss  3.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 61.52s | training loss  3.58 |
    | end of validation epoch  25 | time: 50.29s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 148.73 | loss  3.79 |
| epoch  26 |   200/  475 batches | ms/batch 136.27 | loss  3.46 |
| epoch  26 |   300/  475 batches | ms/batch 132.86 | loss  3.50 |
| epoch  26 |   400/  475 batches | ms/batch 129.71 | loss  3.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 61.57s | training loss  3.55 |
    | end of validation epoch  26 | time: 51.36s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 142.48 | loss  4.28 |
| epoch  27 |   200/  475 batches | ms/batch 135.00 | loss  3.37 |
| epoch  27 |   300/  475 batches | ms/batch 132.07 | loss  3.79 |
| epoch  27 |   400/  475 batches | ms/batch 129.15 | loss  3.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 61.25s | training loss  3.55 |
    | end of validation epoch  27 | time: 51.00s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 148.67 | loss  3.17 |
| epoch  28 |   200/  475 batches | ms/batch 136.60 | loss  3.44 |
| epoch  28 |   300/  475 batches | ms/batch 132.22 | loss  3.41 |
| epoch  28 |   400/  475 batches | ms/batch 129.92 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 62.06s | training loss  3.51 |
    | end of validation epoch  28 | time: 52.31s | validation loss  2.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 151.62 | loss  3.36 |
| epoch  29 |   200/  475 batches | ms/batch 137.81 | loss  3.34 |
| epoch  29 |   300/  475 batches | ms/batch 133.43 | loss  3.39 |
| epoch  29 |   400/  475 batches | ms/batch 130.98 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 62.30s | training loss  3.50 |
    | end of validation epoch  29 | time: 51.31s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 150.79 | loss  3.62 |
| epoch  30 |   200/  475 batches | ms/batch 138.13 | loss  3.54 |
| epoch  30 |   300/  475 batches | ms/batch 133.82 | loss  3.59 |
| epoch  30 |   400/  475 batches | ms/batch 132.44 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 62.58s | training loss  3.47 |
    | end of validation epoch  30 | time: 49.63s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 149.23 | loss  3.21 |
| epoch  31 |   200/  475 batches | ms/batch 135.60 | loss  3.34 |
| epoch  31 |   300/  475 batches | ms/batch 132.75 | loss  3.14 |
| epoch  31 |   400/  475 batches | ms/batch 130.95 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 62.35s | training loss  3.46 |
    | end of validation epoch  31 | time: 52.34s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 143.39 | loss  3.49 |
| epoch  32 |   200/  475 batches | ms/batch 134.17 | loss  3.37 |
| epoch  32 |   300/  475 batches | ms/batch 132.78 | loss  3.04 |
| epoch  32 |   400/  475 batches | ms/batch 131.34 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 62.38s | training loss  3.44 |
    | end of validation epoch  32 | time: 52.24s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 152.89 | loss  3.37 |
| epoch  33 |   200/  475 batches | ms/batch 138.56 | loss  3.18 |
| epoch  33 |   300/  475 batches | ms/batch 134.26 | loss  3.46 |
| epoch  33 |   400/  475 batches | ms/batch 133.02 | loss  3.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 63.14s | training loss  3.42 |
    | end of validation epoch  33 | time: 50.93s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 151.45 | loss  3.23 |
| epoch  34 |   200/  475 batches | ms/batch 138.71 | loss  3.46 |
| epoch  34 |   300/  475 batches | ms/batch 133.97 | loss  3.09 |
| epoch  34 |   400/  475 batches | ms/batch 132.37 | loss  3.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 62.69s | training loss  3.42 |
    | end of validation epoch  34 | time: 51.14s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 153.74 | loss  3.23 |
| epoch  35 |   200/  475 batches | ms/batch 140.34 | loss  3.29 |
| epoch  35 |   300/  475 batches | ms/batch 135.00 | loss  3.10 |
| epoch  35 |   400/  475 batches | ms/batch 133.16 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 63.03s | training loss  3.40 |
    | end of validation epoch  35 | time: 51.87s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 151.21 | loss  3.33 |
| epoch  36 |   200/  475 batches | ms/batch 137.93 | loss  3.09 |
| epoch  36 |   300/  475 batches | ms/batch 135.13 | loss  3.42 |
| epoch  36 |   400/  475 batches | ms/batch 132.79 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 63.14s | training loss  3.40 |
    | end of validation epoch  36 | time: 53.04s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 145.54 | loss  3.33 |
| epoch  37 |   200/  475 batches | ms/batch 138.66 | loss  3.49 |
| epoch  37 |   300/  475 batches | ms/batch 134.60 | loss  3.35 |
| epoch  37 |   400/  475 batches | ms/batch 131.67 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 62.60s | training loss  3.39 |
    | end of validation epoch  37 | time: 52.64s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 152.68 | loss  2.91 |
| epoch  38 |   200/  475 batches | ms/batch 140.94 | loss  3.57 |
| epoch  38 |   300/  475 batches | ms/batch 134.86 | loss  3.56 |
| epoch  38 |   400/  475 batches | ms/batch 132.74 | loss  2.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 63.25s | training loss  3.37 |
    | end of validation epoch  38 | time: 52.64s | validation loss  2.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 151.26 | loss  3.11 |
| epoch  39 |   200/  475 batches | ms/batch 137.89 | loss  3.60 |
| epoch  39 |   300/  475 batches | ms/batch 134.20 | loss  3.08 |
| epoch  39 |   400/  475 batches | ms/batch 132.61 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 63.00s | training loss  3.35 |
    | end of validation epoch  39 | time: 54.25s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 158.64 | loss  3.56 |
| epoch  40 |   200/  475 batches | ms/batch 143.54 | loss  3.48 |
| epoch  40 |   300/  475 batches | ms/batch 140.34 | loss  3.24 |
| epoch  40 |   400/  475 batches | ms/batch 137.25 | loss  3.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 64.74s | training loss  3.35 |
    | end of validation epoch  40 | time: 53.03s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 153.04 | loss  3.11 |
| epoch  41 |   200/  475 batches | ms/batch 137.91 | loss  3.57 |
| epoch  41 |   300/  475 batches | ms/batch 134.47 | loss  3.19 |
| epoch  41 |   400/  475 batches | ms/batch 132.99 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 63.14s | training loss  3.34 |
    | end of validation epoch  41 | time: 52.76s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 152.90 | loss  3.09 |
| epoch  42 |   200/  475 batches | ms/batch 139.90 | loss  2.75 |
| epoch  42 |   300/  475 batches | ms/batch 135.35 | loss  3.43 |
| epoch  42 |   400/  475 batches | ms/batch 133.52 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 63.64s | training loss  3.31 |
    | end of validation epoch  42 | time: 51.22s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 146.96 | loss  3.30 |
| epoch  43 |   200/  475 batches | ms/batch 137.23 | loss  3.38 |
| epoch  43 |   300/  475 batches | ms/batch 133.78 | loss  3.16 |
| epoch  43 |   400/  475 batches | ms/batch 131.52 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 62.45s | training loss  3.32 |
    | end of validation epoch  43 | time: 52.50s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424]
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 148.43 | loss  3.15 |
| epoch  44 |   200/  475 batches | ms/batch 137.76 | loss  3.18 |
| epoch  44 |   300/  475 batches | ms/batch 133.96 | loss  3.31 |
| epoch  44 |   400/  475 batches | ms/batch 132.14 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 63.01s | training loss  3.30 |
    | end of validation epoch  44 | time: 50.91s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 149.64 | loss  3.01 |
| epoch  45 |   200/  475 batches | ms/batch 137.92 | loss  3.39 |
| epoch  45 |   300/  475 batches | ms/batch 134.89 | loss  3.10 |
| epoch  45 |   400/  475 batches | ms/batch 132.95 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 63.02s | training loss  3.30 |
    | end of validation epoch  45 | time: 49.92s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 153.19 | loss  3.24 |
| epoch  46 |   200/  475 batches | ms/batch 138.71 | loss  3.44 |
| epoch  46 |   300/  475 batches | ms/batch 135.05 | loss  3.33 |
| epoch  46 |   400/  475 batches | ms/batch 132.51 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 62.95s | training loss  3.29 |
    | end of validation epoch  46 | time: 50.83s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 155.58 | loss  3.08 |
| epoch  47 |   200/  475 batches | ms/batch 140.95 | loss  3.17 |
| epoch  47 |   300/  475 batches | ms/batch 135.84 | loss  3.25 |
| epoch  47 |   400/  475 batches | ms/batch 132.67 | loss  3.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 62.81s | training loss  3.26 |
    | end of validation epoch  47 | time: 52.23s | validation loss  2.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 151.82 | loss  3.51 |
| epoch  48 |   200/  475 batches | ms/batch 137.17 | loss  3.25 |
| epoch  48 |   300/  475 batches | ms/batch 133.83 | loss  3.21 |
| epoch  48 |   400/  475 batches | ms/batch 132.21 | loss  3.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 62.62s | training loss  3.28 |
    | end of validation epoch  48 | time: 51.47s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 154.49 | loss  3.28 |
| epoch  49 |   200/  475 batches | ms/batch 138.06 | loss  2.95 |
| epoch  49 |   300/  475 batches | ms/batch 135.97 | loss  3.53 |
| epoch  49 |   400/  475 batches | ms/batch 133.82 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 63.22s | training loss  3.26 |
    | end of validation epoch  49 | time: 51.81s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 146.72 | loss  3.37 |
| epoch  50 |   200/  475 batches | ms/batch 138.96 | loss  3.13 |
| epoch  50 |   300/  475 batches | ms/batch 133.81 | loss  2.90 |
| epoch  50 |   400/  475 batches | ms/batch 131.92 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 62.30s | training loss  3.24 |
    | end of validation epoch  50 | time: 51.40s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 154.15 | loss  3.28 |
| epoch  51 |   200/  475 batches | ms/batch 139.90 | loss  3.35 |
| epoch  51 |   300/  475 batches | ms/batch 133.28 | loss  3.14 |
| epoch  51 |   400/  475 batches | ms/batch 131.15 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 61.99s | training loss  3.24 |
    | end of validation epoch  51 | time: 52.69s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 151.54 | loss  3.01 |
| epoch  52 |   200/  475 batches | ms/batch 139.90 | loss  3.42 |
| epoch  52 |   300/  475 batches | ms/batch 134.98 | loss  3.11 |
| epoch  52 |   400/  475 batches | ms/batch 131.80 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 62.27s | training loss  3.23 |
    | end of validation epoch  52 | time: 51.09s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 151.91 | loss  3.02 |
| epoch  53 |   200/  475 batches | ms/batch 139.16 | loss  3.21 |
| epoch  53 |   300/  475 batches | ms/batch 134.75 | loss  3.25 |
| epoch  53 |   400/  475 batches | ms/batch 131.92 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 62.66s | training loss  3.22 |
    | end of validation epoch  53 | time: 51.62s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 154.00 | loss  3.85 |
| epoch  54 |   200/  475 batches | ms/batch 139.15 | loss  3.63 |
| epoch  54 |   300/  475 batches | ms/batch 134.44 | loss  2.91 |
| epoch  54 |   400/  475 batches | ms/batch 131.92 | loss  3.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 62.60s | training loss  3.21 |
    | end of validation epoch  54 | time: 53.43s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 150.48 | loss  3.10 |
| epoch  55 |   200/  475 batches | ms/batch 136.70 | loss  2.91 |
| epoch  55 |   300/  475 batches | ms/batch 132.99 | loss  3.22 |
| epoch  55 |   400/  475 batches | ms/batch 130.29 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 61.96s | training loss  3.20 |
    | end of validation epoch  55 | time: 51.97s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 151.04 | loss  3.16 |
| epoch  56 |   200/  475 batches | ms/batch 136.30 | loss  3.86 |
| epoch  56 |   300/  475 batches | ms/batch 133.50 | loss  3.19 |
| epoch  56 |   400/  475 batches | ms/batch 131.17 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 62.35s | training loss  3.21 |
    | end of validation epoch  56 | time: 50.93s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016]
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 150.07 | loss  3.01 |
| epoch  57 |   200/  475 batches | ms/batch 137.55 | loss  2.95 |
| epoch  57 |   300/  475 batches | ms/batch 135.77 | loss  3.42 |
| epoch  57 |   400/  475 batches | ms/batch 132.74 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 63.23s | training loss  3.20 |
    | end of validation epoch  57 | time: 50.98s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 154.42 | loss  3.11 |
| epoch  58 |   200/  475 batches | ms/batch 140.49 | loss  3.11 |
| epoch  58 |   300/  475 batches | ms/batch 134.59 | loss  3.05 |
| epoch  58 |   400/  475 batches | ms/batch 132.45 | loss  3.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 62.79s | training loss  3.17 |
    | end of validation epoch  58 | time: 51.66s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 155.90 | loss  3.16 |
| epoch  59 |   200/  475 batches | ms/batch 138.70 | loss  3.34 |
| epoch  59 |   300/  475 batches | ms/batch 133.70 | loss  3.08 |
| epoch  59 |   400/  475 batches | ms/batch 131.64 | loss  2.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 62.39s | training loss  3.16 |
    | end of validation epoch  59 | time: 51.30s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 148.44 | loss  3.67 |
| epoch  60 |   200/  475 batches | ms/batch 138.96 | loss  3.22 |
| epoch  60 |   300/  475 batches | ms/batch 133.58 | loss  3.52 |
| epoch  60 |   400/  475 batches | ms/batch 131.52 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 62.42s | training loss  3.17 |
    | end of validation epoch  60 | time: 50.86s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 151.00 | loss  3.07 |
| epoch  61 |   200/  475 batches | ms/batch 137.33 | loss  3.16 |
| epoch  61 |   300/  475 batches | ms/batch 134.02 | loss  3.60 |
| epoch  61 |   400/  475 batches | ms/batch 130.97 | loss  3.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 62.12s | training loss  3.16 |
    | end of validation epoch  61 | time: 52.81s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 153.01 | loss  3.05 |
| epoch  62 |   200/  475 batches | ms/batch 138.27 | loss  3.17 |
| epoch  62 |   300/  475 batches | ms/batch 133.97 | loss  2.81 |
| epoch  62 |   400/  475 batches | ms/batch 132.34 | loss  3.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 62.69s | training loss  3.15 |
    | end of validation epoch  62 | time: 53.38s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 150.59 | loss  2.82 |
| epoch  63 |   200/  475 batches | ms/batch 138.51 | loss  3.23 |
| epoch  63 |   300/  475 batches | ms/batch 133.83 | loss  3.36 |
| epoch  63 |   400/  475 batches | ms/batch 131.77 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 62.47s | training loss  3.15 |
    | end of validation epoch  63 | time: 52.83s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 155.48 | loss  3.16 |
| epoch  64 |   200/  475 batches | ms/batch 141.43 | loss  3.12 |
| epoch  64 |   300/  475 batches | ms/batch 135.78 | loss  2.84 |
| epoch  64 |   400/  475 batches | ms/batch 132.67 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 62.73s | training loss  3.12 |
    | end of validation epoch  64 | time: 51.51s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 154.93 | loss  2.92 |
| epoch  65 |   200/  475 batches | ms/batch 140.38 | loss  2.74 |
| epoch  65 |   300/  475 batches | ms/batch 134.42 | loss  2.97 |
| epoch  65 |   400/  475 batches | ms/batch 131.65 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 62.55s | training loss  3.12 |
    | end of validation epoch  65 | time: 51.59s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 153.32 | loss  3.40 |
| epoch  66 |   200/  475 batches | ms/batch 138.14 | loss  3.48 |
| epoch  66 |   300/  475 batches | ms/batch 134.84 | loss  2.96 |
| epoch  66 |   400/  475 batches | ms/batch 133.03 | loss  2.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 63.19s | training loss  3.12 |
    | end of validation epoch  66 | time: 53.18s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 157.97 | loss  2.45 |
| epoch  67 |   200/  475 batches | ms/batch 142.50 | loss  3.26 |
| epoch  67 |   300/  475 batches | ms/batch 136.35 | loss  3.25 |
| epoch  67 |   400/  475 batches | ms/batch 132.40 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 62.55s | training loss  3.11 |
    | end of validation epoch  67 | time: 52.63s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 153.93 | loss  2.58 |
| epoch  68 |   200/  475 batches | ms/batch 139.28 | loss  2.97 |
| epoch  68 |   300/  475 batches | ms/batch 134.43 | loss  3.07 |
| epoch  68 |   400/  475 batches | ms/batch 132.39 | loss  3.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 62.99s | training loss  3.11 |
    | end of validation epoch  68 | time: 53.25s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 150.62 | loss  3.20 |
| epoch  69 |   200/  475 batches | ms/batch 138.28 | loss  3.19 |
| epoch  69 |   300/  475 batches | ms/batch 134.60 | loss  3.55 |
| epoch  69 |   400/  475 batches | ms/batch 132.91 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 62.88s | training loss  3.09 |
    | end of validation epoch  69 | time: 52.51s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 152.05 | loss  2.98 |
| epoch  70 |   200/  475 batches | ms/batch 136.91 | loss  2.99 |
| epoch  70 |   300/  475 batches | ms/batch 132.45 | loss  2.82 |
| epoch  70 |   400/  475 batches | ms/batch 130.67 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 61.89s | training loss  3.08 |
    | end of validation epoch  70 | time: 53.11s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 154.89 | loss  3.14 |
| epoch  71 |   200/  475 batches | ms/batch 141.29 | loss  3.03 |
| epoch  71 |   300/  475 batches | ms/batch 135.83 | loss  3.23 |
| epoch  71 |   400/  475 batches | ms/batch 133.14 | loss  2.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 63.13s | training loss  3.08 |
    | end of validation epoch  71 | time: 50.82s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 150.21 | loss  3.15 |
| epoch  72 |   200/  475 batches | ms/batch 138.60 | loss  3.20 |
| epoch  72 |   300/  475 batches | ms/batch 133.08 | loss  2.99 |
| epoch  72 |   400/  475 batches | ms/batch 130.76 | loss  2.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 62.41s | training loss  3.10 |
    | end of validation epoch  72 | time: 52.03s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857]
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 152.29 | loss  3.06 |
| epoch  73 |   200/  475 batches | ms/batch 141.75 | loss  3.02 |
| epoch  73 |   300/  475 batches | ms/batch 135.83 | loss  2.72 |
| epoch  73 |   400/  475 batches | ms/batch 133.05 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 63.05s | training loss  3.07 |
    | end of validation epoch  73 | time: 51.62s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 153.27 | loss  3.30 |
| epoch  74 |   200/  475 batches | ms/batch 140.75 | loss  2.99 |
| epoch  74 |   300/  475 batches | ms/batch 135.85 | loss  3.11 |
| epoch  74 |   400/  475 batches | ms/batch 133.03 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 62.75s | training loss  3.08 |
    | end of validation epoch  74 | time: 53.26s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 152.88 | loss  2.82 |
| epoch  75 |   200/  475 batches | ms/batch 138.69 | loss  3.39 |
| epoch  75 |   300/  475 batches | ms/batch 133.92 | loss  2.70 |
| epoch  75 |   400/  475 batches | ms/batch 132.64 | loss  3.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 62.81s | training loss  3.06 |
    | end of validation epoch  75 | time: 52.13s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 150.88 | loss  3.16 |
| epoch  76 |   200/  475 batches | ms/batch 137.13 | loss  3.03 |
| epoch  76 |   300/  475 batches | ms/batch 133.51 | loss  3.16 |
| epoch  76 |   400/  475 batches | ms/batch 130.62 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 62.03s | training loss  3.04 |
    | end of validation epoch  76 | time: 52.02s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 147.84 | loss  3.13 |
| epoch  77 |   200/  475 batches | ms/batch 137.43 | loss  2.88 |
| epoch  77 |   300/  475 batches | ms/batch 133.76 | loss  2.72 |
| epoch  77 |   400/  475 batches | ms/batch 131.48 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 62.38s | training loss  3.04 |
    | end of validation epoch  77 | time: 51.63s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196]
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 153.80 | loss  3.64 |
| epoch  78 |   200/  475 batches | ms/batch 138.07 | loss  3.00 |
| epoch  78 |   300/  475 batches | ms/batch 133.45 | loss  2.98 |
| epoch  78 |   400/  475 batches | ms/batch 134.97 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 63.62s | training loss  3.02 |
    | end of validation epoch  78 | time: 51.46s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 149.23 | loss  3.11 |
| epoch  79 |   200/  475 batches | ms/batch 138.38 | loss  3.03 |
| epoch  79 |   300/  475 batches | ms/batch 134.37 | loss  2.62 |
| epoch  79 |   400/  475 batches | ms/batch 132.56 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 62.72s | training loss  3.02 |
    | end of validation epoch  79 | time: 51.13s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 147.53 | loss  2.92 |
| epoch  80 |   200/  475 batches | ms/batch 138.76 | loss  3.37 |
| epoch  80 |   300/  475 batches | ms/batch 134.32 | loss  3.64 |
| epoch  80 |   400/  475 batches | ms/batch 131.67 | loss  2.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 62.45s | training loss  3.03 |
    | end of validation epoch  80 | time: 51.71s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 152.98 | loss  3.22 |
| epoch  81 |   200/  475 batches | ms/batch 138.92 | loss  2.82 |
| epoch  81 |   300/  475 batches | ms/batch 135.24 | loss  3.06 |
| epoch  81 |   400/  475 batches | ms/batch 132.63 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 62.66s | training loss  3.03 |
    | end of validation epoch  81 | time: 53.06s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 150.88 | loss  3.23 |
| epoch  82 |   200/  475 batches | ms/batch 137.77 | loss  2.72 |
| epoch  82 |   300/  475 batches | ms/batch 132.85 | loss  3.33 |
| epoch  82 |   400/  475 batches | ms/batch 131.39 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 62.29s | training loss  3.02 |
    | end of validation epoch  82 | time: 53.27s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 144.02 | loss  3.04 |
| epoch  83 |   200/  475 batches | ms/batch 137.21 | loss  2.75 |
| epoch  83 |   300/  475 batches | ms/batch 133.81 | loss  2.94 |
| epoch  83 |   400/  475 batches | ms/batch 131.20 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 62.09s | training loss  3.00 |
    | end of validation epoch  83 | time: 52.73s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 151.43 | loss  2.62 |
| epoch  84 |   200/  475 batches | ms/batch 138.77 | loss  2.80 |
| epoch  84 |   300/  475 batches | ms/batch 133.72 | loss  2.95 |
| epoch  84 |   400/  475 batches | ms/batch 131.19 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 62.18s | training loss  3.00 |
    | end of validation epoch  84 | time: 51.78s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 153.26 | loss  3.18 |
| epoch  85 |   200/  475 batches | ms/batch 139.20 | loss  2.65 |
| epoch  85 |   300/  475 batches | ms/batch 135.47 | loss  2.93 |
| epoch  85 |   400/  475 batches | ms/batch 132.42 | loss  3.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 62.80s | training loss  2.98 |
    | end of validation epoch  85 | time: 53.14s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 148.56 | loss  2.86 |
| epoch  86 |   200/  475 batches | ms/batch 137.50 | loss  3.22 |
| epoch  86 |   300/  475 batches | ms/batch 134.07 | loss  3.24 |
| epoch  86 |   400/  475 batches | ms/batch 132.12 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 62.61s | training loss  2.98 |
    | end of validation epoch  86 | time: 50.81s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 148.36 | loss  2.82 |
| epoch  87 |   200/  475 batches | ms/batch 138.39 | loss  3.35 |
| epoch  87 |   300/  475 batches | ms/batch 134.04 | loss  2.77 |
| epoch  87 |   400/  475 batches | ms/batch 131.81 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 62.38s | training loss  2.98 |
    | end of validation epoch  87 | time: 50.99s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 151.20 | loss  3.12 |
| epoch  88 |   200/  475 batches | ms/batch 138.93 | loss  2.80 |
| epoch  88 |   300/  475 batches | ms/batch 133.94 | loss  3.17 |
| epoch  88 |   400/  475 batches | ms/batch 132.79 | loss  2.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 62.96s | training loss  2.98 |
    | end of validation epoch  88 | time: 51.49s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 153.83 | loss  3.60 |
| epoch  89 |   200/  475 batches | ms/batch 139.63 | loss  2.60 |
| epoch  89 |   300/  475 batches | ms/batch 135.32 | loss  2.39 |
| epoch  89 |   400/  475 batches | ms/batch 133.16 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 63.23s | training loss  2.95 |
    | end of validation epoch  89 | time: 49.45s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 155.41 | loss  2.88 |
| epoch  90 |   200/  475 batches | ms/batch 141.26 | loss  3.35 |
| epoch  90 |   300/  475 batches | ms/batch 135.83 | loss  3.01 |
| epoch  90 |   400/  475 batches | ms/batch 133.35 | loss  2.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 62.91s | training loss  2.96 |
    | end of validation epoch  90 | time: 53.06s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 153.63 | loss  2.77 |
| epoch  91 |   200/  475 batches | ms/batch 136.87 | loss  2.97 |
| epoch  91 |   300/  475 batches | ms/batch 133.37 | loss  3.00 |
| epoch  91 |   400/  475 batches | ms/batch 130.71 | loss  2.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 64.74s | training loss  2.96 |
    | end of validation epoch  91 | time: 53.22s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 155.53 | loss  2.93 |
| epoch  92 |   200/  475 batches | ms/batch 140.22 | loss  2.84 |
| epoch  92 |   300/  475 batches | ms/batch 136.11 | loss  2.73 |
| epoch  92 |   400/  475 batches | ms/batch 133.29 | loss  2.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 63.02s | training loss  2.96 |
    | end of validation epoch  92 | time: 52.20s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 153.10 | loss  3.52 |
| epoch  93 |   200/  475 batches | ms/batch 139.05 | loss  3.16 |
| epoch  93 |   300/  475 batches | ms/batch 134.07 | loss  2.63 |
| epoch  93 |   400/  475 batches | ms/batch 132.30 | loss  2.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 63.16s | training loss  2.94 |
    | end of validation epoch  93 | time: 52.04s | validation loss  2.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 153.77 | loss  3.09 |
| epoch  94 |   200/  475 batches | ms/batch 140.18 | loss  2.73 |
| epoch  94 |   300/  475 batches | ms/batch 135.80 | loss  2.96 |
| epoch  94 |   400/  475 batches | ms/batch 132.36 | loss  2.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 62.59s | training loss  2.93 |
    | end of validation epoch  94 | time: 51.21s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 147.57 | loss  3.04 |
| epoch  95 |   200/  475 batches | ms/batch 140.62 | loss  2.54 |
| epoch  95 |   300/  475 batches | ms/batch 135.91 | loss  3.01 |
| epoch  95 |   400/  475 batches | ms/batch 133.28 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 63.27s | training loss  2.93 |
    | end of validation epoch  95 | time: 51.78s | validation loss  2.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794, 2.930094922718249] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816, 2.454059893343629]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 153.15 | loss  2.88 |
| epoch  96 |   200/  475 batches | ms/batch 139.79 | loss  2.38 |
| epoch  96 |   300/  475 batches | ms/batch 135.29 | loss  3.22 |
| epoch  96 |   400/  475 batches | ms/batch 132.37 | loss  3.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 62.68s | training loss  2.91 |
    | end of validation epoch  96 | time: 50.78s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794, 2.930094922718249, 2.910011336677953] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816, 2.454059893343629, 2.500397424737946]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 151.55 | loss  2.74 |
| epoch  97 |   200/  475 batches | ms/batch 140.22 | loss  2.84 |
| epoch  97 |   300/  475 batches | ms/batch 135.15 | loss  2.23 |
| epoch  97 |   400/  475 batches | ms/batch 132.29 | loss  2.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 62.84s | training loss  2.93 |
    | end of validation epoch  97 | time: 51.28s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794, 2.930094922718249, 2.910011336677953, 2.9289957166972913] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816, 2.454059893343629, 2.500397424737946, 2.5020497286019205]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 145.65 | loss  2.67 |
| epoch  98 |   200/  475 batches | ms/batch 139.77 | loss  2.57 |
| epoch  98 |   300/  475 batches | ms/batch 135.22 | loss  2.70 |
| epoch  98 |   400/  475 batches | ms/batch 132.69 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 62.60s | training loss  2.91 |
    | end of validation epoch  98 | time: 50.58s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794, 2.930094922718249, 2.910011336677953, 2.9289957166972913, 2.912970421941657] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816, 2.454059893343629, 2.500397424737946, 2.5020497286019205, 2.4893189949147843]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 149.17 | loss  3.27 |
| epoch  99 |   200/  475 batches | ms/batch 138.50 | loss  2.73 |
| epoch  99 |   300/  475 batches | ms/batch 134.22 | loss  2.39 |
| epoch  99 |   400/  475 batches | ms/batch 131.89 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 62.53s | training loss  2.91 |
    | end of validation epoch  99 | time: 52.21s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.741060898429469, 5.160058138997932, 4.8395474975987485, 4.633897565540514, 4.479521241439016, 4.373373419611077, 4.265988618951095, 4.1898782684928495, 4.110602159500122, 4.060972598728381, 4.006032639051739, 3.9470396393223814, 3.9093940554167097, 3.8589839583949037, 3.8353077727869938, 3.790768155047768, 3.7715498306876736, 3.743978394960102, 3.705868696915476, 3.675363891501176, 3.6501878753461336, 3.6392078229000693, 3.62143685943202, 3.581322190134149, 3.5821003105765894, 3.5465197994834496, 3.5524445860009446, 3.5105938033053747, 3.500580011668958, 3.4695379126699346, 3.4637927301306473, 3.444053730713694, 3.420143932543303, 3.419877537175229, 3.40042070790341, 3.398070115541157, 3.3851299627203693, 3.371460256074604, 3.3499895331734106, 3.3460283309534975, 3.342722712065044, 3.3123825720736857, 3.317678959997077, 3.299487389012387, 3.302709781747115, 3.2927108307888635, 3.2583654192874305, 3.2787833585237203, 3.257001363854659, 3.244760368748715, 3.242680732325504, 3.2314849160846912, 3.2234515671981008, 3.20544958767138, 3.200342682286313, 3.2097677878329627, 3.197937828867059, 3.169851226304707, 3.163136368801719, 3.1721118916963276, 3.1579838833056, 3.1460492284674393, 3.14887217069927, 3.1193176761426424, 3.118746035726447, 3.1182669765070865, 3.109274434039467, 3.1091009220324066, 3.093534809915643, 3.075918876748336, 3.075521690970973, 3.0977484376806963, 3.0662521673503673, 3.076486704976935, 3.060031775926289, 3.041916850240607, 3.0442940450969496, 3.022653165114553, 3.020597774606002, 3.026530374225817, 3.0253390272040117, 3.022878092213681, 3.0016446645636305, 2.995065732755159, 2.983737735246357, 2.9824218127602027, 2.980861079065423, 2.9755520951120475, 2.9529576176091243, 2.956651971214696, 2.9630017978266667, 2.9568264198303225, 2.9367155732606585, 2.929061135743794, 2.930094922718249, 2.910011336677953, 2.9289957166972913, 2.912970421941657, 2.9079424075076457] validation loss is  [5.0762160164969305, 4.583180119009579, 4.1265418128807, 3.9373187237427016, 3.8227713067992393, 3.607999434992045, 3.478642621961962, 3.4840935298374722, 3.3523564659246876, 3.2833662453819725, 3.322302666031012, 3.258934115161415, 3.2059722828263997, 3.123020458622139, 3.1244807824367236, 3.082324240387989, 3.0220649763315666, 3.0117880797185816, 3.04602436658715, 2.9774014048215722, 2.949145429274615, 2.9318646483060693, 3.0017801633402077, 2.915432174666589, 2.892336811338152, 2.887581176116687, 2.8114254955484084, 2.8322993446798885, 2.8223211725218955, 2.8067487027464795, 2.8709936622811965, 2.7674954678831982, 2.75037846845739, 2.7442138796093083, 2.7705610399486638, 2.8191720597884236, 2.7502979911675975, 2.759397378488749, 2.740178737319818, 2.694253879434922, 2.696755234934703, 2.689900384229772, 2.690303652226424, 2.704137966412456, 2.6542792069811783, 2.645245856597644, 2.7105806284591933, 2.6392380840638103, 2.648422297309427, 2.6478642255318263, 2.6169167446489094, 2.616822383984798, 2.6002520042307236, 2.641083866608243, 2.6023847696160067, 2.6607934927740016, 2.650132477784357, 2.607997115920572, 2.5980602923561547, 2.558499202007005, 2.5461684365232453, 2.5656410265369574, 2.563032400708239, 2.5504309570088104, 2.5275017614124202, 2.55599922092021, 2.576140469863635, 2.5135225818938567, 2.564780354499817, 2.5242514019252873, 2.575264634204512, 2.517096105743857, 2.5511769396918162, 2.518773563769685, 2.538414761799724, 2.5175325600039056, 2.516136611209196, 2.534123318535941, 2.472413008954345, 2.5545407894278775, 2.5143532783043483, 2.501847759014418, 2.5085153429448104, 2.493758531177745, 2.4943798900652334, 2.4724200793675015, 2.5025023101758554, 2.5056962856725487, 2.4714626384382488, 2.46879085372476, 2.5103440765573195, 2.497576424053737, 2.4768494628056756, 2.4714754168726816, 2.454059893343629, 2.500397424737946, 2.5020497286019205, 2.4893189949147843, 2.486117329918036]
 Best training model found.
---------------------------------------------------------------------------------------------------
