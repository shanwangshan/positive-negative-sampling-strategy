/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1137.30 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 122.30s | training loss  1.70 |
    | end of validation epoch   1 | time: 86.80s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [1.6988831339655697] validation loss is  [1.0858629231030743]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1126.20 | loss  0.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 121.62s | training loss  1.17 |
    | end of validation epoch   2 | time: 86.86s | validation loss  1.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [1.6988831339655697, 1.16539022514412] validation loss is  [1.0858629231030743, 1.0319232733454555]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 1163.30 | loss  0.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 125.20s | training loss  0.97 |
    | end of validation epoch   3 | time: 79.64s | validation loss  1.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 1143.46 | loss  0.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 122.94s | training loss  0.84 |
    | end of validation epoch   4 | time: 81.76s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 1129.34 | loss  0.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 121.65s | training loss  0.76 |
    | end of validation epoch   5 | time: 84.82s | validation loss  1.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 1142.22 | loss  0.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 122.72s | training loss  0.69 |
    | end of validation epoch   6 | time: 79.31s | validation loss  1.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1198.59 | loss  0.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 128.72s | training loss  0.61 |
    | end of validation epoch   7 | time: 81.52s | validation loss  1.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1127.66 | loss  0.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 121.52s | training loss  0.57 |
    | end of validation epoch   8 | time: 84.90s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1139.97 | loss  0.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 122.72s | training loss  0.53 |
    | end of validation epoch   9 | time: 79.28s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1142.37 | loss  0.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 123.10s | training loss  0.48 |
    | end of validation epoch  10 | time: 81.51s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1132.88 | loss  0.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 121.81s | training loss  0.45 |
    | end of validation epoch  11 | time: 84.70s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1138.20 | loss  0.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 122.35s | training loss  0.42 |
    | end of validation epoch  12 | time: 79.55s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1136.47 | loss  0.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 123.47s | training loss  0.38 |
    | end of validation epoch  13 | time: 82.16s | validation loss  1.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1144.31 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 123.56s | training loss  0.37 |
    | end of validation epoch  14 | time: 85.11s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1141.63 | loss  0.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 123.30s | training loss  0.35 |
    | end of validation epoch  15 | time: 79.90s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1141.47 | loss  0.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 122.83s | training loss  0.35 |
    | end of validation epoch  16 | time: 81.89s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1151.42 | loss  0.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 124.91s | training loss  0.31 |
    | end of validation epoch  17 | time: 84.98s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1183.78 | loss  0.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 128.66s | training loss  0.31 |
    | end of validation epoch  18 | time: 79.55s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557]
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1167.75 | loss  0.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 126.06s | training loss  0.29 |
    | end of validation epoch  19 | time: 82.62s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 1144.39 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 127.14s | training loss  0.28 |
    | end of validation epoch  20 | time: 85.44s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 1144.25 | loss  0.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 123.30s | training loss  0.26 |
    | end of validation epoch  21 | time: 79.23s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1143.13 | loss  0.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 123.24s | training loss  0.24 |
    | end of validation epoch  22 | time: 81.74s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1144.98 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 123.32s | training loss  0.23 |
    | end of validation epoch  23 | time: 85.03s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1145.83 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 123.43s | training loss  0.22 |
    | end of validation epoch  24 | time: 79.85s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1145.92 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 123.01s | training loss  0.22 |
    | end of validation epoch  25 | time: 81.67s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328]
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1138.44 | loss  0.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 122.70s | training loss  0.21 |
    | end of validation epoch  26 | time: 85.16s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1144.70 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 123.36s | training loss  0.21 |
    | end of validation epoch  27 | time: 79.56s | validation loss  1.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1143.34 | loss  0.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 123.38s | training loss  0.19 |
    | end of validation epoch  28 | time: 82.20s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1132.97 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 121.74s | training loss  0.19 |
    | end of validation epoch  29 | time: 84.72s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018]
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1139.26 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 122.70s | training loss  0.18 |
    | end of validation epoch  30 | time: 79.57s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1142.42 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 122.95s | training loss  0.17 |
    | end of validation epoch  31 | time: 81.63s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1136.33 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 122.04s | training loss  0.18 |
    | end of validation epoch  32 | time: 84.94s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681]
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1139.92 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 122.35s | training loss  0.17 |
    | end of validation epoch  33 | time: 79.58s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1140.59 | loss  0.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 122.49s | training loss  0.17 |
    | end of validation epoch  34 | time: 81.60s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558]
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1133.17 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 121.89s | training loss  0.16 |
    | end of validation epoch  35 | time: 85.05s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1138.07 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 122.28s | training loss  0.15 |
    | end of validation epoch  36 | time: 79.55s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1152.34 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 124.38s | training loss  0.15 |
    | end of validation epoch  37 | time: 81.27s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1138.83 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 123.05s | training loss  0.14 |
    | end of validation epoch  38 | time: 84.64s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 1140.26 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 122.77s | training loss  0.14 |
    | end of validation epoch  39 | time: 79.63s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915]
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 1140.62 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 122.61s | training loss  0.12 |
    | end of validation epoch  40 | time: 82.22s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 1132.32 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 122.46s | training loss  0.13 |
    | end of validation epoch  41 | time: 84.80s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 1135.95 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 121.95s | training loss  0.13 |
    | end of validation epoch  42 | time: 79.62s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 1144.09 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 122.91s | training loss  0.13 |
    | end of validation epoch  43 | time: 82.07s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 1132.46 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 122.96s | training loss  0.13 |
    | end of validation epoch  44 | time: 84.85s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729]
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 1135.64 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 123.65s | training loss  0.12 |
    | end of validation epoch  45 | time: 79.62s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1140.66 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 123.46s | training loss  0.11 |
    | end of validation epoch  46 | time: 81.60s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 1146.90 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 124.11s | training loss  0.11 |
    | end of validation epoch  47 | time: 85.17s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 1137.32 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 122.19s | training loss  0.12 |
    | end of validation epoch  48 | time: 79.29s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 1141.34 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 122.60s | training loss  0.11 |
    | end of validation epoch  49 | time: 81.73s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 1141.99 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 123.07s | training loss  0.10 |
    | end of validation epoch  50 | time: 84.86s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 1141.55 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 122.84s | training loss  0.10 |
    | end of validation epoch  51 | time: 79.52s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 1147.59 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 123.42s | training loss  0.10 |
    | end of validation epoch  52 | time: 86.67s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 1131.61 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 121.51s | training loss  0.10 |
    | end of validation epoch  53 | time: 84.35s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 1128.60 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 121.44s | training loss  0.10 |
    | end of validation epoch  54 | time: 79.14s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1132.44 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 121.61s | training loss  0.10 |
    | end of validation epoch  55 | time: 81.80s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 1123.54 | loss  0.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 121.44s | training loss  0.10 |
    | end of validation epoch  56 | time: 85.09s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 1132.52 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 121.98s | training loss  0.09 |
    | end of validation epoch  57 | time: 79.87s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 1136.38 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 122.67s | training loss  0.10 |
    | end of validation epoch  58 | time: 81.50s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 1129.59 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 122.19s | training loss  0.08 |
    | end of validation epoch  59 | time: 84.60s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 1135.91 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 122.34s | training loss  0.09 |
    | end of validation epoch  60 | time: 79.84s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 1143.28 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 122.96s | training loss  0.10 |
    | end of validation epoch  61 | time: 81.59s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 1134.14 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 122.21s | training loss  0.08 |
    | end of validation epoch  62 | time: 85.09s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 1133.54 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 122.60s | training loss  0.08 |
    | end of validation epoch  63 | time: 79.58s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 1189.08 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 127.70s | training loss  0.09 |
    | end of validation epoch  64 | time: 81.90s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 1134.94 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 122.54s | training loss  0.08 |
    | end of validation epoch  65 | time: 84.87s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 1140.35 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 122.64s | training loss  0.08 |
    | end of validation epoch  66 | time: 79.32s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 1142.94 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 122.83s | training loss  0.07 |
    | end of validation epoch  67 | time: 82.03s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 1134.76 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 121.97s | training loss  0.07 |
    | end of validation epoch  68 | time: 84.76s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 1140.55 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 122.36s | training loss  0.07 |
    | end of validation epoch  69 | time: 79.93s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 1142.03 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 123.19s | training loss  0.07 |
    | end of validation epoch  70 | time: 81.55s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 1131.11 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 122.73s | training loss  0.07 |
    | end of validation epoch  71 | time: 85.00s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 1139.78 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 122.44s | training loss  0.08 |
    | end of validation epoch  72 | time: 79.31s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 1142.26 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 122.59s | training loss  0.08 |
    | end of validation epoch  73 | time: 81.49s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 1133.62 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 122.01s | training loss  0.08 |
    | end of validation epoch  74 | time: 84.86s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 1134.49 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 121.97s | training loss  0.07 |
    | end of validation epoch  75 | time: 80.01s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 1140.70 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 122.83s | training loss  0.07 |
    | end of validation epoch  76 | time: 81.54s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 1131.15 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 121.89s | training loss  0.07 |
    | end of validation epoch  77 | time: 84.83s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 1138.84 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 122.46s | training loss  0.07 |
    | end of validation epoch  78 | time: 79.61s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 1140.52 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 122.46s | training loss  0.08 |
    | end of validation epoch  79 | time: 81.78s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 1133.61 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 121.86s | training loss  0.06 |
    | end of validation epoch  80 | time: 84.58s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 1135.17 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 122.66s | training loss  0.06 |
    | end of validation epoch  81 | time: 79.59s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 1142.19 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 122.87s | training loss  0.06 |
    | end of validation epoch  82 | time: 81.45s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 1129.78 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 122.46s | training loss  0.06 |
    | end of validation epoch  83 | time: 85.11s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 1144.98 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 122.97s | training loss  0.06 |
    | end of validation epoch  84 | time: 79.85s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 1141.83 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 122.67s | training loss  0.06 |
    | end of validation epoch  85 | time: 81.89s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 1133.73 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 122.14s | training loss  0.08 |
    | end of validation epoch  86 | time: 85.30s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1205.24 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 129.72s | training loss  0.06 |
    | end of validation epoch  87 | time: 79.80s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 1140.08 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 123.68s | training loss  0.06 |
    | end of validation epoch  88 | time: 81.96s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 1135.97 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 121.79s | training loss  0.05 |
    | end of validation epoch  89 | time: 85.02s | validation loss  1.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 1138.00 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 122.48s | training loss  0.06 |
    | end of validation epoch  90 | time: 80.03s | validation loss  1.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 1142.85 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 122.99s | training loss  0.06 |
    | end of validation epoch  91 | time: 81.32s | validation loss  1.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 1132.18 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 122.15s | training loss  0.06 |
    | end of validation epoch  92 | time: 84.87s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 1138.76 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 122.31s | training loss  0.06 |
    | end of validation epoch  93 | time: 79.53s | validation loss  1.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 1142.45 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 122.66s | training loss  0.05 |
    | end of validation epoch  94 | time: 81.69s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1132.38 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 122.51s | training loss  0.05 |
    | end of validation epoch  95 | time: 85.12s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357, 0.053347332323355985] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236, 1.593689275809993]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 1134.75 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 122.21s | training loss  0.05 |
    | end of validation epoch  96 | time: 80.14s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357, 0.053347332323355985, 0.05466625787032483] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236, 1.593689275809993, 1.4359990594081562]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 1143.16 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 122.69s | training loss  0.05 |
    | end of validation epoch  97 | time: 81.85s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357, 0.053347332323355985, 0.05466625787032483, 0.04967251359401187] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236, 1.593689275809993, 1.4359990594081562, 1.4251221562541712]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 1135.63 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 122.75s | training loss  0.04 |
    | end of validation epoch  98 | time: 84.89s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357, 0.053347332323355985, 0.05466625787032483, 0.04967251359401187, 0.04471205276445494] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236, 1.593689275809993, 1.4359990594081562, 1.4251221562541712, 1.4228836110657237]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 1140.69 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 122.79s | training loss  0.05 |
    | end of validation epoch  99 | time: 79.79s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [1.6988831339655697, 1.16539022514412, 0.9720854641080977, 0.8439043180362599, 0.7567345422130447, 0.6882471265019597, 0.6072783757437457, 0.5727019930208052, 0.5339941689828495, 0.4839350916780867, 0.45051658489145674, 0.4217350536638552, 0.38066969395757794, 0.3725443354866526, 0.350176235442763, 0.34795923880091656, 0.3113931331548605, 0.3126276163636027, 0.2904209208649558, 0.2811880138543275, 0.26397077423763704, 0.244308940663531, 0.22927340126789367, 0.22158885411582552, 0.22249930372109283, 0.2149635711768726, 0.21261798644119556, 0.18817478243832117, 0.19281692030999037, 0.1814628446491452, 0.17005920349746137, 0.17714314931282052, 0.1735576430822278, 0.17028474384868467, 0.16203096297544403, 0.1482854602304665, 0.14813201552307284, 0.13816778717545775, 0.14333597780415067, 0.12467608664621103, 0.12554235890641943, 0.13354721272716652, 0.12825820175511343, 0.13160121987934584, 0.12266835644169971, 0.11332553529457466, 0.11242785824982969, 0.1151137313730008, 0.10957809457102337, 0.1018848067468351, 0.0961734941480933, 0.09813378355256072, 0.10297212400683411, 0.09965423180780432, 0.1005693412813786, 0.0956849133646166, 0.08727595830957095, 0.09890161995012481, 0.07980303149166945, 0.0924843445185337, 0.09840312695785149, 0.08295329089637275, 0.07711181802222052, 0.08527873857534146, 0.07944648452774361, 0.0830029411207851, 0.07299046361869252, 0.0739159960433975, 0.0661104292365479, 0.06841805491697144, 0.06620044770507931, 0.07959014114395187, 0.07595756616409835, 0.0767533243843564, 0.07321034077353575, 0.07283396123665141, 0.06758934650402348, 0.06786870241567895, 0.07596451054990024, 0.06278600772792423, 0.06322074143702651, 0.060060349326614325, 0.05732182463471618, 0.05941458705913376, 0.05721743630564159, 0.07562776220341523, 0.05962359978657988, 0.05710654218706328, 0.0505193503120461, 0.05639311865911827, 0.05963158469043068, 0.05532478649370574, 0.05629771914299544, 0.04923671157401357, 0.053347332323355985, 0.05466625787032483, 0.04967251359401187, 0.04471205276445494, 0.05241786389452246] validation loss is  [1.0858629231030743, 1.0319232733454555, 0.9976114342765262, 1.0749484216600347, 1.016526195181844, 0.9987977629061788, 1.0213469841595118, 1.078668232250493, 1.0374149755807593, 1.0462074034924929, 1.0877084915215771, 1.1495482571675286, 1.0340837456460577, 1.1783859512458246, 1.0521103877496596, 1.0904338468487065, 1.0730651582901676, 1.0380788930536557, 1.316078745527193, 1.1461357787096251, 1.1976131720609071, 1.1550336583944347, 1.3586754956049845, 1.1322922535667506, 1.172665854547328, 1.2563525825777713, 1.027709734432089, 1.0862867658336957, 1.1342482266870018, 1.3649963860370917, 1.3376328986487351, 1.2188922009857681, 1.2017716991103953, 1.1532969285423558, 1.1935628669258829, 1.243657072140195, 1.2335061376555434, 1.2297956967765156, 1.3485646788418915, 1.2868009601061203, 1.1331681588004965, 1.2217268079063313, 1.419579882414837, 1.2135605152749729, 1.1921619062195532, 1.3840209825430065, 1.2469035273388727, 1.2757308549286488, 1.2947216373092185, 1.3066488519931834, 1.327517841780112, 1.252173323892445, 1.5417401099887986, 1.264743636065153, 1.3243707253908117, 1.500038134205776, 1.5342276906158077, 1.3123268120992482, 1.3935479598197464, 1.441214796791731, 1.3805816417686099, 1.3382955919187225, 1.3770084245916223, 1.417373250044572, 1.2924984646379016, 1.4272745645866962, 1.4866864215388584, 1.4157569426073071, 1.4483532537725903, 1.3165474749160542, 1.5304354752467286, 1.4720440147078382, 1.5481501458756004, 1.435672917683405, 1.5530219281384536, 1.5230794722447172, 1.3720234612798474, 1.5002146974826853, 1.5209998331823347, 1.5167750797457604, 1.556324044991925, 1.568452207034473, 1.5999674232880352, 1.522770727771179, 1.56419856287539, 1.525199359749725, 1.631763214069603, 1.3693365573417395, 1.6474751412121502, 1.7495222747869168, 1.644778221050122, 1.450904598343186, 1.6868133126602818, 1.6179331146727236, 1.593689275809993, 1.4359990594081562, 1.4251221562541712, 1.4228836110657237, 1.4189616471679376]
