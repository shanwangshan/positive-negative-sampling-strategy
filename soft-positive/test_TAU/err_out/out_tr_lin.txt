/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1149.77 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 123.97s | training loss  2.97 |
    | end of validation epoch   1 | time: 83.56s | validation loss  2.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.9670324368519827] validation loss is  [2.0729401956001916]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1115.16 | loss  2.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 122.43s | training loss  2.62 |
    | end of validation epoch   2 | time: 83.25s | validation loss  1.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.9670324368519827, 2.619785418381562] validation loss is  [2.0729401956001916, 1.8560847739378612]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 1123.57 | loss  2.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 121.95s | training loss  2.41 |
    | end of validation epoch   3 | time: 75.26s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 1136.25 | loss  2.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 122.19s | training loss  2.27 |
    | end of validation epoch   4 | time: 77.35s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 1114.78 | loss  2.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 120.31s | training loss  2.16 |
    | end of validation epoch   5 | time: 82.85s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 1128.48 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 121.50s | training loss  2.05 |
    | end of validation epoch   6 | time: 76.14s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1189.29 | loss  2.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 127.73s | training loss  1.97 |
    | end of validation epoch   7 | time: 76.71s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1121.86 | loss  2.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 121.34s | training loss  1.92 |
    | end of validation epoch   8 | time: 81.90s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1131.67 | loss  2.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 122.21s | training loss  1.86 |
    | end of validation epoch   9 | time: 76.61s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1132.54 | loss  1.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 121.79s | training loss  1.82 |
    | end of validation epoch  10 | time: 76.75s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1120.72 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 120.83s | training loss  1.78 |
    | end of validation epoch  11 | time: 82.29s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1123.05 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 121.67s | training loss  1.76 |
    | end of validation epoch  12 | time: 76.60s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1127.35 | loss  1.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 121.54s | training loss  1.72 |
    | end of validation epoch  13 | time: 76.85s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1123.48 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 120.97s | training loss  1.70 |
    | end of validation epoch  14 | time: 82.07s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1126.19 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 121.35s | training loss  1.65 |
    | end of validation epoch  15 | time: 75.85s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1133.83 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 122.29s | training loss  1.64 |
    | end of validation epoch  16 | time: 80.82s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1132.63 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 122.17s | training loss  1.62 |
    | end of validation epoch  17 | time: 83.21s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1198.27 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 130.14s | training loss  1.61 |
    | end of validation epoch  18 | time: 75.82s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1163.37 | loss  1.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 126.92s | training loss  1.58 |
    | end of validation epoch  19 | time: 78.32s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 1140.47 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 123.13s | training loss  1.56 |
    | end of validation epoch  20 | time: 82.31s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 1144.45 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 123.52s | training loss  1.54 |
    | end of validation epoch  21 | time: 76.99s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1142.44 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 123.76s | training loss  1.53 |
    | end of validation epoch  22 | time: 77.42s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1135.58 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 122.64s | training loss  1.52 |
    | end of validation epoch  23 | time: 82.28s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1129.33 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 122.75s | training loss  1.52 |
    | end of validation epoch  24 | time: 76.43s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1135.19 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 122.21s | training loss  1.50 |
    | end of validation epoch  25 | time: 77.86s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1126.33 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 121.45s | training loss  1.50 |
    | end of validation epoch  26 | time: 82.29s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1135.58 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 122.49s | training loss  1.51 |
    | end of validation epoch  27 | time: 76.21s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627]
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1135.55 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 122.31s | training loss  1.49 |
    | end of validation epoch  28 | time: 77.26s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1123.25 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 121.53s | training loss  1.49 |
    | end of validation epoch  29 | time: 81.80s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1127.14 | loss  1.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 121.47s | training loss  1.48 |
    | end of validation epoch  30 | time: 76.29s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1131.05 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 122.93s | training loss  1.46 |
    | end of validation epoch  31 | time: 77.91s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1126.63 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 122.69s | training loss  1.47 |
    | end of validation epoch  32 | time: 81.80s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036]
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1129.47 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 121.77s | training loss  1.46 |
    | end of validation epoch  33 | time: 76.42s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1132.77 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 122.13s | training loss  1.45 |
    | end of validation epoch  34 | time: 76.99s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1122.65 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 121.04s | training loss  1.45 |
    | end of validation epoch  35 | time: 82.77s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1132.28 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 121.89s | training loss  1.46 |
    | end of validation epoch  36 | time: 76.88s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881]
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1130.82 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 121.79s | training loss  1.43 |
    | end of validation epoch  37 | time: 77.56s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1125.86 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 121.47s | training loss  1.42 |
    | end of validation epoch  38 | time: 82.38s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 1123.55 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 121.92s | training loss  1.43 |
    | end of validation epoch  39 | time: 76.81s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 1132.50 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 121.93s | training loss  1.42 |
    | end of validation epoch  40 | time: 77.86s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 1131.37 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 122.04s | training loss  1.44 |
    | end of validation epoch  41 | time: 82.30s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 1126.72 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 121.44s | training loss  1.42 |
    | end of validation epoch  42 | time: 76.37s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 1126.90 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 121.57s | training loss  1.42 |
    | end of validation epoch  43 | time: 77.40s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 1126.59 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 122.44s | training loss  1.42 |
    | end of validation epoch  44 | time: 82.14s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872]
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 1125.46 | loss  1.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 121.28s | training loss  1.41 |
    | end of validation epoch  45 | time: 76.67s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1130.10 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 121.62s | training loss  1.41 |
    | end of validation epoch  46 | time: 77.07s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 1128.25 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 121.71s | training loss  1.41 |
    | end of validation epoch  47 | time: 82.96s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 1127.99 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 121.36s | training loss  1.43 |
    | end of validation epoch  48 | time: 76.20s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 1135.92 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 122.58s | training loss  1.42 |
    | end of validation epoch  49 | time: 79.14s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602]
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 1126.73 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 121.31s | training loss  1.41 |
    | end of validation epoch  50 | time: 82.06s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 1130.55 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 121.67s | training loss  1.41 |
    | end of validation epoch  51 | time: 75.74s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 1131.32 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 121.92s | training loss  1.40 |
    | end of validation epoch  52 | time: 76.96s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 1120.28 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 120.94s | training loss  1.40 |
    | end of validation epoch  53 | time: 86.70s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 1123.58 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 120.99s | training loss  1.41 |
    | end of validation epoch  54 | time: 76.15s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1125.55 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 121.07s | training loss  1.40 |
    | end of validation epoch  55 | time: 77.53s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 1123.97 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 121.29s | training loss  1.39 |
    | end of validation epoch  56 | time: 82.00s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 1125.16 | loss  1.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 121.26s | training loss  1.40 |
    | end of validation epoch  57 | time: 76.12s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 1124.39 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 121.04s | training loss  1.40 |
    | end of validation epoch  58 | time: 77.45s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 1122.18 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 121.92s | training loss  1.40 |
    | end of validation epoch  59 | time: 81.79s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 1119.38 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 120.58s | training loss  1.40 |
    | end of validation epoch  60 | time: 75.94s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 1131.91 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 121.83s | training loss  1.40 |
    | end of validation epoch  61 | time: 77.09s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 1118.09 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 121.00s | training loss  1.41 |
    | end of validation epoch  62 | time: 82.29s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 1123.53 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 121.10s | training loss  1.39 |
    | end of validation epoch  63 | time: 76.21s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 1128.25 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 121.40s | training loss  1.38 |
    | end of validation epoch  64 | time: 77.23s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 1165.06 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 125.31s | training loss  1.40 |
    | end of validation epoch  65 | time: 81.80s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 1126.96 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 121.42s | training loss  1.41 |
    | end of validation epoch  66 | time: 76.17s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 1124.63 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 121.99s | training loss  1.40 |
    | end of validation epoch  67 | time: 77.15s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 1121.68 | loss  1.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 120.65s | training loss  1.40 |
    | end of validation epoch  68 | time: 81.51s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 1127.10 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 121.38s | training loss  1.40 |
    | end of validation epoch  69 | time: 76.10s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 1124.15 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 121.05s | training loss  1.39 |
    | end of validation epoch  70 | time: 77.56s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 1124.43 | loss  1.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 121.02s | training loss  1.40 |
    | end of validation epoch  71 | time: 81.86s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 1125.28 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 121.06s | training loss  1.40 |
    | end of validation epoch  72 | time: 75.59s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 1127.47 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 121.26s | training loss  1.39 |
    | end of validation epoch  73 | time: 77.17s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 1121.07 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 120.64s | training loss  1.38 |
    | end of validation epoch  74 | time: 81.89s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 1122.93 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 120.90s | training loss  1.38 |
    | end of validation epoch  75 | time: 75.93s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 1127.15 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 121.35s | training loss  1.39 |
    | end of validation epoch  76 | time: 76.88s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 1121.48 | loss  1.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 121.12s | training loss  1.39 |
    | end of validation epoch  77 | time: 82.05s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 1124.74 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 121.28s | training loss  1.41 |
    | end of validation epoch  78 | time: 76.55s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 1125.64 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 121.11s | training loss  1.39 |
    | end of validation epoch  79 | time: 76.16s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 1123.14 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 120.93s | training loss  1.40 |
    | end of validation epoch  80 | time: 81.84s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 1121.33 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 120.81s | training loss  1.39 |
    | end of validation epoch  81 | time: 76.16s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 1123.97 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 121.43s | training loss  1.39 |
    | end of validation epoch  82 | time: 77.22s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 1120.63 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 121.42s | training loss  1.39 |
    | end of validation epoch  83 | time: 82.23s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 1122.96 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 120.97s | training loss  1.38 |
    | end of validation epoch  84 | time: 75.76s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 1136.44 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 122.21s | training loss  1.39 |
    | end of validation epoch  85 | time: 79.46s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 1121.81 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 121.01s | training loss  1.40 |
    | end of validation epoch  86 | time: 82.13s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1125.40 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 121.51s | training loss  1.38 |
    | end of validation epoch  87 | time: 75.65s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 1128.01 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 121.46s | training loss  1.39 |
    | end of validation epoch  88 | time: 84.09s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 1128.12 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 121.75s | training loss  1.40 |
    | end of validation epoch  89 | time: 82.11s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 1124.99 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 121.35s | training loss  1.39 |
    | end of validation epoch  90 | time: 76.30s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 1132.30 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 121.90s | training loss  1.38 |
    | end of validation epoch  91 | time: 77.17s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 1120.77 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 121.22s | training loss  1.41 |
    | end of validation epoch  92 | time: 82.43s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 1122.14 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 121.15s | training loss  1.38 |
    | end of validation epoch  93 | time: 76.38s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 1130.05 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 121.70s | training loss  1.39 |
    | end of validation epoch  94 | time: 77.17s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1122.14 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 121.10s | training loss  1.39 |
    | end of validation epoch  95 | time: 82.76s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803, 1.3901965618133545] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868, 1.1211654891570408]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 1123.36 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 121.01s | training loss  1.40 |
    | end of validation epoch  96 | time: 76.36s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803, 1.3901965618133545, 1.4049271559930063] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868, 1.1211654891570408, 1.1481260412062209]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 1128.32 | loss  1.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 121.83s | training loss  1.38 |
    | end of validation epoch  97 | time: 76.67s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803, 1.3901965618133545, 1.4049271559930063, 1.3796116317714657] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868, 1.1211654891570408, 1.1481260412062209, 1.1201597740873694]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 1120.04 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 120.83s | training loss  1.39 |
    | end of validation epoch  98 | time: 82.15s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803, 1.3901965618133545, 1.4049271559930063, 1.3796116317714657, 1.3923585307490718] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868, 1.1211654891570408, 1.1481260412062209, 1.1201597740873694, 1.1126201112444203]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 1128.09 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 121.44s | training loss  1.39 |
    | end of validation epoch  99 | time: 76.53s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.9670324368519827, 2.619785418381562, 2.413171287055488, 2.272561016383472, 2.1591402120418377, 2.0499645471572876, 1.9680674409007166, 1.9198862142391033, 1.8590229094565451, 1.8162452723528888, 1.7774956580754873, 1.7550655496013057, 1.72275089036237, 1.6962547635172938, 1.649490990080275, 1.6419242502332807, 1.6177017656532493, 1.605126590342135, 1.5788844082806561, 1.5620521607699696, 1.5382120029346362, 1.5259257490570481, 1.5194829841991802, 1.5158859061765242, 1.5049956886617988, 1.5006476619222142, 1.5114151089041084, 1.4895058311857619, 1.493415043160722, 1.480449870899991, 1.4588547354345922, 1.472329229921908, 1.459200847256291, 1.4503723050022985, 1.4486268264753324, 1.4577411466890626, 1.4336735278636485, 1.4240682952038877, 1.4268529243297405, 1.4183933036821383, 1.4353523963206523, 1.4241408206321098, 1.4196598175409678, 1.4204014636374809, 1.4083868050360464, 1.414672445606541, 1.4123148875193552, 1.4275278304074261, 1.4155103365580242, 1.4121253050125397, 1.4082097395046338, 1.4023935204153661, 1.4026125648000218, 1.4063766432238054, 1.3995098455532178, 1.3922110022725285, 1.4005760287379359, 1.3969707929336272, 1.3965420658523973, 1.3992920136666513, 1.40202631606712, 1.4065818765141942, 1.387847824139638, 1.3819249406591192, 1.3951341848115664, 1.406268470996135, 1.402046425922497, 1.404988829080049, 1.3962339721284471, 1.389010097529437, 1.3993237469647382, 1.3969432888804256, 1.3948086113543123, 1.3829178616807267, 1.3849721921456826, 1.3949871739825688, 1.3911697735657562, 1.4093519481452736, 1.3936737152907226, 1.3980523004188194, 1.3918412896963928, 1.388208247519828, 1.3940365497056428, 1.3831303549242449, 1.3861562851312998, 1.4029027105451704, 1.3848767323536917, 1.3867645199234422, 1.4043684725288872, 1.3902594624338924, 1.3797975381215413, 1.4053354424399298, 1.3796603583000802, 1.3944968071069803, 1.3901965618133545, 1.4049271559930063, 1.3796116317714657, 1.3923585307490718, 1.3946891496847342] validation loss is  [2.0729401956001916, 1.8560847739378612, 1.6847286224365234, 1.5999751711885135, 1.5066159976025422, 1.4501581874986489, 1.4073137566447258, 1.356195290883382, 1.3313848910232384, 1.3310304594536622, 1.3086328332622845, 1.2866681131223838, 1.2674891855567694, 1.254370170334975, 1.2490350734442472, 1.2599034861971934, 1.23025627185901, 1.231377615282933, 1.204976759230097, 1.2125976650665204, 1.1985983749230702, 1.1853696775312226, 1.1922046610464652, 1.1885153697803617, 1.186538145877421, 1.171540254727006, 1.1730868344505627, 1.173440585223337, 1.1654440195610125, 1.1723820551608999, 1.1656145425513387, 1.1696459610636036, 1.1756538733219106, 1.16152216711392, 1.1588855640341837, 1.1617142862329881, 1.154062080817918, 1.164932236696283, 1.1459053919340174, 1.161857416542868, 1.167987114439408, 1.14419233178099, 1.160289131725828, 1.154992585380872, 1.1619537345444162, 1.15352467695872, 1.1484387343128521, 1.1426247423514724, 1.1502536007513602, 1.1397155582283933, 1.1391024831682444, 1.1443882004047434, 1.1367958110446732, 1.1356046525761485, 1.1412237553546827, 1.1354775798196595, 1.1372307514150937, 1.131923709064722, 1.1496497423698504, 1.1537464916085203, 1.1406885261336963, 1.13917069354405, 1.1230734124158819, 1.1408710218966007, 1.1426219611118238, 1.119158084814747, 1.1149842254817486, 1.1293188882991672, 1.1385740001375477, 1.1319595500826836, 1.143593559662501, 1.1270354349787037, 1.1234727343544364, 1.132056293077767, 1.131563091961046, 1.126706346248587, 1.1252928624550502, 1.1261782196039956, 1.1237756072854002, 1.1237612776458263, 1.1301886529351275, 1.1219400816286604, 1.1169884214177728, 1.1314175066848595, 1.133755799382925, 1.1241248144457738, 1.1457093004137278, 1.1464540896316369, 1.122728429424266, 1.1186664579436183, 1.1178715045874317, 1.1165035646408796, 1.1193906391660373, 1.1211846486354868, 1.1211654891570408, 1.1481260412062209, 1.1201597740873694, 1.1126201112444203, 1.1067653133844335]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
