{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 2910.57 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 351.26s | training loss  3.14 |
    | end of validation epoch   1 | time: 100.12s | validation loss  2.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [3.1420509836695216] validation loss is  [2.1796791156133017]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1047.32 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 112.22s | training loss  2.80 |
    | end of validation epoch   2 | time: 44.20s | validation loss  1.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [3.1420509836695216, 2.7995409793681927] validation loss is  [2.1796791156133017, 1.9324800198276837]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 1104.69 | loss  2.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 115.83s | training loss  2.57 |
    | end of validation epoch   3 | time: 40.67s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 641.21 | loss  2.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 70.90s | training loss  2.39 |
    | end of validation epoch   4 | time: 39.21s | validation loss  1.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 666.41 | loss  2.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 73.20s | training loss  2.26 |
    | end of validation epoch   5 | time: 47.70s | validation loss  1.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 1314.36 | loss  2.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 140.99s | training loss  2.17 |
    | end of validation epoch   6 | time: 52.71s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1286.49 | loss  1.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 137.07s | training loss  2.06 |
    | end of validation epoch   7 | time: 60.89s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1844.51 | loss  1.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 191.82s | training loss  1.99 |
    | end of validation epoch   8 | time: 52.96s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 809.92 | loss  1.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 88.91s | training loss  1.94 |
    | end of validation epoch   9 | time: 41.36s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 717.82 | loss  1.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 77.37s | training loss  1.92 |
    | end of validation epoch  10 | time: 82.00s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1051.23 | loss  2.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 112.66s | training loss  1.86 |
    | end of validation epoch  11 | time: 54.32s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 698.78 | loss  2.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 75.68s | training loss  1.82 |
    | end of validation epoch  12 | time: 41.34s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 764.95 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 119.32s | training loss  1.79 |
    | end of validation epoch  13 | time: 47.37s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 890.60 | loss  1.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 91.00s | training loss  1.75 |
    | end of validation epoch  14 | time: 52.61s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 874.71 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 94.61s | training loss  1.72 |
    | end of validation epoch  15 | time: 38.81s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1107.87 | loss  1.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 115.85s | training loss  1.71 |
    | end of validation epoch  16 | time: 46.48s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 655.71 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 70.94s | training loss  1.68 |
    | end of validation epoch  17 | time: 39.34s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 634.73 | loss  1.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 68.96s | training loss  1.66 |
    | end of validation epoch  18 | time: 42.24s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1363.95 | loss  1.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 136.63s | training loss  1.65 |
    | end of validation epoch  19 | time: 56.18s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 907.42 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 98.04s | training loss  1.64 |
    | end of validation epoch  20 | time: 38.88s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 659.30 | loss  1.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 71.10s | training loss  1.61 |
    | end of validation epoch  21 | time: 45.88s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1117.78 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 118.68s | training loss  1.60 |
    | end of validation epoch  22 | time: 41.09s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 655.49 | loss  1.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 70.40s | training loss  1.59 |
    | end of validation epoch  23 | time: 41.19s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 640.41 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 68.90s | training loss  1.58 |
    | end of validation epoch  24 | time: 41.80s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 815.16 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 124.20s | training loss  1.57 |
    | end of validation epoch  25 | time: 52.06s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 646.93 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 69.82s | training loss  1.55 |
    | end of validation epoch  26 | time: 39.69s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 623.49 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 67.79s | training loss  1.54 |
    | end of validation epoch  27 | time: 40.05s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 632.93 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 68.62s | training loss  1.54 |
    | end of validation epoch  28 | time: 41.65s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803]
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 967.52 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 101.44s | training loss  1.52 |
    | end of validation epoch  29 | time: 39.47s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 610.15 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 66.13s | training loss  1.54 |
    | end of validation epoch  30 | time: 39.67s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 618.06 | loss  1.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 65.94s | training loss  1.53 |
    | end of validation epoch  31 | time: 41.54s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827]
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 948.18 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 100.91s | training loss  1.51 |
    | end of validation epoch  32 | time: 44.60s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 635.98 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 68.87s | training loss  1.52 |
    | end of validation epoch  33 | time: 40.35s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 611.33 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 66.01s | training loss  1.50 |
    | end of validation epoch  34 | time: 40.67s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 613.76 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 66.26s | training loss  1.50 |
    | end of validation epoch  35 | time: 41.34s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1022.67 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 108.15s | training loss  1.50 |
    | end of validation epoch  36 | time: 40.80s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 606.61 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 65.97s | training loss  1.50 |
    | end of validation epoch  37 | time: 41.48s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 594.07 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 65.26s | training loss  1.50 |
    | end of validation epoch  38 | time: 40.31s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 906.85 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 99.26s | training loss  1.49 |
    | end of validation epoch  39 | time: 41.18s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 609.91 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 65.85s | training loss  1.47 |
    | end of validation epoch  40 | time: 39.89s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 614.78 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 66.56s | training loss  1.49 |
    | end of validation epoch  41 | time: 40.41s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 613.79 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 66.34s | training loss  1.48 |
    | end of validation epoch  42 | time: 40.52s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 961.03 | loss  1.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 104.12s | training loss  1.49 |
    | end of validation epoch  43 | time: 40.01s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 694.75 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 76.68s | training loss  1.48 |
    | end of validation epoch  44 | time: 51.73s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 825.18 | loss  1.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 88.60s | training loss  1.49 |
    | end of validation epoch  45 | time: 39.28s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1083.73 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 113.21s | training loss  1.47 |
    | end of validation epoch  46 | time: 39.78s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 625.88 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 67.53s | training loss  1.48 |
    | end of validation epoch  47 | time: 48.53s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 901.01 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 97.48s | training loss  1.45 |
    | end of validation epoch  48 | time: 39.21s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 1077.99 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 115.10s | training loss  1.47 |
    | end of validation epoch  49 | time: 49.13s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666]
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 904.29 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 98.03s | training loss  1.46 |
    | end of validation epoch  50 | time: 40.79s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 663.78 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 71.29s | training loss  1.46 |
    | end of validation epoch  51 | time: 54.86s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 844.41 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 88.99s | training loss  1.47 |
    | end of validation epoch  52 | time: 39.97s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 774.05 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 84.24s | training loss  1.46 |
    | end of validation epoch  53 | time: 51.32s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 865.89 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 95.06s | training loss  1.46 |
    | end of validation epoch  54 | time: 39.16s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1016.15 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 106.43s | training loss  1.46 |
    | end of validation epoch  55 | time: 42.28s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 613.58 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 66.33s | training loss  1.46 |
    | end of validation epoch  56 | time: 39.07s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 603.25 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 65.48s | training loss  1.47 |
    | end of validation epoch  57 | time: 39.66s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 908.06 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 99.11s | training loss  1.46 |
    | end of validation epoch  58 | time: 39.04s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 605.80 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 65.14s | training loss  1.45 |
    | end of validation epoch  59 | time: 39.31s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 590.67 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 65.65s | training loss  1.47 |
    | end of validation epoch  60 | time: 39.46s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 607.02 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 65.88s | training loss  1.45 |
    | end of validation epoch  61 | time: 75.06s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 772.53 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 82.84s | training loss  1.45 |
    | end of validation epoch  62 | time: 42.84s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 656.19 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 71.11s | training loss  1.46 |
    | end of validation epoch  63 | time: 43.62s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 682.27 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 73.66s | training loss  1.46 |
    | end of validation epoch  64 | time: 40.68s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 1060.73 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 113.38s | training loss  1.46 |
    | end of validation epoch  65 | time: 41.76s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 654.52 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 70.47s | training loss  1.46 |
    | end of validation epoch  66 | time: 40.10s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 614.64 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 66.93s | training loss  1.47 |
    | end of validation epoch  67 | time: 38.84s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 612.88 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 66.79s | training loss  1.45 |
    | end of validation epoch  68 | time: 64.07s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 651.24 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 69.83s | training loss  1.46 |
    | end of validation epoch  69 | time: 45.45s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 589.14 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 63.95s | training loss  1.45 |
    | end of validation epoch  70 | time: 40.00s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 578.21 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 63.10s | training loss  1.44 |
    | end of validation epoch  71 | time: 41.89s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 965.27 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 102.49s | training loss  1.44 |
    | end of validation epoch  72 | time: 41.31s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 596.25 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 64.46s | training loss  1.46 |
    | end of validation epoch  73 | time: 40.43s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 575.75 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 63.36s | training loss  1.46 |
    | end of validation epoch  74 | time: 39.88s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 597.99 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 64.20s | training loss  1.45 |
    | end of validation epoch  75 | time: 40.05s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 996.89 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 102.99s | training loss  1.45 |
    | end of validation epoch  76 | time: 39.30s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 601.06 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 64.55s | training loss  1.44 |
    | end of validation epoch  77 | time: 42.36s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 590.54 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 63.66s | training loss  1.44 |
    | end of validation epoch  78 | time: 38.83s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 586.95 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 95.48s | training loss  1.45 |
    | end of validation epoch  79 | time: 38.94s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 545.76 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 59.02s | training loss  1.45 |
    | end of validation epoch  80 | time: 38.28s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 546.63 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 58.93s | training loss  1.44 |
    | end of validation epoch  81 | time: 38.60s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 544.07 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 58.59s | training loss  1.45 |
    | end of validation epoch  82 | time: 38.24s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 808.98 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 87.27s | training loss  1.44 |
    | end of validation epoch  83 | time: 38.59s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 552.92 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 59.62s | training loss  1.45 |
    | end of validation epoch  84 | time: 38.68s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 546.43 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 58.90s | training loss  1.44 |
    | end of validation epoch  85 | time: 38.50s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 540.89 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 58.32s | training loss  1.45 |
    | end of validation epoch  86 | time: 40.67s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1136.34 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 118.44s | training loss  1.46 |
    | end of validation epoch  87 | time: 43.29s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 578.34 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 63.14s | training loss  1.45 |
    | end of validation epoch  88 | time: 40.62s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 562.21 | loss  1.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 60.59s | training loss  1.46 |
    | end of validation epoch  89 | time: 38.19s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 554.31 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 59.88s | training loss  1.45 |
    | end of validation epoch  90 | time: 38.37s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 775.54 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 85.14s | training loss  1.45 |
    | end of validation epoch  91 | time: 40.80s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 543.27 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 59.65s | training loss  1.47 |
    | end of validation epoch  92 | time: 38.15s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 537.53 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 58.07s | training loss  1.44 |
    | end of validation epoch  93 | time: 40.72s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 550.08 | loss  1.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 59.34s | training loss  1.44 |
    | end of validation epoch  94 | time: 38.42s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 732.66 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 77.88s | training loss  1.44 |
    | end of validation epoch  95 | time: 45.05s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292, 1.43973095459981] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099, 1.184392058600982]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 611.34 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 67.18s | training loss  1.45 |
    | end of validation epoch  96 | time: 38.38s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292, 1.43973095459981, 1.447619730287844] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099, 1.184392058600982, 1.2017183278997738]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 557.30 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 60.76s | training loss  1.46 |
    | end of validation epoch  97 | time: 38.32s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292, 1.43973095459981, 1.447619730287844, 1.4561283556190696] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099, 1.184392058600982, 1.2017183278997738, 1.1993451944241922]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 562.97 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 61.75s | training loss  1.44 |
    | end of validation epoch  98 | time: 38.13s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292, 1.43973095459981, 1.447619730287844, 1.4561283556190696, 1.441332137262499] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099, 1.184392058600982, 1.2017183278997738, 1.1993451944241922, 1.187456293652455]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 571.64 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 61.40s | training loss  1.44 |
    | end of validation epoch  99 | time: 38.33s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [3.1420509836695216, 2.7995409793681927, 2.572584953394022, 2.3872011738854484, 2.2621904020910866, 2.1743559300362527, 2.064561832058537, 1.993217049418269, 1.9417341840159785, 1.915892742775582, 1.8565051727466755, 1.8246043899037816, 1.792219555055773, 1.7477907017544583, 1.7171981152113494, 1.710331569920789, 1.684577760395703, 1.6590775992419269, 1.6527411465172295, 1.639912705163698, 1.6103829628712423, 1.5959687512200158, 1.5875339604712821, 1.5763665480656668, 1.569364400597306, 1.553275874069145, 1.538437729483252, 1.5445265447771228, 1.5197701830047745, 1.537395892916499, 1.5308789347743128, 1.5115544162355028, 1.51562935060209, 1.497752840454514, 1.4970423595325366, 1.4960983787570987, 1.4959746577718236, 1.5014309056170352, 1.4936834477089547, 1.4731973291517377, 1.4875923599208798, 1.480178332543588, 1.4888540055300739, 1.4760658837653495, 1.4891684388255213, 1.4689863189920649, 1.4828552340602015, 1.4494392678544328, 1.4749029316343703, 1.4623700262189985, 1.4571652186883461, 1.4720027178257435, 1.4604556313506118, 1.4627225903777388, 1.4606988838127068, 1.4621790346798595, 1.4664110718546688, 1.4583226302722554, 1.4509151701454643, 1.467241140099259, 1.4460743375726648, 1.4489313653997473, 1.4578469100299183, 1.4575567073650189, 1.4638732564341914, 1.4565274253621832, 1.473038852751792, 1.4492297924316682, 1.4636072253321741, 1.4531854412577174, 1.4405741369402087, 1.4449492338541392, 1.4569296579103213, 1.4572457075119019, 1.4474231348381386, 1.4505789612864588, 1.4448883952321232, 1.443796548757467, 1.448319852889121, 1.4510765945589221, 1.4395113614228394, 1.4534764515387046, 1.4445641695916116, 1.4483389682597942, 1.444271283106761, 1.4502746446712598, 1.455318232914349, 1.4490112442154068, 1.4550316516343538, 1.4509404532544248, 1.4472602101059648, 1.4651631116867065, 1.4416131060402673, 1.4374942392916292, 1.43973095459981, 1.447619730287844, 1.4561283556190696, 1.441332137262499, 1.4380891881547533] validation loss is  [2.1796791156133017, 1.9324800198276837, 1.777592810491721, 1.6577403793732326, 1.5794792336722214, 1.5412417563299339, 1.4894084073603153, 1.458369515215357, 1.42152840582033, 1.4108649635066588, 1.4279583518703778, 1.370407626653711, 1.3533829686542351, 1.3413956444710493, 1.3253137730062008, 1.3404380387316148, 1.3246207423508167, 1.2885905876755714, 1.3294462282210588, 1.284246610167126, 1.2973099102576573, 1.298110278012852, 1.2674197008212407, 1.2714613322168589, 1.2603968186303973, 1.2663095863536, 1.2703043439735968, 1.2648991039022803, 1.2603953095773857, 1.2530465939392645, 1.254653758679827, 1.2511145202443004, 1.2524362454811733, 1.2437707285086315, 1.2304627889146407, 1.229597842010359, 1.2379535970588524, 1.2165927970781922, 1.221110325306654, 1.2233348870649934, 1.2180936442067225, 1.2208904940634966, 1.2250230954959989, 1.2085610311478376, 1.213157545775175, 1.2048810590058565, 1.2223052059610684, 1.2122217205663521, 1.2090102847044666, 1.2092820095519226, 1.219062691864868, 1.2058143829926848, 1.2176847215741873, 1.2086850519602497, 1.2159844838703673, 1.2145746409272153, 1.2040407443419099, 1.2153321513906121, 1.2013391104216378, 1.232374853764971, 1.208355367494126, 1.193434185969333, 1.1883507554108899, 1.2059775652984779, 1.2042733874792855, 1.1810057029748957, 1.213911150271694, 1.2016076035797596, 1.1980942239363988, 1.1977437874302268, 1.1886939437439044, 1.1998472595587373, 1.2013472495600581, 1.1919942470267415, 1.2061368285988767, 1.1978345795844991, 1.1928708556418617, 1.2006994641075532, 1.195309237887462, 1.1977670084064205, 1.1902890109146635, 1.1915185920273264, 1.1976511161774397, 1.197371290375789, 1.1870424489801128, 1.194181317773958, 1.1916763670742512, 1.2068782870968182, 1.1867880560457706, 1.2050929544493556, 1.1931238770484924, 1.1862153466790915, 1.1908723978946607, 1.1993969141816099, 1.184392058600982, 1.2017183278997738, 1.1993451944241922, 1.187456293652455, 1.205106866856416]
