/run/nvme/job_2995872/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/scratch/asignal/shanshan/Audio-video-ACL/real_ssl_classification_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 800.37 | loss  7.17 |
| epoch   1 |   200/  475 batches | ms/batch 437.28 | loss  6.76 |
| epoch   1 |   300/  475 batches | ms/batch 316.76 | loss  7.06 |
| epoch   1 |   400/  475 batches | ms/batch 256.44 | loss  6.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 109.69s | training loss  6.84 |
    | end of validation epoch   1 | time: 54.79s | validation loss  5.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.842419893365157] validation loss is  [5.582213714343159]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 90.05 | loss  6.35 |
| epoch   2 |   200/  475 batches | ms/batch 80.65 | loss  6.18 |
| epoch   2 |   300/  475 batches | ms/batch 77.44 | loss  6.32 |
| epoch   2 |   400/  475 batches | ms/batch 75.78 | loss  6.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 35.82s | training loss  6.41 |
    | end of validation epoch   2 | time: 31.11s | validation loss  5.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.842419893365157, 6.409541765514173] validation loss is  [5.582213714343159, 5.311275842810879]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 90.61 | loss  6.33 |
| epoch   3 |   200/  475 batches | ms/batch 80.11 | loss  6.34 |
| epoch   3 |   300/  475 batches | ms/batch 76.50 | loss  6.11 |
| epoch   3 |   400/  475 batches | ms/batch 74.56 | loss  6.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 35.55s | training loss  6.14 |
    | end of validation epoch   3 | time: 31.04s | validation loss  5.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 89.80 | loss  6.39 |
| epoch   4 |   200/  475 batches | ms/batch 79.01 | loss  6.20 |
| epoch   4 |   300/  475 batches | ms/batch 75.84 | loss  5.79 |
| epoch   4 |   400/  475 batches | ms/batch 74.10 | loss  5.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 35.31s | training loss  5.91 |
    | end of validation epoch   4 | time: 31.04s | validation loss  4.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 89.06 | loss  5.46 |
| epoch   5 |   200/  475 batches | ms/batch 79.23 | loss  5.35 |
| epoch   5 |   300/  475 batches | ms/batch 76.33 | loss  5.72 |
| epoch   5 |   400/  475 batches | ms/batch 74.18 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 35.41s | training loss  5.73 |
    | end of validation epoch   5 | time: 31.10s | validation loss  4.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 88.47 | loss  5.46 |
| epoch   6 |   200/  475 batches | ms/batch 78.95 | loss  5.57 |
| epoch   6 |   300/  475 batches | ms/batch 75.67 | loss  5.45 |
| epoch   6 |   400/  475 batches | ms/batch 73.84 | loss  5.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 35.23s | training loss  5.58 |
    | end of validation epoch   6 | time: 31.11s | validation loss  4.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 89.91 | loss  5.62 |
| epoch   7 |   200/  475 batches | ms/batch 79.22 | loss  5.78 |
| epoch   7 |   300/  475 batches | ms/batch 75.58 | loss  5.76 |
| epoch   7 |   400/  475 batches | ms/batch 73.81 | loss  5.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 35.28s | training loss  5.46 |
    | end of validation epoch   7 | time: 30.97s | validation loss  4.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 91.32 | loss  5.52 |
| epoch   8 |   200/  475 batches | ms/batch 80.05 | loss  5.37 |
| epoch   8 |   300/  475 batches | ms/batch 76.49 | loss  4.99 |
| epoch   8 |   400/  475 batches | ms/batch 74.88 | loss  5.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 35.59s | training loss  5.35 |
    | end of validation epoch   8 | time: 31.00s | validation loss  4.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 89.46 | loss  5.19 |
| epoch   9 |   200/  475 batches | ms/batch 79.79 | loss  5.39 |
| epoch   9 |   300/  475 batches | ms/batch 76.47 | loss  5.41 |
| epoch   9 |   400/  475 batches | ms/batch 74.66 | loss  5.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 35.53s | training loss  5.25 |
    | end of validation epoch   9 | time: 31.18s | validation loss  4.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 90.56 | loss  5.56 |
| epoch  10 |   200/  475 batches | ms/batch 79.97 | loss  5.17 |
| epoch  10 |   300/  475 batches | ms/batch 75.87 | loss  5.19 |
| epoch  10 |   400/  475 batches | ms/batch 74.07 | loss  5.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 35.41s | training loss  5.18 |
    | end of validation epoch  10 | time: 31.08s | validation loss  4.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 88.84 | loss  5.36 |
| epoch  11 |   200/  475 batches | ms/batch 79.47 | loss  5.27 |
| epoch  11 |   300/  475 batches | ms/batch 77.69 | loss  4.79 |
| epoch  11 |   400/  475 batches | ms/batch 75.76 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 36.12s | training loss  5.11 |
    | end of validation epoch  11 | time: 30.95s | validation loss  4.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 90.90 | loss  5.11 |
| epoch  12 |   200/  475 batches | ms/batch 80.31 | loss  4.68 |
| epoch  12 |   300/  475 batches | ms/batch 76.32 | loss  5.31 |
| epoch  12 |   400/  475 batches | ms/batch 74.58 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 35.56s | training loss  5.05 |
    | end of validation epoch  12 | time: 30.94s | validation loss  4.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 89.21 | loss  5.14 |
| epoch  13 |   200/  475 batches | ms/batch 79.05 | loss  4.90 |
| epoch  13 |   300/  475 batches | ms/batch 75.49 | loss  4.76 |
| epoch  13 |   400/  475 batches | ms/batch 73.77 | loss  5.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 35.27s | training loss  4.99 |
    | end of validation epoch  13 | time: 30.98s | validation loss  4.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 89.57 | loss  4.78 |
| epoch  14 |   200/  475 batches | ms/batch 78.92 | loss  5.02 |
| epoch  14 |   300/  475 batches | ms/batch 75.61 | loss  4.77 |
| epoch  14 |   400/  475 batches | ms/batch 74.25 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 35.22s | training loss  4.94 |
    | end of validation epoch  14 | time: 30.93s | validation loss  4.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 90.43 | loss  5.06 |
| epoch  15 |   200/  475 batches | ms/batch 79.59 | loss  4.86 |
| epoch  15 |   300/  475 batches | ms/batch 76.06 | loss  4.90 |
| epoch  15 |   400/  475 batches | ms/batch 74.17 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 35.28s | training loss  4.89 |
    | end of validation epoch  15 | time: 30.95s | validation loss  4.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 89.41 | loss  4.80 |
| epoch  16 |   200/  475 batches | ms/batch 78.86 | loss  4.99 |
| epoch  16 |   300/  475 batches | ms/batch 75.61 | loss  4.90 |
| epoch  16 |   400/  475 batches | ms/batch 73.83 | loss  4.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 35.28s | training loss  4.85 |
    | end of validation epoch  16 | time: 30.94s | validation loss  4.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 89.97 | loss  5.07 |
| epoch  17 |   200/  475 batches | ms/batch 79.18 | loss  4.78 |
| epoch  17 |   300/  475 batches | ms/batch 75.86 | loss  4.81 |
| epoch  17 |   400/  475 batches | ms/batch 74.10 | loss  4.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 35.48s | training loss  4.83 |
    | end of validation epoch  17 | time: 31.06s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 89.56 | loss  4.93 |
| epoch  18 |   200/  475 batches | ms/batch 79.30 | loss  4.76 |
| epoch  18 |   300/  475 batches | ms/batch 76.21 | loss  4.74 |
| epoch  18 |   400/  475 batches | ms/batch 74.68 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 35.54s | training loss  4.79 |
    | end of validation epoch  18 | time: 30.99s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 91.47 | loss  4.45 |
| epoch  19 |   200/  475 batches | ms/batch 79.55 | loss  5.14 |
| epoch  19 |   300/  475 batches | ms/batch 75.66 | loss  4.65 |
| epoch  19 |   400/  475 batches | ms/batch 74.09 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 35.28s | training loss  4.76 |
    | end of validation epoch  19 | time: 30.96s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 89.39 | loss  4.70 |
| epoch  20 |   200/  475 batches | ms/batch 79.20 | loss  4.87 |
| epoch  20 |   300/  475 batches | ms/batch 75.47 | loss  4.74 |
| epoch  20 |   400/  475 batches | ms/batch 73.90 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 35.22s | training loss  4.72 |
    | end of validation epoch  20 | time: 31.00s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 89.25 | loss  4.77 |
| epoch  21 |   200/  475 batches | ms/batch 78.77 | loss  4.33 |
| epoch  21 |   300/  475 batches | ms/batch 76.10 | loss  4.66 |
| epoch  21 |   400/  475 batches | ms/batch 74.15 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 35.28s | training loss  4.71 |
    | end of validation epoch  21 | time: 30.98s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 89.04 | loss  4.64 |
| epoch  22 |   200/  475 batches | ms/batch 78.37 | loss  4.80 |
| epoch  22 |   300/  475 batches | ms/batch 75.18 | loss  4.65 |
| epoch  22 |   400/  475 batches | ms/batch 73.71 | loss  4.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 35.17s | training loss  4.69 |
    | end of validation epoch  22 | time: 30.98s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 88.89 | loss  4.95 |
| epoch  23 |   200/  475 batches | ms/batch 78.93 | loss  4.63 |
| epoch  23 |   300/  475 batches | ms/batch 75.71 | loss  4.73 |
| epoch  23 |   400/  475 batches | ms/batch 74.01 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 35.24s | training loss  4.67 |
    | end of validation epoch  23 | time: 31.06s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 90.04 | loss  4.99 |
| epoch  24 |   200/  475 batches | ms/batch 79.55 | loss  4.52 |
| epoch  24 |   300/  475 batches | ms/batch 75.76 | loss  4.74 |
| epoch  24 |   400/  475 batches | ms/batch 74.36 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 35.34s | training loss  4.65 |
    | end of validation epoch  24 | time: 30.98s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 91.36 | loss  4.81 |
| epoch  25 |   200/  475 batches | ms/batch 80.03 | loss  4.51 |
| epoch  25 |   300/  475 batches | ms/batch 76.27 | loss  4.64 |
| epoch  25 |   400/  475 batches | ms/batch 74.24 | loss  5.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 35.40s | training loss  4.63 |
    | end of validation epoch  25 | time: 31.01s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 90.16 | loss  4.66 |
| epoch  26 |   200/  475 batches | ms/batch 79.65 | loss  4.90 |
| epoch  26 |   300/  475 batches | ms/batch 75.78 | loss  4.52 |
| epoch  26 |   400/  475 batches | ms/batch 74.07 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 35.34s | training loss  4.62 |
    | end of validation epoch  26 | time: 30.94s | validation loss  3.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 88.48 | loss  4.59 |
| epoch  27 |   200/  475 batches | ms/batch 78.51 | loss  4.46 |
| epoch  27 |   300/  475 batches | ms/batch 75.06 | loss  4.81 |
| epoch  27 |   400/  475 batches | ms/batch 74.07 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 35.21s | training loss  4.61 |
    | end of validation epoch  27 | time: 30.91s | validation loss  3.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 88.61 | loss  4.29 |
| epoch  28 |   200/  475 batches | ms/batch 79.58 | loss  4.55 |
| epoch  28 |   300/  475 batches | ms/batch 76.49 | loss  4.29 |
| epoch  28 |   400/  475 batches | ms/batch 75.27 | loss  4.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 35.70s | training loss  4.60 |
    | end of validation epoch  28 | time: 30.94s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 88.78 | loss  4.22 |
| epoch  29 |   200/  475 batches | ms/batch 79.75 | loss  4.45 |
| epoch  29 |   300/  475 batches | ms/batch 76.36 | loss  4.74 |
| epoch  29 |   400/  475 batches | ms/batch 74.54 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 35.53s | training loss  4.58 |
    | end of validation epoch  29 | time: 30.95s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 90.63 | loss  4.27 |
| epoch  30 |   200/  475 batches | ms/batch 79.78 | loss  4.63 |
| epoch  30 |   300/  475 batches | ms/batch 76.29 | loss  4.30 |
| epoch  30 |   400/  475 batches | ms/batch 74.07 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 35.29s | training loss  4.57 |
    | end of validation epoch  30 | time: 30.95s | validation loss  3.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 88.70 | loss  4.68 |
| epoch  31 |   200/  475 batches | ms/batch 79.05 | loss  4.48 |
| epoch  31 |   300/  475 batches | ms/batch 75.93 | loss  4.52 |
| epoch  31 |   400/  475 batches | ms/batch 74.06 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 35.27s | training loss  4.56 |
    | end of validation epoch  31 | time: 30.95s | validation loss  3.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 91.48 | loss  4.51 |
| epoch  32 |   200/  475 batches | ms/batch 80.09 | loss  4.44 |
| epoch  32 |   300/  475 batches | ms/batch 75.96 | loss  4.81 |
| epoch  32 |   400/  475 batches | ms/batch 74.53 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 35.38s | training loss  4.55 |
    | end of validation epoch  32 | time: 34.03s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 90.07 | loss  4.36 |
| epoch  33 |   200/  475 batches | ms/batch 79.54 | loss  4.46 |
| epoch  33 |   300/  475 batches | ms/batch 76.03 | loss  4.71 |
| epoch  33 |   400/  475 batches | ms/batch 74.37 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 35.19s | training loss  4.54 |
    | end of validation epoch  33 | time: 31.05s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 90.00 | loss  4.64 |
| epoch  34 |   200/  475 batches | ms/batch 79.42 | loss  4.27 |
| epoch  34 |   300/  475 batches | ms/batch 75.54 | loss  4.72 |
| epoch  34 |   400/  475 batches | ms/batch 74.12 | loss  4.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 35.50s | training loss  4.53 |
    | end of validation epoch  34 | time: 31.15s | validation loss  3.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 92.12 | loss  4.34 |
| epoch  35 |   200/  475 batches | ms/batch 79.87 | loss  4.55 |
| epoch  35 |   300/  475 batches | ms/batch 76.10 | loss  4.69 |
| epoch  35 |   400/  475 batches | ms/batch 74.11 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 35.37s | training loss  4.53 |
    | end of validation epoch  35 | time: 31.07s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 90.80 | loss  4.60 |
| epoch  36 |   200/  475 batches | ms/batch 80.47 | loss  4.24 |
| epoch  36 |   300/  475 batches | ms/batch 76.74 | loss  4.74 |
| epoch  36 |   400/  475 batches | ms/batch 74.59 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 35.52s | training loss  4.52 |
    | end of validation epoch  36 | time: 31.10s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 89.09 | loss  4.72 |
| epoch  37 |   200/  475 batches | ms/batch 79.13 | loss  4.46 |
| epoch  37 |   300/  475 batches | ms/batch 75.95 | loss  4.64 |
| epoch  37 |   400/  475 batches | ms/batch 74.15 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 35.26s | training loss  4.51 |
    | end of validation epoch  37 | time: 31.05s | validation loss  3.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 90.17 | loss  4.53 |
| epoch  38 |   200/  475 batches | ms/batch 78.90 | loss  4.66 |
| epoch  38 |   300/  475 batches | ms/batch 75.89 | loss  4.53 |
| epoch  38 |   400/  475 batches | ms/batch 74.18 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 35.42s | training loss  4.52 |
    | end of validation epoch  38 | time: 31.02s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 89.49 | loss  4.59 |
| epoch  39 |   200/  475 batches | ms/batch 79.06 | loss  4.40 |
| epoch  39 |   300/  475 batches | ms/batch 75.75 | loss  4.29 |
| epoch  39 |   400/  475 batches | ms/batch 74.18 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 35.30s | training loss  4.50 |
    | end of validation epoch  39 | time: 31.07s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 90.64 | loss  4.58 |
| epoch  40 |   200/  475 batches | ms/batch 79.96 | loss  4.77 |
| epoch  40 |   300/  475 batches | ms/batch 76.23 | loss  4.46 |
| epoch  40 |   400/  475 batches | ms/batch 74.42 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 35.55s | training loss  4.50 |
    | end of validation epoch  40 | time: 31.17s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 90.09 | loss  4.46 |
| epoch  41 |   200/  475 batches | ms/batch 79.33 | loss  4.61 |
| epoch  41 |   300/  475 batches | ms/batch 76.10 | loss  4.57 |
| epoch  41 |   400/  475 batches | ms/batch 74.30 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 35.61s | training loss  4.50 |
    | end of validation epoch  41 | time: 31.13s | validation loss  3.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645]
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 89.57 | loss  4.33 |
| epoch  42 |   200/  475 batches | ms/batch 79.67 | loss  4.69 |
| epoch  42 |   300/  475 batches | ms/batch 76.88 | loss  4.35 |
| epoch  42 |   400/  475 batches | ms/batch 74.76 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 35.52s | training loss  4.49 |
    | end of validation epoch  42 | time: 31.13s | validation loss  3.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 90.13 | loss  4.35 |
| epoch  43 |   200/  475 batches | ms/batch 78.90 | loss  4.50 |
| epoch  43 |   300/  475 batches | ms/batch 75.44 | loss  4.11 |
| epoch  43 |   400/  475 batches | ms/batch 73.85 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 35.15s | training loss  4.50 |
    | end of validation epoch  43 | time: 31.05s | validation loss  3.86 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099]
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 88.70 | loss  4.30 |
| epoch  44 |   200/  475 batches | ms/batch 79.61 | loss  4.38 |
| epoch  44 |   300/  475 batches | ms/batch 76.31 | loss  4.53 |
| epoch  44 |   400/  475 batches | ms/batch 74.53 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 35.44s | training loss  4.48 |
    | end of validation epoch  44 | time: 31.18s | validation loss  3.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 89.52 | loss  4.27 |
| epoch  45 |   200/  475 batches | ms/batch 79.01 | loss  4.30 |
| epoch  45 |   300/  475 batches | ms/batch 75.76 | loss  4.49 |
| epoch  45 |   400/  475 batches | ms/batch 73.60 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 35.17s | training loss  4.49 |
    | end of validation epoch  45 | time: 31.11s | validation loss  3.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043]
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 89.33 | loss  4.76 |
| epoch  46 |   200/  475 batches | ms/batch 78.66 | loss  4.80 |
| epoch  46 |   300/  475 batches | ms/batch 75.64 | loss  4.59 |
| epoch  46 |   400/  475 batches | ms/batch 73.82 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 35.26s | training loss  4.48 |
    | end of validation epoch  46 | time: 31.03s | validation loss  3.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 89.00 | loss  4.63 |
| epoch  47 |   200/  475 batches | ms/batch 79.04 | loss  4.68 |
| epoch  47 |   300/  475 batches | ms/batch 75.68 | loss  4.47 |
| epoch  47 |   400/  475 batches | ms/batch 73.92 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 35.21s | training loss  4.48 |
    | end of validation epoch  47 | time: 31.05s | validation loss  3.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711]
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 89.81 | loss  4.77 |
| epoch  48 |   200/  475 batches | ms/batch 79.50 | loss  4.47 |
| epoch  48 |   300/  475 batches | ms/batch 75.61 | loss  4.50 |
| epoch  48 |   400/  475 batches | ms/batch 73.80 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 35.18s | training loss  4.47 |
    | end of validation epoch  48 | time: 31.19s | validation loss  3.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 90.65 | loss  4.26 |
| epoch  49 |   200/  475 batches | ms/batch 80.04 | loss  4.65 |
| epoch  49 |   300/  475 batches | ms/batch 76.13 | loss  4.40 |
| epoch  49 |   400/  475 batches | ms/batch 74.20 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 35.47s | training loss  4.47 |
    | end of validation epoch  49 | time: 31.05s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 88.93 | loss  4.43 |
| epoch  50 |   200/  475 batches | ms/batch 80.01 | loss  4.30 |
| epoch  50 |   300/  475 batches | ms/batch 76.12 | loss  4.77 |
| epoch  50 |   400/  475 batches | ms/batch 74.64 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 35.59s | training loss  4.48 |
    | end of validation epoch  50 | time: 31.11s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 89.13 | loss  4.36 |
| epoch  51 |   200/  475 batches | ms/batch 79.30 | loss  4.22 |
| epoch  51 |   300/  475 batches | ms/batch 76.23 | loss  4.02 |
| epoch  51 |   400/  475 batches | ms/batch 74.23 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 35.40s | training loss  4.48 |
    | end of validation epoch  51 | time: 31.20s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677]
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 89.25 | loss  4.61 |
| epoch  52 |   200/  475 batches | ms/batch 79.19 | loss  4.51 |
| epoch  52 |   300/  475 batches | ms/batch 76.25 | loss  4.82 |
| epoch  52 |   400/  475 batches | ms/batch 74.39 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 35.59s | training loss  4.49 |
    | end of validation epoch  52 | time: 31.06s | validation loss  3.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622]
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 91.13 | loss  4.88 |
| epoch  53 |   200/  475 batches | ms/batch 79.39 | loss  4.27 |
| epoch  53 |   300/  475 batches | ms/batch 76.14 | loss  4.11 |
| epoch  53 |   400/  475 batches | ms/batch 73.97 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 35.28s | training loss  4.46 |
    | end of validation epoch  53 | time: 31.05s | validation loss  3.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 89.79 | loss  4.05 |
| epoch  54 |   200/  475 batches | ms/batch 79.41 | loss  4.44 |
| epoch  54 |   300/  475 batches | ms/batch 75.62 | loss  4.63 |
| epoch  54 |   400/  475 batches | ms/batch 74.02 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 35.35s | training loss  4.47 |
    | end of validation epoch  54 | time: 31.09s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 89.09 | loss  4.71 |
| epoch  55 |   200/  475 batches | ms/batch 79.70 | loss  4.47 |
| epoch  55 |   300/  475 batches | ms/batch 76.01 | loss  4.58 |
| epoch  55 |   400/  475 batches | ms/batch 74.23 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 35.48s | training loss  4.46 |
    | end of validation epoch  55 | time: 31.07s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 90.47 | loss  4.63 |
| epoch  56 |   200/  475 batches | ms/batch 79.60 | loss  4.98 |
| epoch  56 |   300/  475 batches | ms/batch 76.14 | loss  4.94 |
| epoch  56 |   400/  475 batches | ms/batch 74.21 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 35.39s | training loss  4.48 |
    | end of validation epoch  56 | time: 31.08s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507]
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 88.89 | loss  4.44 |
| epoch  57 |   200/  475 batches | ms/batch 78.92 | loss  4.37 |
| epoch  57 |   300/  475 batches | ms/batch 75.69 | loss  4.63 |
| epoch  57 |   400/  475 batches | ms/batch 74.09 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 35.35s | training loss  4.46 |
    | end of validation epoch  57 | time: 31.04s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 238.28 | loss  4.37 |
| epoch  58 |   200/  475 batches | ms/batch 153.11 | loss  4.52 |
| epoch  58 |   300/  475 batches | ms/batch 125.28 | loss  4.38 |
| epoch  58 |   400/  475 batches | ms/batch 111.06 | loss  4.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 50.10s | training loss  4.48 |
    | end of validation epoch  58 | time: 31.33s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 91.05 | loss  4.40 |
| epoch  59 |   200/  475 batches | ms/batch 78.90 | loss  4.50 |
| epoch  59 |   300/  475 batches | ms/batch 75.86 | loss  4.82 |
| epoch  59 |   400/  475 batches | ms/batch 74.04 | loss  5.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 35.30s | training loss  4.46 |
    | end of validation epoch  59 | time: 31.15s | validation loss  3.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323]
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 90.21 | loss  4.40 |
| epoch  60 |   200/  475 batches | ms/batch 79.65 | loss  4.30 |
| epoch  60 |   300/  475 batches | ms/batch 76.12 | loss  4.30 |
| epoch  60 |   400/  475 batches | ms/batch 74.29 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 35.43s | training loss  4.46 |
    | end of validation epoch  60 | time: 31.11s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 90.81 | loss  4.73 |
| epoch  61 |   200/  475 batches | ms/batch 80.35 | loss  4.73 |
| epoch  61 |   300/  475 batches | ms/batch 77.10 | loss  4.03 |
| epoch  61 |   400/  475 batches | ms/batch 75.37 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 35.75s | training loss  4.45 |
    | end of validation epoch  61 | time: 31.08s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 89.31 | loss  4.44 |
| epoch  62 |   200/  475 batches | ms/batch 79.09 | loss  4.40 |
| epoch  62 |   300/  475 batches | ms/batch 75.77 | loss  4.26 |
| epoch  62 |   400/  475 batches | ms/batch 74.04 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 35.28s | training loss  4.47 |
    | end of validation epoch  62 | time: 31.11s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 90.25 | loss  3.91 |
| epoch  63 |   200/  475 batches | ms/batch 79.23 | loss  4.46 |
| epoch  63 |   300/  475 batches | ms/batch 76.55 | loss  4.64 |
| epoch  63 |   400/  475 batches | ms/batch 75.00 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 35.75s | training loss  4.46 |
    | end of validation epoch  63 | time: 31.19s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 90.02 | loss  4.39 |
| epoch  64 |   200/  475 batches | ms/batch 79.77 | loss  4.77 |
| epoch  64 |   300/  475 batches | ms/batch 75.67 | loss  4.42 |
| epoch  64 |   400/  475 batches | ms/batch 74.91 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 35.23s | training loss  4.47 |
    | end of validation epoch  64 | time: 31.05s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257]
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 89.34 | loss  4.26 |
| epoch  65 |   200/  475 batches | ms/batch 79.62 | loss  4.80 |
| epoch  65 |   300/  475 batches | ms/batch 76.40 | loss  4.29 |
| epoch  65 |   400/  475 batches | ms/batch 74.59 | loss  4.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 35.44s | training loss  4.47 |
    | end of validation epoch  65 | time: 31.20s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 89.84 | loss  4.26 |
| epoch  66 |   200/  475 batches | ms/batch 79.99 | loss  4.36 |
| epoch  66 |   300/  475 batches | ms/batch 76.34 | loss  4.26 |
| epoch  66 |   400/  475 batches | ms/batch 74.24 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 35.28s | training loss  4.45 |
    | end of validation epoch  66 | time: 31.13s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 88.80 | loss  4.34 |
| epoch  67 |   200/  475 batches | ms/batch 79.18 | loss  3.99 |
| epoch  67 |   300/  475 batches | ms/batch 75.89 | loss  4.43 |
| epoch  67 |   400/  475 batches | ms/batch 74.19 | loss  4.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 35.39s | training loss  4.46 |
    | end of validation epoch  67 | time: 31.01s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 89.85 | loss  4.62 |
| epoch  68 |   200/  475 batches | ms/batch 78.90 | loss  4.40 |
| epoch  68 |   300/  475 batches | ms/batch 75.32 | loss  4.28 |
| epoch  68 |   400/  475 batches | ms/batch 73.85 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 35.23s | training loss  4.45 |
    | end of validation epoch  68 | time: 31.19s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 89.25 | loss  4.57 |
| epoch  69 |   200/  475 batches | ms/batch 79.18 | loss  4.28 |
| epoch  69 |   300/  475 batches | ms/batch 76.01 | loss  4.31 |
| epoch  69 |   400/  475 batches | ms/batch 74.08 | loss  4.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 35.18s | training loss  4.47 |
    | end of validation epoch  69 | time: 31.14s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 89.37 | loss  4.55 |
| epoch  70 |   200/  475 batches | ms/batch 79.20 | loss  4.27 |
| epoch  70 |   300/  475 batches | ms/batch 75.44 | loss  4.20 |
| epoch  70 |   400/  475 batches | ms/batch 73.93 | loss  4.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 35.22s | training loss  4.46 |
    | end of validation epoch  70 | time: 31.10s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724]
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 88.47 | loss  4.70 |
| epoch  71 |   200/  475 batches | ms/batch 79.45 | loss  4.30 |
| epoch  71 |   300/  475 batches | ms/batch 76.39 | loss  4.58 |
| epoch  71 |   400/  475 batches | ms/batch 74.86 | loss  4.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 35.64s | training loss  4.45 |
    | end of validation epoch  71 | time: 31.14s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 90.88 | loss  4.48 |
| epoch  72 |   200/  475 batches | ms/batch 80.17 | loss  4.56 |
| epoch  72 |   300/  475 batches | ms/batch 75.87 | loss  4.66 |
| epoch  72 |   400/  475 batches | ms/batch 74.09 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 35.25s | training loss  4.45 |
    | end of validation epoch  72 | time: 31.13s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 90.37 | loss  4.34 |
| epoch  73 |   200/  475 batches | ms/batch 79.28 | loss  4.13 |
| epoch  73 |   300/  475 batches | ms/batch 75.94 | loss  4.24 |
| epoch  73 |   400/  475 batches | ms/batch 74.15 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 35.43s | training loss  4.46 |
    | end of validation epoch  73 | time: 31.07s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 91.73 | loss  4.31 |
| epoch  74 |   200/  475 batches | ms/batch 80.24 | loss  4.45 |
| epoch  74 |   300/  475 batches | ms/batch 76.09 | loss  4.25 |
| epoch  74 |   400/  475 batches | ms/batch 74.27 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 35.52s | training loss  4.46 |
    | end of validation epoch  74 | time: 31.26s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 88.91 | loss  4.45 |
| epoch  75 |   200/  475 batches | ms/batch 79.73 | loss  4.25 |
| epoch  75 |   300/  475 batches | ms/batch 76.12 | loss  4.47 |
| epoch  75 |   400/  475 batches | ms/batch 74.07 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 35.16s | training loss  4.47 |
    | end of validation epoch  75 | time: 31.09s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 89.59 | loss  4.05 |
| epoch  76 |   200/  475 batches | ms/batch 79.43 | loss  4.28 |
| epoch  76 |   300/  475 batches | ms/batch 76.47 | loss  4.50 |
| epoch  76 |   400/  475 batches | ms/batch 74.16 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 35.43s | training loss  4.44 |
    | end of validation epoch  76 | time: 31.01s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 89.83 | loss  4.46 |
| epoch  77 |   200/  475 batches | ms/batch 79.97 | loss  4.54 |
| epoch  77 |   300/  475 batches | ms/batch 76.45 | loss  4.26 |
| epoch  77 |   400/  475 batches | ms/batch 74.32 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 35.29s | training loss  4.46 |
    | end of validation epoch  77 | time: 31.13s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 89.68 | loss  4.26 |
| epoch  78 |   200/  475 batches | ms/batch 79.00 | loss  4.39 |
| epoch  78 |   300/  475 batches | ms/batch 76.05 | loss  4.67 |
| epoch  78 |   400/  475 batches | ms/batch 74.25 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 35.32s | training loss  4.45 |
    | end of validation epoch  78 | time: 31.04s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 89.28 | loss  4.62 |
| epoch  79 |   200/  475 batches | ms/batch 79.26 | loss  4.48 |
| epoch  79 |   300/  475 batches | ms/batch 75.56 | loss  4.40 |
| epoch  79 |   400/  475 batches | ms/batch 74.02 | loss  4.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 35.46s | training loss  4.45 |
    | end of validation epoch  79 | time: 31.13s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 92.11 | loss  4.39 |
| epoch  80 |   200/  475 batches | ms/batch 80.24 | loss  4.53 |
| epoch  80 |   300/  475 batches | ms/batch 76.64 | loss  4.57 |
| epoch  80 |   400/  475 batches | ms/batch 75.04 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 35.70s | training loss  4.45 |
    | end of validation epoch  80 | time: 31.21s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 90.19 | loss  4.50 |
| epoch  81 |   200/  475 batches | ms/batch 79.90 | loss  4.19 |
| epoch  81 |   300/  475 batches | ms/batch 75.65 | loss  3.99 |
| epoch  81 |   400/  475 batches | ms/batch 74.43 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 35.49s | training loss  4.45 |
    | end of validation epoch  81 | time: 31.26s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 91.11 | loss  4.80 |
| epoch  82 |   200/  475 batches | ms/batch 80.45 | loss  4.43 |
| epoch  82 |   300/  475 batches | ms/batch 76.42 | loss  4.74 |
| epoch  82 |   400/  475 batches | ms/batch 74.25 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 35.47s | training loss  4.45 |
    | end of validation epoch  82 | time: 31.20s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 93.47 | loss  4.66 |
| epoch  83 |   200/  475 batches | ms/batch 81.28 | loss  3.86 |
| epoch  83 |   300/  475 batches | ms/batch 76.83 | loss  4.35 |
| epoch  83 |   400/  475 batches | ms/batch 74.77 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 35.64s | training loss  4.45 |
    | end of validation epoch  83 | time: 30.96s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 90.89 | loss  4.34 |
| epoch  84 |   200/  475 batches | ms/batch 80.24 | loss  4.36 |
| epoch  84 |   300/  475 batches | ms/batch 76.06 | loss  4.55 |
| epoch  84 |   400/  475 batches | ms/batch 74.63 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 35.52s | training loss  4.46 |
    | end of validation epoch  84 | time: 30.99s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 90.70 | loss  4.49 |
| epoch  85 |   200/  475 batches | ms/batch 79.73 | loss  4.56 |
| epoch  85 |   300/  475 batches | ms/batch 75.74 | loss  4.29 |
| epoch  85 |   400/  475 batches | ms/batch 73.94 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 35.36s | training loss  4.45 |
    | end of validation epoch  85 | time: 30.99s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 90.48 | loss  4.58 |
| epoch  86 |   200/  475 batches | ms/batch 79.71 | loss  4.73 |
| epoch  86 |   300/  475 batches | ms/batch 76.25 | loss  4.44 |
| epoch  86 |   400/  475 batches | ms/batch 74.38 | loss  4.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 35.40s | training loss  4.46 |
    | end of validation epoch  86 | time: 30.96s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 91.32 | loss  4.65 |
| epoch  87 |   200/  475 batches | ms/batch 80.08 | loss  4.25 |
| epoch  87 |   300/  475 batches | ms/batch 76.23 | loss  4.22 |
| epoch  87 |   400/  475 batches | ms/batch 74.31 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 35.41s | training loss  4.44 |
    | end of validation epoch  87 | time: 30.98s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 91.41 | loss  4.22 |
| epoch  88 |   200/  475 batches | ms/batch 80.17 | loss  4.32 |
| epoch  88 |   300/  475 batches | ms/batch 76.88 | loss  4.35 |
| epoch  88 |   400/  475 batches | ms/batch 74.57 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 35.57s | training loss  4.45 |
    | end of validation epoch  88 | time: 31.02s | validation loss  3.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 89.53 | loss  4.17 |
| epoch  89 |   200/  475 batches | ms/batch 80.13 | loss  4.62 |
| epoch  89 |   300/  475 batches | ms/batch 76.11 | loss  4.39 |
| epoch  89 |   400/  475 batches | ms/batch 74.44 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 35.51s | training loss  4.46 |
    | end of validation epoch  89 | time: 31.07s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 92.12 | loss  4.24 |
| epoch  90 |   200/  475 batches | ms/batch 80.69 | loss  4.32 |
| epoch  90 |   300/  475 batches | ms/batch 76.33 | loss  4.04 |
| epoch  90 |   400/  475 batches | ms/batch 74.68 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 35.62s | training loss  4.45 |
    | end of validation epoch  90 | time: 30.97s | validation loss  3.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 89.89 | loss  4.43 |
| epoch  91 |   200/  475 batches | ms/batch 79.64 | loss  4.51 |
| epoch  91 |   300/  475 batches | ms/batch 75.97 | loss  4.36 |
| epoch  91 |   400/  475 batches | ms/batch 74.10 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 35.20s | training loss  4.45 |
    | end of validation epoch  91 | time: 30.99s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 88.89 | loss  4.49 |
| epoch  92 |   200/  475 batches | ms/batch 79.61 | loss  4.79 |
| epoch  92 |   300/  475 batches | ms/batch 75.86 | loss  4.17 |
| epoch  92 |   400/  475 batches | ms/batch 74.25 | loss  4.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 35.35s | training loss  4.45 |
    | end of validation epoch  92 | time: 30.92s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 89.49 | loss  4.34 |
| epoch  93 |   200/  475 batches | ms/batch 79.66 | loss  4.46 |
| epoch  93 |   300/  475 batches | ms/batch 76.32 | loss  4.40 |
| epoch  93 |   400/  475 batches | ms/batch 74.66 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 35.57s | training loss  4.44 |
    | end of validation epoch  93 | time: 31.11s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 89.71 | loss  4.24 |
| epoch  94 |   200/  475 batches | ms/batch 80.07 | loss  4.33 |
| epoch  94 |   300/  475 batches | ms/batch 76.00 | loss  4.36 |
| epoch  94 |   400/  475 batches | ms/batch 74.12 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 35.37s | training loss  4.46 |
    | end of validation epoch  94 | time: 31.08s | validation loss  3.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 89.44 | loss  4.78 |
| epoch  95 |   200/  475 batches | ms/batch 79.00 | loss  4.87 |
| epoch  95 |   300/  475 batches | ms/batch 76.20 | loss  4.51 |
| epoch  95 |   400/  475 batches | ms/batch 74.02 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 35.32s | training loss  4.45 |
    | end of validation epoch  95 | time: 30.90s | validation loss  3.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919, 4.453087870949193] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035, 3.782703844439082]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 89.86 | loss  4.30 |
| epoch  96 |   200/  475 batches | ms/batch 79.31 | loss  4.35 |
| epoch  96 |   300/  475 batches | ms/batch 76.19 | loss  4.39 |
| epoch  96 |   400/  475 batches | ms/batch 74.43 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 35.55s | training loss  4.44 |
    | end of validation epoch  96 | time: 30.94s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919, 4.453087870949193, 4.44485657742149] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035, 3.782703844439082, 3.788116200631406]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 91.14 | loss  4.44 |
| epoch  97 |   200/  475 batches | ms/batch 80.34 | loss  4.35 |
| epoch  97 |   300/  475 batches | ms/batch 76.69 | loss  4.79 |
| epoch  97 |   400/  475 batches | ms/batch 74.43 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 35.43s | training loss  4.46 |
    | end of validation epoch  97 | time: 31.12s | validation loss  3.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919, 4.453087870949193, 4.44485657742149, 4.455732314461156] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035, 3.782703844439082, 3.788116200631406, 3.779247610508895]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 89.35 | loss  4.28 |
| epoch  98 |   200/  475 batches | ms/batch 79.16 | loss  4.46 |
| epoch  98 |   300/  475 batches | ms/batch 76.12 | loss  4.48 |
| epoch  98 |   400/  475 batches | ms/batch 73.88 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 35.33s | training loss  4.44 |
    | end of validation epoch  98 | time: 30.97s | validation loss  3.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919, 4.453087870949193, 4.44485657742149, 4.455732314461156, 4.442206135298076] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035, 3.782703844439082, 3.788116200631406, 3.779247610508895, 3.79192852973938]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 88.95 | loss  4.69 |
| epoch  99 |   200/  475 batches | ms/batch 79.00 | loss  4.37 |
| epoch  99 |   300/  475 batches | ms/batch 75.71 | loss  4.48 |
| epoch  99 |   400/  475 batches | ms/batch 73.95 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 35.30s | training loss  4.44 |
    | end of validation epoch  99 | time: 30.98s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.842419893365157, 6.409541765514173, 6.140292746895238, 5.910959519838032, 5.728600903561241, 5.580802858252274, 5.455017095866956, 5.35382462250559, 5.2541392165736145, 5.183492788013659, 5.1065905500713145, 5.049035796115273, 4.993207438619513, 4.938633360611765, 4.887415362910221, 4.846933643943385, 4.82570486771433, 4.787853665602834, 4.764622859954834, 4.7213459466633045, 4.708085443597091, 4.693031527368646, 4.668107623050087, 4.654057269849275, 4.632371146553441, 4.621080786052503, 4.60949966029117, 4.601381015777588, 4.578999969582808, 4.56573273207012, 4.556884353035374, 4.552340287660297, 4.537551357871608, 4.533088550065693, 4.5327324395430715, 4.516756685658505, 4.5147591068870145, 4.519514372474269, 4.499346789811787, 4.502522696946796, 4.502093034543489, 4.492654791882163, 4.495174978657773, 4.483673970071893, 4.491230944081357, 4.482881644901476, 4.4848870352694865, 4.473039302323994, 4.468184705031546, 4.476744917819374, 4.477683861883063, 4.486450705277292, 4.464217207557277, 4.465287284349141, 4.46037423384817, 4.475143736287167, 4.459796586287649, 4.475452416068629, 4.463125725796348, 4.458061263937699, 4.449896983598408, 4.467978534698486, 4.45725218622308, 4.470288200378418, 4.472697517495407, 4.45385469637419, 4.455147134379336, 4.45464998345626, 4.466750990717035, 4.460246670873541, 4.452811348563746, 4.448761512856734, 4.455947766554983, 4.45734399243405, 4.469618389731959, 4.438570848264193, 4.45948921304, 4.447278976942363, 4.453556877437391, 4.4547343078412505, 4.449605224508988, 4.454640309183221, 4.45048515420211, 4.463455500351755, 4.447108058427509, 4.460067925202219, 4.444140092448184, 4.449050518838983, 4.459023535879035, 4.446448201631245, 4.450310050562808, 4.449269166243703, 4.439826179303621, 4.457353114077919, 4.453087870949193, 4.44485657742149, 4.455732314461156, 4.442206135298076, 4.441518024645354] validation loss is  [5.582213714343159, 5.311275842810879, 5.101811240701115, 4.939343135897853, 4.812368653401607, 4.678381282742284, 4.587977469468317, 4.513315713706136, 4.449211613470767, 4.409720448886647, 4.3601537512130095, 4.292892588286841, 4.2611046618774155, 4.2148723442013525, 4.1891804142158575, 4.1634010507279084, 4.121668060286706, 4.099954611113091, 4.080055340999315, 4.057182618549892, 4.054255072810069, 4.018601549773657, 4.026960412995154, 4.001836788754503, 3.976801307261491, 3.969388148363899, 3.97277497644184, 3.953018066261997, 3.946575391192396, 3.937987792391737, 3.9290513291078457, 3.917142222909366, 3.9198977806988884, 3.9160675080884406, 3.8896483854085457, 3.891698656963701, 3.8896196349328305, 3.8810110853499724, 3.879659514467255, 3.857851789779022, 3.882245454467645, 3.872381404668343, 3.861325384188099, 3.840092302370472, 3.8540118421827043, 3.8408661349480893, 3.844286812453711, 3.8321751205861068, 3.8222673019441236, 3.815230634032177, 3.8230802872601677, 3.8309887457294622, 3.8374199065841545, 3.8219564682295344, 3.820038522992815, 3.8156346613619507, 3.8188960291758307, 3.8028471610125374, 3.8269155386115323, 3.8148244288789126, 3.8165495155238305, 3.817169261579754, 3.816540491681139, 3.8086284188663257, 3.8081977387436297, 3.804603446431521, 3.7936912684881388, 3.8101069286089984, 3.800488351773815, 3.7945893291665724, 3.798074315575992, 3.8146300756630778, 3.8023480868139186, 3.8031628392323724, 3.803545052263917, 3.7905430753691856, 3.78846951693046, 3.8195501014965925, 3.7921193868172267, 3.8089607863866983, 3.7948712962014333, 3.8036635983891847, 3.799352908334812, 3.794278591620822, 3.7907675715053784, 3.7939672189600326, 3.802520525555651, 3.7848552315175032, 3.8121345524026564, 3.783885981856274, 3.7935649827748787, 3.7980465909012224, 3.791890681290827, 3.8245487473592035, 3.782703844439082, 3.788116200631406, 3.779247610508895, 3.79192852973938, 3.7964342201457306]
