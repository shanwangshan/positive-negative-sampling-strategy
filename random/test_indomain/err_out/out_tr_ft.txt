/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 137.95 | loss  5.76 |
| epoch   1 |   200/  475 batches | ms/batch 126.32 | loss  5.72 |
| epoch   1 |   300/  475 batches | ms/batch 123.08 | loss  5.43 |
| epoch   1 |   400/  475 batches | ms/batch 121.13 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 57.31s | training loss  5.76 |
    | end of validation epoch   1 | time: 49.78s | validation loss  5.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.76120064785606] validation loss is  [5.198945610463118]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 137.49 | loss  5.34 |
| epoch   2 |   200/  475 batches | ms/batch 125.04 | loss  5.55 |
| epoch   2 |   300/  475 batches | ms/batch 120.47 | loss  5.53 |
| epoch   2 |   400/  475 batches | ms/batch 118.57 | loss  5.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 56.14s | training loss  5.26 |
    | end of validation epoch   2 | time: 49.76s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.76120064785606, 5.2628436449954386] validation loss is  [5.198945610463118, 4.698070798601423]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 142.17 | loss  5.24 |
| epoch   3 |   200/  475 batches | ms/batch 129.62 | loss  5.09 |
| epoch   3 |   300/  475 batches | ms/batch 125.72 | loss  4.89 |
| epoch   3 |   400/  475 batches | ms/batch 124.21 | loss  5.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 58.58s | training loss  4.98 |
    | end of validation epoch   3 | time: 48.25s | validation loss  4.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 143.79 | loss  4.67 |
| epoch   4 |   200/  475 batches | ms/batch 131.00 | loss  4.92 |
| epoch   4 |   300/  475 batches | ms/batch 126.14 | loss  4.85 |
| epoch   4 |   400/  475 batches | ms/batch 122.53 | loss  4.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 57.86s | training loss  4.75 |
    | end of validation epoch   4 | time: 49.43s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 142.49 | loss  4.54 |
| epoch   5 |   200/  475 batches | ms/batch 128.26 | loss  4.84 |
| epoch   5 |   300/  475 batches | ms/batch 123.84 | loss  4.10 |
| epoch   5 |   400/  475 batches | ms/batch 121.57 | loss  4.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 57.35s | training loss  4.60 |
    | end of validation epoch   5 | time: 48.54s | validation loss  3.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 142.16 | loss  4.75 |
| epoch   6 |   200/  475 batches | ms/batch 128.28 | loss  4.61 |
| epoch   6 |   300/  475 batches | ms/batch 124.89 | loss  4.81 |
| epoch   6 |   400/  475 batches | ms/batch 122.22 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 57.67s | training loss  4.48 |
    | end of validation epoch   6 | time: 47.99s | validation loss  3.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 139.08 | loss  4.23 |
| epoch   7 |   200/  475 batches | ms/batch 129.70 | loss  4.47 |
| epoch   7 |   300/  475 batches | ms/batch 124.77 | loss  4.58 |
| epoch   7 |   400/  475 batches | ms/batch 122.15 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 57.72s | training loss  4.37 |
    | end of validation epoch   7 | time: 48.50s | validation loss  3.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 144.66 | loss  4.37 |
| epoch   8 |   200/  475 batches | ms/batch 129.25 | loss  4.20 |
| epoch   8 |   300/  475 batches | ms/batch 124.55 | loss  4.11 |
| epoch   8 |   400/  475 batches | ms/batch 122.54 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 57.86s | training loss  4.30 |
    | end of validation epoch   8 | time: 48.78s | validation loss  3.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 140.84 | loss  3.92 |
| epoch   9 |   200/  475 batches | ms/batch 127.95 | loss  4.11 |
| epoch   9 |   300/  475 batches | ms/batch 123.84 | loss  4.57 |
| epoch   9 |   400/  475 batches | ms/batch 121.66 | loss  4.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 57.55s | training loss  4.22 |
    | end of validation epoch   9 | time: 49.57s | validation loss  3.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 149.40 | loss  4.25 |
| epoch  10 |   200/  475 batches | ms/batch 133.37 | loss  4.18 |
| epoch  10 |   300/  475 batches | ms/batch 127.49 | loss  4.25 |
| epoch  10 |   400/  475 batches | ms/batch 124.79 | loss  4.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 58.59s | training loss  4.15 |
    | end of validation epoch  10 | time: 47.67s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 145.34 | loss  4.22 |
| epoch  11 |   200/  475 batches | ms/batch 131.14 | loss  3.97 |
| epoch  11 |   300/  475 batches | ms/batch 125.99 | loss  3.94 |
| epoch  11 |   400/  475 batches | ms/batch 122.83 | loss  3.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 57.94s | training loss  4.10 |
    | end of validation epoch  11 | time: 50.96s | validation loss  3.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 138.09 | loss  3.89 |
| epoch  12 |   200/  475 batches | ms/batch 126.12 | loss  4.37 |
| epoch  12 |   300/  475 batches | ms/batch 122.41 | loss  4.02 |
| epoch  12 |   400/  475 batches | ms/batch 122.47 | loss  3.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 57.52s | training loss  4.03 |
    | end of validation epoch  12 | time: 50.34s | validation loss  3.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 137.39 | loss  4.36 |
| epoch  13 |   200/  475 batches | ms/batch 125.48 | loss  3.75 |
| epoch  13 |   300/  475 batches | ms/batch 122.33 | loss  3.76 |
| epoch  13 |   400/  475 batches | ms/batch 119.84 | loss  3.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 56.69s | training loss  4.00 |
    | end of validation epoch  13 | time: 47.61s | validation loss  3.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 142.12 | loss  4.48 |
| epoch  14 |   200/  475 batches | ms/batch 128.11 | loss  3.98 |
| epoch  14 |   300/  475 batches | ms/batch 124.55 | loss  3.84 |
| epoch  14 |   400/  475 batches | ms/batch 121.10 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 57.03s | training loss  3.94 |
    | end of validation epoch  14 | time: 47.82s | validation loss  3.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 139.12 | loss  4.55 |
| epoch  15 |   200/  475 batches | ms/batch 125.60 | loss  4.51 |
| epoch  15 |   300/  475 batches | ms/batch 122.33 | loss  4.20 |
| epoch  15 |   400/  475 batches | ms/batch 119.11 | loss  3.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 56.45s | training loss  3.90 |
    | end of validation epoch  15 | time: 49.49s | validation loss  3.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 137.26 | loss  3.89 |
| epoch  16 |   200/  475 batches | ms/batch 126.12 | loss  3.60 |
| epoch  16 |   300/  475 batches | ms/batch 123.90 | loss  3.93 |
| epoch  16 |   400/  475 batches | ms/batch 121.86 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 57.96s | training loss  3.88 |
    | end of validation epoch  16 | time: 49.11s | validation loss  3.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 140.00 | loss  3.66 |
| epoch  17 |   200/  475 batches | ms/batch 129.29 | loss  3.79 |
| epoch  17 |   300/  475 batches | ms/batch 125.08 | loss  3.83 |
| epoch  17 |   400/  475 batches | ms/batch 123.17 | loss  3.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 57.88s | training loss  3.84 |
    | end of validation epoch  17 | time: 48.15s | validation loss  3.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 144.71 | loss  3.51 |
| epoch  18 |   200/  475 batches | ms/batch 129.13 | loss  3.80 |
| epoch  18 |   300/  475 batches | ms/batch 124.24 | loss  3.95 |
| epoch  18 |   400/  475 batches | ms/batch 121.72 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 57.25s | training loss  3.81 |
    | end of validation epoch  18 | time: 49.39s | validation loss  3.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 138.07 | loss  3.63 |
| epoch  19 |   200/  475 batches | ms/batch 126.50 | loss  4.32 |
| epoch  19 |   300/  475 batches | ms/batch 122.19 | loss  3.43 |
| epoch  19 |   400/  475 batches | ms/batch 122.06 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 57.49s | training loss  3.80 |
    | end of validation epoch  19 | time: 48.19s | validation loss  3.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 144.09 | loss  3.77 |
| epoch  20 |   200/  475 batches | ms/batch 131.21 | loss  3.78 |
| epoch  20 |   300/  475 batches | ms/batch 126.02 | loss  3.73 |
| epoch  20 |   400/  475 batches | ms/batch 123.36 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 58.16s | training loss  3.76 |
    | end of validation epoch  20 | time: 48.07s | validation loss  3.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 143.50 | loss  3.61 |
| epoch  21 |   200/  475 batches | ms/batch 127.04 | loss  4.27 |
| epoch  21 |   300/  475 batches | ms/batch 121.59 | loss  3.81 |
| epoch  21 |   400/  475 batches | ms/batch 119.71 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 56.70s | training loss  3.72 |
    | end of validation epoch  21 | time: 48.79s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 135.75 | loss  4.01 |
| epoch  22 |   200/  475 batches | ms/batch 123.68 | loss  3.53 |
| epoch  22 |   300/  475 batches | ms/batch 120.76 | loss  3.62 |
| epoch  22 |   400/  475 batches | ms/batch 118.80 | loss  3.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 56.56s | training loss  3.70 |
    | end of validation epoch  22 | time: 48.49s | validation loss  3.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 135.54 | loss  3.56 |
| epoch  23 |   200/  475 batches | ms/batch 126.30 | loss  3.59 |
| epoch  23 |   300/  475 batches | ms/batch 124.25 | loss  3.73 |
| epoch  23 |   400/  475 batches | ms/batch 121.88 | loss  3.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 57.05s | training loss  3.69 |
    | end of validation epoch  23 | time: 48.28s | validation loss  2.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 146.41 | loss  3.45 |
| epoch  24 |   200/  475 batches | ms/batch 130.73 | loss  3.74 |
| epoch  24 |   300/  475 batches | ms/batch 124.96 | loss  3.78 |
| epoch  24 |   400/  475 batches | ms/batch 122.54 | loss  4.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 57.80s | training loss  3.64 |
    | end of validation epoch  24 | time: 49.25s | validation loss  3.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 139.83 | loss  3.30 |
| epoch  25 |   200/  475 batches | ms/batch 126.41 | loss  3.57 |
| epoch  25 |   300/  475 batches | ms/batch 121.72 | loss  3.53 |
| epoch  25 |   400/  475 batches | ms/batch 119.29 | loss  3.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 56.60s | training loss  3.63 |
    | end of validation epoch  25 | time: 48.29s | validation loss  2.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 139.26 | loss  3.75 |
| epoch  26 |   200/  475 batches | ms/batch 126.34 | loss  3.30 |
| epoch  26 |   300/  475 batches | ms/batch 121.25 | loss  3.88 |
| epoch  26 |   400/  475 batches | ms/batch 119.91 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 56.75s | training loss  3.61 |
    | end of validation epoch  26 | time: 48.71s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 140.38 | loss  4.04 |
| epoch  27 |   200/  475 batches | ms/batch 127.45 | loss  3.48 |
| epoch  27 |   300/  475 batches | ms/batch 123.64 | loss  3.77 |
| epoch  27 |   400/  475 batches | ms/batch 121.58 | loss  3.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 57.48s | training loss  3.60 |
    | end of validation epoch  27 | time: 48.08s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 145.72 | loss  3.43 |
| epoch  28 |   200/  475 batches | ms/batch 130.83 | loss  3.70 |
| epoch  28 |   300/  475 batches | ms/batch 125.15 | loss  3.51 |
| epoch  28 |   400/  475 batches | ms/batch 122.16 | loss  3.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 57.75s | training loss  3.56 |
    | end of validation epoch  28 | time: 49.19s | validation loss  2.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 137.92 | loss  3.65 |
| epoch  29 |   200/  475 batches | ms/batch 127.52 | loss  3.54 |
| epoch  29 |   300/  475 batches | ms/batch 123.35 | loss  3.78 |
| epoch  29 |   400/  475 batches | ms/batch 121.42 | loss  3.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 57.69s | training loss  3.55 |
    | end of validation epoch  29 | time: 48.82s | validation loss  2.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 136.68 | loss  4.15 |
| epoch  30 |   200/  475 batches | ms/batch 126.37 | loss  3.76 |
| epoch  30 |   300/  475 batches | ms/batch 122.57 | loss  3.39 |
| epoch  30 |   400/  475 batches | ms/batch 120.98 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 57.22s | training loss  3.54 |
    | end of validation epoch  30 | time: 48.05s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 141.14 | loss  3.17 |
| epoch  31 |   200/  475 batches | ms/batch 129.80 | loss  3.12 |
| epoch  31 |   300/  475 batches | ms/batch 125.39 | loss  3.49 |
| epoch  31 |   400/  475 batches | ms/batch 121.86 | loss  3.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 57.62s | training loss  3.51 |
    | end of validation epoch  31 | time: 49.57s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 138.40 | loss  3.53 |
| epoch  32 |   200/  475 batches | ms/batch 128.56 | loss  3.53 |
| epoch  32 |   300/  475 batches | ms/batch 122.56 | loss  3.61 |
| epoch  32 |   400/  475 batches | ms/batch 120.17 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 56.72s | training loss  3.49 |
    | end of validation epoch  32 | time: 49.20s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 139.96 | loss  3.14 |
| epoch  33 |   200/  475 batches | ms/batch 126.89 | loss  3.62 |
| epoch  33 |   300/  475 batches | ms/batch 124.16 | loss  3.35 |
| epoch  33 |   400/  475 batches | ms/batch 122.63 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 57.87s | training loss  3.48 |
    | end of validation epoch  33 | time: 48.00s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 142.24 | loss  3.75 |
| epoch  34 |   200/  475 batches | ms/batch 130.70 | loss  3.34 |
| epoch  34 |   300/  475 batches | ms/batch 126.86 | loss  3.55 |
| epoch  34 |   400/  475 batches | ms/batch 124.25 | loss  3.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 58.66s | training loss  3.48 |
    | end of validation epoch  34 | time: 48.59s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 142.03 | loss  3.42 |
| epoch  35 |   200/  475 batches | ms/batch 128.39 | loss  3.84 |
| epoch  35 |   300/  475 batches | ms/batch 123.71 | loss  3.88 |
| epoch  35 |   400/  475 batches | ms/batch 121.02 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 57.26s | training loss  3.45 |
    | end of validation epoch  35 | time: 48.68s | validation loss  2.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 138.03 | loss  3.58 |
| epoch  36 |   200/  475 batches | ms/batch 126.27 | loss  3.83 |
| epoch  36 |   300/  475 batches | ms/batch 123.90 | loss  3.27 |
| epoch  36 |   400/  475 batches | ms/batch 121.78 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 57.26s | training loss  3.44 |
    | end of validation epoch  36 | time: 48.48s | validation loss  2.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 145.13 | loss  3.45 |
| epoch  37 |   200/  475 batches | ms/batch 131.85 | loss  3.57 |
| epoch  37 |   300/  475 batches | ms/batch 126.08 | loss  3.45 |
| epoch  37 |   400/  475 batches | ms/batch 123.58 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 58.20s | training loss  3.43 |
    | end of validation epoch  37 | time: 49.87s | validation loss  2.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 140.34 | loss  3.05 |
| epoch  38 |   200/  475 batches | ms/batch 129.59 | loss  3.07 |
| epoch  38 |   300/  475 batches | ms/batch 124.28 | loss  3.30 |
| epoch  38 |   400/  475 batches | ms/batch 121.33 | loss  3.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 57.31s | training loss  3.41 |
    | end of validation epoch  38 | time: 48.35s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 136.98 | loss  3.36 |
| epoch  39 |   200/  475 batches | ms/batch 125.64 | loss  2.86 |
| epoch  39 |   300/  475 batches | ms/batch 121.63 | loss  3.50 |
| epoch  39 |   400/  475 batches | ms/batch 118.99 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 56.56s | training loss  3.42 |
    | end of validation epoch  39 | time: 49.57s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644]
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 139.33 | loss  3.47 |
| epoch  40 |   200/  475 batches | ms/batch 128.46 | loss  3.33 |
| epoch  40 |   300/  475 batches | ms/batch 124.74 | loss  3.24 |
| epoch  40 |   400/  475 batches | ms/batch 120.90 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 57.45s | training loss  3.40 |
    | end of validation epoch  40 | time: 48.04s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 144.28 | loss  3.51 |
| epoch  41 |   200/  475 batches | ms/batch 132.07 | loss  3.59 |
| epoch  41 |   300/  475 batches | ms/batch 125.78 | loss  3.19 |
| epoch  41 |   400/  475 batches | ms/batch 122.56 | loss  2.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 57.83s | training loss  3.38 |
    | end of validation epoch  41 | time: 48.23s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 140.38 | loss  3.25 |
| epoch  42 |   200/  475 batches | ms/batch 127.76 | loss  3.44 |
| epoch  42 |   300/  475 batches | ms/batch 123.68 | loss  3.46 |
| epoch  42 |   400/  475 batches | ms/batch 120.28 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 57.12s | training loss  3.36 |
    | end of validation epoch  42 | time: 49.60s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 141.38 | loss  3.42 |
| epoch  43 |   200/  475 batches | ms/batch 129.51 | loss  3.14 |
| epoch  43 |   300/  475 batches | ms/batch 124.02 | loss  3.26 |
| epoch  43 |   400/  475 batches | ms/batch 121.89 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 57.38s | training loss  3.35 |
    | end of validation epoch  43 | time: 49.21s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 143.05 | loss  3.71 |
| epoch  44 |   200/  475 batches | ms/batch 132.67 | loss  3.06 |
| epoch  44 |   300/  475 batches | ms/batch 127.79 | loss  3.23 |
| epoch  44 |   400/  475 batches | ms/batch 125.12 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 59.14s | training loss  3.34 |
    | end of validation epoch  44 | time: 49.32s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 144.57 | loss  3.22 |
| epoch  45 |   200/  475 batches | ms/batch 129.97 | loss  3.38 |
| epoch  45 |   300/  475 batches | ms/batch 124.61 | loss  3.40 |
| epoch  45 |   400/  475 batches | ms/batch 121.15 | loss  3.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 57.29s | training loss  3.34 |
    | end of validation epoch  45 | time: 50.76s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072]
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 139.51 | loss  3.23 |
| epoch  46 |   200/  475 batches | ms/batch 127.19 | loss  3.41 |
| epoch  46 |   300/  475 batches | ms/batch 125.92 | loss  3.22 |
| epoch  46 |   400/  475 batches | ms/batch 124.30 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 59.16s | training loss  3.33 |
    | end of validation epoch  46 | time: 49.93s | validation loss  2.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 140.69 | loss  3.43 |
| epoch  47 |   200/  475 batches | ms/batch 133.13 | loss  3.62 |
| epoch  47 |   300/  475 batches | ms/batch 129.12 | loss  3.26 |
| epoch  47 |   400/  475 batches | ms/batch 125.87 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 59.10s | training loss  3.31 |
    | end of validation epoch  47 | time: 48.38s | validation loss  2.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 146.06 | loss  3.57 |
| epoch  48 |   200/  475 batches | ms/batch 127.98 | loss  3.09 |
| epoch  48 |   300/  475 batches | ms/batch 122.30 | loss  3.37 |
| epoch  48 |   400/  475 batches | ms/batch 120.45 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 57.03s | training loss  3.31 |
    | end of validation epoch  48 | time: 49.15s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 140.29 | loss  3.11 |
| epoch  49 |   200/  475 batches | ms/batch 128.73 | loss  2.86 |
| epoch  49 |   300/  475 batches | ms/batch 123.73 | loss  3.27 |
| epoch  49 |   400/  475 batches | ms/batch 121.51 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 57.55s | training loss  3.29 |
    | end of validation epoch  49 | time: 48.19s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 137.13 | loss  3.53 |
| epoch  50 |   200/  475 batches | ms/batch 126.52 | loss  3.11 |
| epoch  50 |   300/  475 batches | ms/batch 124.07 | loss  2.92 |
| epoch  50 |   400/  475 batches | ms/batch 121.85 | loss  3.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 57.50s | training loss  3.29 |
    | end of validation epoch  50 | time: 48.39s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 141.72 | loss  3.38 |
| epoch  51 |   200/  475 batches | ms/batch 127.51 | loss  3.53 |
| epoch  51 |   300/  475 batches | ms/batch 122.17 | loss  3.17 |
| epoch  51 |   400/  475 batches | ms/batch 119.85 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 56.30s | training loss  3.28 |
    | end of validation epoch  51 | time: 48.37s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 136.72 | loss  3.12 |
| epoch  52 |   200/  475 batches | ms/batch 124.60 | loss  2.88 |
| epoch  52 |   300/  475 batches | ms/batch 121.79 | loss  2.91 |
| epoch  52 |   400/  475 batches | ms/batch 119.30 | loss  3.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 56.10s | training loss  3.24 |
    | end of validation epoch  52 | time: 48.62s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 139.06 | loss  3.11 |
| epoch  53 |   200/  475 batches | ms/batch 126.60 | loss  2.75 |
| epoch  53 |   300/  475 batches | ms/batch 121.80 | loss  3.36 |
| epoch  53 |   400/  475 batches | ms/batch 120.17 | loss  3.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 56.82s | training loss  3.24 |
    | end of validation epoch  53 | time: 47.94s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 144.97 | loss  2.86 |
| epoch  54 |   200/  475 batches | ms/batch 131.40 | loss  3.22 |
| epoch  54 |   300/  475 batches | ms/batch 125.36 | loss  3.39 |
| epoch  54 |   400/  475 batches | ms/batch 122.93 | loss  3.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 57.73s | training loss  3.24 |
    | end of validation epoch  54 | time: 48.97s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 143.74 | loss  3.24 |
| epoch  55 |   200/  475 batches | ms/batch 129.24 | loss  3.27 |
| epoch  55 |   300/  475 batches | ms/batch 124.11 | loss  3.09 |
| epoch  55 |   400/  475 batches | ms/batch 121.34 | loss  3.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 57.26s | training loss  3.23 |
    | end of validation epoch  55 | time: 48.03s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 137.72 | loss  2.86 |
| epoch  56 |   200/  475 batches | ms/batch 126.07 | loss  3.26 |
| epoch  56 |   300/  475 batches | ms/batch 122.12 | loss  3.07 |
| epoch  56 |   400/  475 batches | ms/batch 120.24 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 57.26s | training loss  3.22 |
    | end of validation epoch  56 | time: 48.10s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 143.44 | loss  2.99 |
| epoch  57 |   200/  475 batches | ms/batch 128.40 | loss  3.06 |
| epoch  57 |   300/  475 batches | ms/batch 125.54 | loss  3.15 |
| epoch  57 |   400/  475 batches | ms/batch 122.93 | loss  2.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 57.84s | training loss  3.21 |
    | end of validation epoch  57 | time: 48.81s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 139.30 | loss  3.20 |
| epoch  58 |   200/  475 batches | ms/batch 129.28 | loss  3.05 |
| epoch  58 |   300/  475 batches | ms/batch 123.54 | loss  2.90 |
| epoch  58 |   400/  475 batches | ms/batch 120.56 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 56.99s | training loss  3.21 |
    | end of validation epoch  58 | time: 49.88s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 143.31 | loss  2.96 |
| epoch  59 |   200/  475 batches | ms/batch 128.78 | loss  3.24 |
| epoch  59 |   300/  475 batches | ms/batch 123.38 | loss  3.31 |
| epoch  59 |   400/  475 batches | ms/batch 120.28 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 56.93s | training loss  3.19 |
    | end of validation epoch  59 | time: 48.36s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 139.25 | loss  3.08 |
| epoch  60 |   200/  475 batches | ms/batch 127.80 | loss  3.11 |
| epoch  60 |   300/  475 batches | ms/batch 123.62 | loss  3.36 |
| epoch  60 |   400/  475 batches | ms/batch 122.23 | loss  2.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 57.75s | training loss  3.19 |
    | end of validation epoch  60 | time: 47.77s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222]
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 143.92 | loss  2.94 |
| epoch  61 |   200/  475 batches | ms/batch 130.64 | loss  3.34 |
| epoch  61 |   300/  475 batches | ms/batch 126.07 | loss  3.63 |
| epoch  61 |   400/  475 batches | ms/batch 122.53 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 57.81s | training loss  3.18 |
    | end of validation epoch  61 | time: 49.06s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 142.19 | loss  3.00 |
| epoch  62 |   200/  475 batches | ms/batch 126.75 | loss  2.71 |
| epoch  62 |   300/  475 batches | ms/batch 121.91 | loss  3.54 |
| epoch  62 |   400/  475 batches | ms/batch 119.70 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 56.58s | training loss  3.19 |
    | end of validation epoch  62 | time: 49.86s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 141.44 | loss  3.03 |
| epoch  63 |   200/  475 batches | ms/batch 126.85 | loss  2.98 |
| epoch  63 |   300/  475 batches | ms/batch 123.36 | loss  3.09 |
| epoch  63 |   400/  475 batches | ms/batch 122.06 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 57.37s | training loss  3.15 |
    | end of validation epoch  63 | time: 48.59s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 140.07 | loss  3.11 |
| epoch  64 |   200/  475 batches | ms/batch 130.24 | loss  3.32 |
| epoch  64 |   300/  475 batches | ms/batch 126.11 | loss  3.32 |
| epoch  64 |   400/  475 batches | ms/batch 123.39 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 58.30s | training loss  3.15 |
    | end of validation epoch  64 | time: 48.71s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 138.82 | loss  3.13 |
| epoch  65 |   200/  475 batches | ms/batch 129.64 | loss  2.63 |
| epoch  65 |   300/  475 batches | ms/batch 124.36 | loss  3.10 |
| epoch  65 |   400/  475 batches | ms/batch 121.04 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 56.84s | training loss  3.16 |
    | end of validation epoch  65 | time: 49.51s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 139.07 | loss  3.36 |
| epoch  66 |   200/  475 batches | ms/batch 125.41 | loss  2.90 |
| epoch  66 |   300/  475 batches | ms/batch 121.05 | loss  3.52 |
| epoch  66 |   400/  475 batches | ms/batch 119.58 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 56.64s | training loss  3.15 |
    | end of validation epoch  66 | time: 48.10s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 140.60 | loss  3.41 |
| epoch  67 |   200/  475 batches | ms/batch 127.71 | loss  3.13 |
| epoch  67 |   300/  475 batches | ms/batch 123.32 | loss  3.01 |
| epoch  67 |   400/  475 batches | ms/batch 122.06 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 57.62s | training loss  3.13 |
    | end of validation epoch  67 | time: 49.17s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 144.62 | loss  2.83 |
| epoch  68 |   200/  475 batches | ms/batch 129.48 | loss  3.26 |
| epoch  68 |   300/  475 batches | ms/batch 124.01 | loss  2.93 |
| epoch  68 |   400/  475 batches | ms/batch 121.04 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 57.26s | training loss  3.12 |
    | end of validation epoch  68 | time: 48.68s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 136.52 | loss  2.89 |
| epoch  69 |   200/  475 batches | ms/batch 125.26 | loss  3.25 |
| epoch  69 |   300/  475 batches | ms/batch 121.74 | loss  3.12 |
| epoch  69 |   400/  475 batches | ms/batch 120.39 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 56.66s | training loss  3.12 |
    | end of validation epoch  69 | time: 48.91s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 141.22 | loss  2.51 |
| epoch  70 |   200/  475 batches | ms/batch 128.62 | loss  2.75 |
| epoch  70 |   300/  475 batches | ms/batch 124.28 | loss  3.75 |
| epoch  70 |   400/  475 batches | ms/batch 122.56 | loss  3.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 57.88s | training loss  3.11 |
    | end of validation epoch  70 | time: 50.05s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 144.46 | loss  3.52 |
| epoch  71 |   200/  475 batches | ms/batch 130.92 | loss  3.38 |
| epoch  71 |   300/  475 batches | ms/batch 125.37 | loss  3.52 |
| epoch  71 |   400/  475 batches | ms/batch 122.64 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 58.09s | training loss  3.11 |
    | end of validation epoch  71 | time: 51.12s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 137.11 | loss  3.06 |
| epoch  72 |   200/  475 batches | ms/batch 124.73 | loss  2.99 |
| epoch  72 |   300/  475 batches | ms/batch 121.60 | loss  2.93 |
| epoch  72 |   400/  475 batches | ms/batch 118.98 | loss  2.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 56.34s | training loss  3.10 |
    | end of validation epoch  72 | time: 49.80s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 137.12 | loss  3.41 |
| epoch  73 |   200/  475 batches | ms/batch 125.72 | loss  2.98 |
| epoch  73 |   300/  475 batches | ms/batch 124.17 | loss  3.03 |
| epoch  73 |   400/  475 batches | ms/batch 121.82 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 57.54s | training loss  3.09 |
    | end of validation epoch  73 | time: 48.05s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 140.70 | loss  2.85 |
| epoch  74 |   200/  475 batches | ms/batch 129.45 | loss  3.18 |
| epoch  74 |   300/  475 batches | ms/batch 126.34 | loss  3.37 |
| epoch  74 |   400/  475 batches | ms/batch 123.73 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 58.23s | training loss  3.10 |
    | end of validation epoch  74 | time: 48.29s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 149.98 | loss  3.03 |
| epoch  75 |   200/  475 batches | ms/batch 131.77 | loss  3.07 |
| epoch  75 |   300/  475 batches | ms/batch 126.43 | loss  3.17 |
| epoch  75 |   400/  475 batches | ms/batch 123.39 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 57.86s | training loss  3.08 |
    | end of validation epoch  75 | time: 50.15s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 139.44 | loss  3.24 |
| epoch  76 |   200/  475 batches | ms/batch 127.07 | loss  3.09 |
| epoch  76 |   300/  475 batches | ms/batch 123.09 | loss  3.34 |
| epoch  76 |   400/  475 batches | ms/batch 121.70 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 57.44s | training loss  3.05 |
    | end of validation epoch  76 | time: 49.36s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 143.52 | loss  3.02 |
| epoch  77 |   200/  475 batches | ms/batch 129.16 | loss  3.39 |
| epoch  77 |   300/  475 batches | ms/batch 124.72 | loss  3.43 |
| epoch  77 |   400/  475 batches | ms/batch 122.87 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 57.93s | training loss  3.07 |
    | end of validation epoch  77 | time: 47.72s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 140.37 | loss  3.06 |
| epoch  78 |   200/  475 batches | ms/batch 127.93 | loss  2.93 |
| epoch  78 |   300/  475 batches | ms/batch 122.26 | loss  3.33 |
| epoch  78 |   400/  475 batches | ms/batch 119.74 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 56.42s | training loss  3.05 |
    | end of validation epoch  78 | time: 48.02s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 138.01 | loss  3.03 |
| epoch  79 |   200/  475 batches | ms/batch 125.86 | loss  2.90 |
| epoch  79 |   300/  475 batches | ms/batch 121.61 | loss  2.81 |
| epoch  79 |   400/  475 batches | ms/batch 119.30 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 56.35s | training loss  3.06 |
    | end of validation epoch  79 | time: 49.17s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 137.03 | loss  3.38 |
| epoch  80 |   200/  475 batches | ms/batch 125.17 | loss  3.58 |
| epoch  80 |   300/  475 batches | ms/batch 121.36 | loss  2.77 |
| epoch  80 |   400/  475 batches | ms/batch 119.83 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 56.67s | training loss  3.06 |
    | end of validation epoch  80 | time: 48.02s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 141.92 | loss  2.92 |
| epoch  81 |   200/  475 batches | ms/batch 130.12 | loss  2.96 |
| epoch  81 |   300/  475 batches | ms/batch 125.36 | loss  3.13 |
| epoch  81 |   400/  475 batches | ms/batch 122.21 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 57.82s | training loss  3.03 |
    | end of validation epoch  81 | time: 48.63s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 141.14 | loss  2.83 |
| epoch  82 |   200/  475 batches | ms/batch 128.60 | loss  3.24 |
| epoch  82 |   300/  475 batches | ms/batch 123.13 | loss  2.55 |
| epoch  82 |   400/  475 batches | ms/batch 120.36 | loss  2.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 57.10s | training loss  3.03 |
    | end of validation epoch  82 | time: 50.39s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 138.33 | loss  3.16 |
| epoch  83 |   200/  475 batches | ms/batch 125.14 | loss  3.11 |
| epoch  83 |   300/  475 batches | ms/batch 123.02 | loss  2.57 |
| epoch  83 |   400/  475 batches | ms/batch 121.93 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 57.75s | training loss  3.03 |
    | end of validation epoch  83 | time: 48.53s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 145.46 | loss  2.88 |
| epoch  84 |   200/  475 batches | ms/batch 132.04 | loss  3.12 |
| epoch  84 |   300/  475 batches | ms/batch 125.56 | loss  2.95 |
| epoch  84 |   400/  475 batches | ms/batch 123.68 | loss  3.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 58.39s | training loss  3.03 |
    | end of validation epoch  84 | time: 48.27s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 146.89 | loss  2.78 |
| epoch  85 |   200/  475 batches | ms/batch 129.59 | loss  2.78 |
| epoch  85 |   300/  475 batches | ms/batch 124.28 | loss  3.13 |
| epoch  85 |   400/  475 batches | ms/batch 121.52 | loss  2.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 57.33s | training loss  3.00 |
    | end of validation epoch  85 | time: 49.57s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 134.26 | loss  2.92 |
| epoch  86 |   200/  475 batches | ms/batch 124.83 | loss  3.03 |
| epoch  86 |   300/  475 batches | ms/batch 121.17 | loss  3.04 |
| epoch  86 |   400/  475 batches | ms/batch 120.00 | loss  3.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 57.01s | training loss  3.00 |
    | end of validation epoch  86 | time: 48.66s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 137.85 | loss  2.98 |
| epoch  87 |   200/  475 batches | ms/batch 130.77 | loss  2.99 |
| epoch  87 |   300/  475 batches | ms/batch 125.70 | loss  3.27 |
| epoch  87 |   400/  475 batches | ms/batch 122.76 | loss  3.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 57.97s | training loss  3.01 |
    | end of validation epoch  87 | time: 48.12s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 143.98 | loss  3.17 |
| epoch  88 |   200/  475 batches | ms/batch 131.09 | loss  2.92 |
| epoch  88 |   300/  475 batches | ms/batch 125.76 | loss  2.81 |
| epoch  88 |   400/  475 batches | ms/batch 122.57 | loss  3.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 57.89s | training loss  2.97 |
    | end of validation epoch  88 | time: 48.35s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 137.99 | loss  3.40 |
| epoch  89 |   200/  475 batches | ms/batch 125.14 | loss  2.82 |
| epoch  89 |   300/  475 batches | ms/batch 122.00 | loss  3.27 |
| epoch  89 |   400/  475 batches | ms/batch 120.02 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 56.83s | training loss  2.98 |
    | end of validation epoch  89 | time: 49.42s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 139.58 | loss  2.72 |
| epoch  90 |   200/  475 batches | ms/batch 127.18 | loss  3.36 |
| epoch  90 |   300/  475 batches | ms/batch 124.79 | loss  3.18 |
| epoch  90 |   400/  475 batches | ms/batch 122.53 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 57.88s | training loss  2.98 |
    | end of validation epoch  90 | time: 48.82s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 139.90 | loss  2.82 |
| epoch  91 |   200/  475 batches | ms/batch 128.83 | loss  3.26 |
| epoch  91 |   300/  475 batches | ms/batch 125.87 | loss  3.15 |
| epoch  91 |   400/  475 batches | ms/batch 122.95 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 57.62s | training loss  2.98 |
    | end of validation epoch  91 | time: 49.07s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 137.65 | loss  2.30 |
| epoch  92 |   200/  475 batches | ms/batch 125.14 | loss  3.28 |
| epoch  92 |   300/  475 batches | ms/batch 121.11 | loss  2.61 |
| epoch  92 |   400/  475 batches | ms/batch 118.86 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 55.91s | training loss  2.96 |
    | end of validation epoch  92 | time: 49.10s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 139.29 | loss  3.00 |
| epoch  93 |   200/  475 batches | ms/batch 125.18 | loss  2.99 |
| epoch  93 |   300/  475 batches | ms/batch 121.63 | loss  2.62 |
| epoch  93 |   400/  475 batches | ms/batch 119.48 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 56.64s | training loss  2.97 |
    | end of validation epoch  93 | time: 48.81s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 134.04 | loss  2.65 |
| epoch  94 |   200/  475 batches | ms/batch 127.81 | loss  2.80 |
| epoch  94 |   300/  475 batches | ms/batch 122.89 | loss  2.50 |
| epoch  94 |   400/  475 batches | ms/batch 121.41 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 57.53s | training loss  2.98 |
    | end of validation epoch  94 | time: 49.02s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 147.30 | loss  2.92 |
| epoch  95 |   200/  475 batches | ms/batch 131.83 | loss  2.93 |
| epoch  95 |   300/  475 batches | ms/batch 126.44 | loss  3.01 |
| epoch  95 |   400/  475 batches | ms/batch 122.34 | loss  3.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 57.91s | training loss  2.95 |
    | end of validation epoch  95 | time: 48.15s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359, 2.9485259196632785] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272, 2.516650646674533]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 138.08 | loss  3.16 |
| epoch  96 |   200/  475 batches | ms/batch 126.26 | loss  3.14 |
| epoch  96 |   300/  475 batches | ms/batch 122.08 | loss  2.84 |
| epoch  96 |   400/  475 batches | ms/batch 120.56 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 56.98s | training loss  2.93 |
    | end of validation epoch  96 | time: 48.41s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359, 2.9485259196632785, 2.9332753296902303] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272, 2.516650646674533, 2.517771337212635]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 138.26 | loss  2.85 |
| epoch  97 |   200/  475 batches | ms/batch 127.32 | loss  2.80 |
| epoch  97 |   300/  475 batches | ms/batch 125.46 | loss  2.52 |
| epoch  97 |   400/  475 batches | ms/batch 122.85 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 58.17s | training loss  2.96 |
    | end of validation epoch  97 | time: 48.42s | validation loss  2.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359, 2.9485259196632785, 2.9332753296902303, 2.9588393050745916] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272, 2.516650646674533, 2.517771337212635, 2.460722990396644]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 142.58 | loss  3.02 |
| epoch  98 |   200/  475 batches | ms/batch 130.12 | loss  3.21 |
| epoch  98 |   300/  475 batches | ms/batch 124.60 | loss  2.90 |
| epoch  98 |   400/  475 batches | ms/batch 121.98 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 57.47s | training loss  2.92 |
    | end of validation epoch  98 | time: 48.51s | validation loss  2.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359, 2.9485259196632785, 2.9332753296902303, 2.9588393050745916, 2.9179488608711646] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272, 2.516650646674533, 2.517771337212635, 2.460722990396644, 2.488597083492439]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 142.19 | loss  2.79 |
| epoch  99 |   200/  475 batches | ms/batch 127.27 | loss  3.01 |
| epoch  99 |   300/  475 batches | ms/batch 123.01 | loss  2.91 |
| epoch  99 |   400/  475 batches | ms/batch 121.35 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 57.37s | training loss  2.92 |
    | end of validation epoch  99 | time: 48.36s | validation loss  2.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.76120064785606, 5.2628436449954386, 4.979679052453292, 4.7536192663092365, 4.600839591779207, 4.477518741708053, 4.367862859525179, 4.296081865712216, 4.215047218423141, 4.154887962341308, 4.098036277168675, 4.034967885268362, 4.002131164450394, 3.942690500460173, 3.9030638077384547, 3.875463913867348, 3.8394180026807283, 3.8070872050837465, 3.797915775901393, 3.7586574378766513, 3.7152860270048444, 3.6969043581109298, 3.6856100278151662, 3.6410200550681666, 3.6302982711791993, 3.6075425469247917, 3.600551243330303, 3.559930568996229, 3.5530985430667275, 3.5379497111471077, 3.5120223662727756, 3.4945031753339264, 3.48395170864306, 3.4770563642602217, 3.4493010776921325, 3.4369285995081853, 3.429381017183003, 3.4056826506162943, 3.415472825200934, 3.3960909025292647, 3.3805695468501042, 3.3637759901347914, 3.3488109187075965, 3.3393510070600008, 3.3410925548955013, 3.332102630514848, 3.306090065303602, 3.308533246893632, 3.293413277174297, 3.2875390875966923, 3.2777643665514495, 3.244892911911011, 3.2392856989408796, 3.2439212794052925, 3.2282030978955722, 3.223490710509451, 3.210604716351158, 3.208528063924689, 3.192046743192171, 3.1933759478518837, 3.180790060946816, 3.1877378468764457, 3.1470939214606033, 3.1491064909884803, 3.155000195754202, 3.147255072844656, 3.126889796507986, 3.1164822613565546, 3.117397107576069, 3.109698805558054, 3.1121313275788958, 3.098246751082571, 3.0894520036797775, 3.1021243622428494, 3.075987333498503, 3.047294028432746, 3.0748564338684083, 3.0510077210476525, 3.0565924102381654, 3.0560134009311075, 3.0303767018569143, 3.032919467624865, 3.0275449652420847, 3.0270105547654, 3.003670268309744, 2.9959503153750773, 3.014150653638338, 2.9741860826391924, 2.9798905006207916, 2.9770286524923226, 2.9839018897006384, 2.9618886074266935, 2.9653538402758146, 2.97521800342359, 2.9485259196632785, 2.9332753296902303, 2.9588393050745916, 2.9179488608711646, 2.9239717322901675] validation loss is  [5.198945610463118, 4.698070798601423, 4.3667722068914845, 4.122693524641149, 3.8958642162194774, 3.799884932381766, 3.6642837965187907, 3.573393777638924, 3.55199319775365, 3.4511919903154133, 3.387860632744156, 3.3479152246683586, 3.3435600585296372, 3.2432267625792686, 3.2216410296303883, 3.195516021311784, 3.116159803727094, 3.2151192516839804, 3.1062973126643847, 3.0861901555742537, 3.0200154520884284, 3.0312152830492547, 2.966706955132364, 2.998743101328361, 2.953573447315633, 2.975890586356155, 2.894317881399844, 2.9277719529736945, 2.900347485261805, 2.8926512874475048, 2.8065021318547867, 2.8548197565960285, 2.8528020221646093, 2.8181777541376962, 2.843313531715329, 2.796212168300853, 2.800285225154973, 2.7350731156453363, 2.7838877008742644, 2.7387108963076807, 2.7321178452307437, 2.774184395285214, 2.7227619054938565, 2.725061895466652, 2.744029542979072, 2.7129238673618863, 2.757856018402997, 2.6666515494595053, 2.671059440164005, 2.667297398342806, 2.7725542172664355, 2.666213782895513, 2.6929710011522308, 2.6713652550673284, 2.66230072093611, 2.6975164774085294, 2.6386608576574244, 2.6609886744443108, 2.616294078466271, 2.6798985024460222, 2.629260998814046, 2.6229074602367497, 2.619335034314324, 2.613068217990779, 2.633275353608011, 2.636704853602818, 2.581485679169663, 2.558157292734675, 2.5982154998458733, 2.6189134521644655, 2.576710338352107, 2.577856656883945, 2.5606206294869174, 2.5906195460247394, 2.595565051591697, 2.5669924762068677, 2.505134136736894, 2.5412101986027564, 2.5703218684476963, 2.5039962209573314, 2.5723892510438167, 2.6072631633582235, 2.5121108023058465, 2.580280050510118, 2.5025589526200496, 2.533439942768642, 2.532080730470289, 2.5467935560130273, 2.5267013441614745, 2.5395271116945923, 2.546366368021284, 2.5678983796544435, 2.521803427143257, 2.5135078199771272, 2.516650646674533, 2.517771337212635, 2.460722990396644, 2.488597083492439, 2.475100965059104]
