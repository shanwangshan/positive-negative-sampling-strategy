/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/random_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 145.60 | loss  7.23 |
| epoch   1 |   200/  475 batches | ms/batch 135.89 | loss  7.04 |
| epoch   1 |   300/  475 batches | ms/batch 131.69 | loss  6.71 |
| epoch   1 |   400/  475 batches | ms/batch 129.11 | loss  6.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 61.23s | training loss  6.88 |
    | end of validation epoch   1 | time: 51.77s | validation loss  5.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.880268483412893] validation loss is  [5.627471038273403]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 141.45 | loss  6.29 |
| epoch   2 |   200/  475 batches | ms/batch 127.96 | loss  6.48 |
| epoch   2 |   300/  475 batches | ms/batch 125.27 | loss  6.78 |
| epoch   2 |   400/  475 batches | ms/batch 122.73 | loss  6.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 58.37s | training loss  6.48 |
    | end of validation epoch   2 | time: 50.81s | validation loss  5.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.880268483412893, 6.479767758218865] validation loss is  [5.627471038273403, 5.405702013929351]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 140.48 | loss  6.47 |
| epoch   3 |   200/  475 batches | ms/batch 128.92 | loss  5.91 |
| epoch   3 |   300/  475 batches | ms/batch 125.21 | loss  6.03 |
| epoch   3 |   400/  475 batches | ms/batch 123.06 | loss  6.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 58.49s | training loss  6.23 |
    | end of validation epoch   3 | time: 50.19s | validation loss  5.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 140.56 | loss  6.21 |
| epoch   4 |   200/  475 batches | ms/batch 130.26 | loss  6.08 |
| epoch   4 |   300/  475 batches | ms/batch 126.61 | loss  6.17 |
| epoch   4 |   400/  475 batches | ms/batch 124.96 | loss  6.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 59.21s | training loss  6.01 |
    | end of validation epoch   4 | time: 50.03s | validation loss  5.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 141.56 | loss  6.00 |
| epoch   5 |   200/  475 batches | ms/batch 129.23 | loss  6.39 |
| epoch   5 |   300/  475 batches | ms/batch 125.36 | loss  5.79 |
| epoch   5 |   400/  475 batches | ms/batch 124.10 | loss  5.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 59.24s | training loss  5.84 |
    | end of validation epoch   5 | time: 49.90s | validation loss  4.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 142.73 | loss  5.66 |
| epoch   6 |   200/  475 batches | ms/batch 130.47 | loss  5.82 |
| epoch   6 |   300/  475 batches | ms/batch 125.47 | loss  5.56 |
| epoch   6 |   400/  475 batches | ms/batch 124.66 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 59.05s | training loss  5.68 |
    | end of validation epoch   6 | time: 49.96s | validation loss  4.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 138.12 | loss  5.67 |
| epoch   7 |   200/  475 batches | ms/batch 129.78 | loss  5.02 |
| epoch   7 |   300/  475 batches | ms/batch 125.95 | loss  5.87 |
| epoch   7 |   400/  475 batches | ms/batch 124.46 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 59.21s | training loss  5.54 |
    | end of validation epoch   7 | time: 50.05s | validation loss  4.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 142.00 | loss  5.44 |
| epoch   8 |   200/  475 batches | ms/batch 129.81 | loss  5.26 |
| epoch   8 |   300/  475 batches | ms/batch 128.17 | loss  5.59 |
| epoch   8 |   400/  475 batches | ms/batch 126.19 | loss  5.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 59.62s | training loss  5.45 |
    | end of validation epoch   8 | time: 51.26s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 143.69 | loss  4.94 |
| epoch   9 |   200/  475 batches | ms/batch 132.37 | loss  5.44 |
| epoch   9 |   300/  475 batches | ms/batch 128.38 | loss  5.45 |
| epoch   9 |   400/  475 batches | ms/batch 126.83 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 59.93s | training loss  5.35 |
    | end of validation epoch   9 | time: 50.88s | validation loss  4.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 140.74 | loss  5.34 |
| epoch  10 |   200/  475 batches | ms/batch 131.01 | loss  5.55 |
| epoch  10 |   300/  475 batches | ms/batch 125.73 | loss  5.54 |
| epoch  10 |   400/  475 batches | ms/batch 123.63 | loss  5.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 58.73s | training loss  5.27 |
    | end of validation epoch  10 | time: 50.08s | validation loss  4.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 138.62 | loss  5.06 |
| epoch  11 |   200/  475 batches | ms/batch 127.71 | loss  4.67 |
| epoch  11 |   300/  475 batches | ms/batch 124.32 | loss  4.69 |
| epoch  11 |   400/  475 batches | ms/batch 122.46 | loss  5.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 58.38s | training loss  5.18 |
    | end of validation epoch  11 | time: 49.85s | validation loss  4.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 142.64 | loss  5.71 |
| epoch  12 |   200/  475 batches | ms/batch 133.11 | loss  5.16 |
| epoch  12 |   300/  475 batches | ms/batch 128.34 | loss  5.00 |
| epoch  12 |   400/  475 batches | ms/batch 126.50 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 59.92s | training loss  5.12 |
    | end of validation epoch  12 | time: 51.16s | validation loss  4.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 139.48 | loss  5.20 |
| epoch  13 |   200/  475 batches | ms/batch 132.13 | loss  5.04 |
| epoch  13 |   300/  475 batches | ms/batch 126.98 | loss  5.06 |
| epoch  13 |   400/  475 batches | ms/batch 123.74 | loss  5.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 58.57s | training loss  5.06 |
    | end of validation epoch  13 | time: 51.86s | validation loss  4.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 142.34 | loss  5.21 |
| epoch  14 |   200/  475 batches | ms/batch 129.59 | loss  5.03 |
| epoch  14 |   300/  475 batches | ms/batch 125.14 | loss  4.97 |
| epoch  14 |   400/  475 batches | ms/batch 123.24 | loss  5.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 58.65s | training loss  5.03 |
    | end of validation epoch  14 | time: 50.19s | validation loss  4.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 145.13 | loss  4.84 |
| epoch  15 |   200/  475 batches | ms/batch 132.10 | loss  4.69 |
| epoch  15 |   300/  475 batches | ms/batch 127.62 | loss  5.07 |
| epoch  15 |   400/  475 batches | ms/batch 125.36 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 59.54s | training loss  4.99 |
    | end of validation epoch  15 | time: 50.17s | validation loss  4.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 146.25 | loss  4.75 |
| epoch  16 |   200/  475 batches | ms/batch 133.48 | loss  4.83 |
| epoch  16 |   300/  475 batches | ms/batch 129.01 | loss  5.18 |
| epoch  16 |   400/  475 batches | ms/batch 126.62 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 60.04s | training loss  4.94 |
    | end of validation epoch  16 | time: 49.96s | validation loss  4.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 145.66 | loss  4.80 |
| epoch  17 |   200/  475 batches | ms/batch 132.08 | loss  5.00 |
| epoch  17 |   300/  475 batches | ms/batch 127.40 | loss  4.49 |
| epoch  17 |   400/  475 batches | ms/batch 125.60 | loss  5.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 59.41s | training loss  4.90 |
    | end of validation epoch  17 | time: 50.21s | validation loss  4.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 140.11 | loss  4.57 |
| epoch  18 |   200/  475 batches | ms/batch 128.95 | loss  4.74 |
| epoch  18 |   300/  475 batches | ms/batch 124.80 | loss  5.07 |
| epoch  18 |   400/  475 batches | ms/batch 123.10 | loss  5.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 58.48s | training loss  4.87 |
    | end of validation epoch  18 | time: 51.98s | validation loss  4.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 140.50 | loss  4.99 |
| epoch  19 |   200/  475 batches | ms/batch 128.45 | loss  4.73 |
| epoch  19 |   300/  475 batches | ms/batch 124.28 | loss  4.84 |
| epoch  19 |   400/  475 batches | ms/batch 123.59 | loss  5.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 58.78s | training loss  4.85 |
    | end of validation epoch  19 | time: 49.90s | validation loss  4.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 143.36 | loss  4.70 |
| epoch  20 |   200/  475 batches | ms/batch 130.30 | loss  4.95 |
| epoch  20 |   300/  475 batches | ms/batch 126.59 | loss  4.95 |
| epoch  20 |   400/  475 batches | ms/batch 125.17 | loss  4.90 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 59.34s | training loss  4.81 |
    | end of validation epoch  20 | time: 48.78s | validation loss  4.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 144.03 | loss  5.00 |
| epoch  21 |   200/  475 batches | ms/batch 131.69 | loss  4.57 |
| epoch  21 |   300/  475 batches | ms/batch 127.14 | loss  4.87 |
| epoch  21 |   400/  475 batches | ms/batch 125.64 | loss  4.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 59.36s | training loss  4.78 |
    | end of validation epoch  21 | time: 51.30s | validation loss  4.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 143.39 | loss  4.86 |
| epoch  22 |   200/  475 batches | ms/batch 129.30 | loss  4.62 |
| epoch  22 |   300/  475 batches | ms/batch 124.86 | loss  4.83 |
| epoch  22 |   400/  475 batches | ms/batch 122.54 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 58.29s | training loss  4.77 |
    | end of validation epoch  22 | time: 50.14s | validation loss  4.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 141.70 | loss  4.90 |
| epoch  23 |   200/  475 batches | ms/batch 131.61 | loss  4.81 |
| epoch  23 |   300/  475 batches | ms/batch 126.55 | loss  4.88 |
| epoch  23 |   400/  475 batches | ms/batch 123.83 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 58.92s | training loss  4.75 |
    | end of validation epoch  23 | time: 50.86s | validation loss  4.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 143.06 | loss  5.01 |
| epoch  24 |   200/  475 batches | ms/batch 129.32 | loss  4.63 |
| epoch  24 |   300/  475 batches | ms/batch 126.14 | loss  4.81 |
| epoch  24 |   400/  475 batches | ms/batch 123.85 | loss  4.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 58.89s | training loss  4.73 |
    | end of validation epoch  24 | time: 51.47s | validation loss  4.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 145.50 | loss  4.84 |
| epoch  25 |   200/  475 batches | ms/batch 132.51 | loss  4.76 |
| epoch  25 |   300/  475 batches | ms/batch 127.47 | loss  5.14 |
| epoch  25 |   400/  475 batches | ms/batch 125.40 | loss  4.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 59.43s | training loss  4.71 |
    | end of validation epoch  25 | time: 49.94s | validation loss  4.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 141.75 | loss  4.48 |
| epoch  26 |   200/  475 batches | ms/batch 131.11 | loss  4.66 |
| epoch  26 |   300/  475 batches | ms/batch 128.77 | loss  4.92 |
| epoch  26 |   400/  475 batches | ms/batch 126.60 | loss  4.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 59.77s | training loss  4.69 |
    | end of validation epoch  26 | time: 50.87s | validation loss  4.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 145.06 | loss  4.66 |
| epoch  27 |   200/  475 batches | ms/batch 131.92 | loss  4.44 |
| epoch  27 |   300/  475 batches | ms/batch 127.81 | loss  4.72 |
| epoch  27 |   400/  475 batches | ms/batch 125.22 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 59.51s | training loss  4.67 |
    | end of validation epoch  27 | time: 50.97s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 143.68 | loss  4.49 |
| epoch  28 |   200/  475 batches | ms/batch 129.89 | loss  4.79 |
| epoch  28 |   300/  475 batches | ms/batch 124.71 | loss  4.83 |
| epoch  28 |   400/  475 batches | ms/batch 123.69 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 58.52s | training loss  4.65 |
    | end of validation epoch  28 | time: 50.74s | validation loss  4.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 138.08 | loss  4.45 |
| epoch  29 |   200/  475 batches | ms/batch 128.13 | loss  4.64 |
| epoch  29 |   300/  475 batches | ms/batch 124.66 | loss  4.65 |
| epoch  29 |   400/  475 batches | ms/batch 123.27 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 58.74s | training loss  4.65 |
    | end of validation epoch  29 | time: 50.70s | validation loss  4.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 143.67 | loss  4.83 |
| epoch  30 |   200/  475 batches | ms/batch 133.25 | loss  4.86 |
| epoch  30 |   300/  475 batches | ms/batch 127.54 | loss  4.55 |
| epoch  30 |   400/  475 batches | ms/batch 125.03 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 59.23s | training loss  4.64 |
    | end of validation epoch  30 | time: 50.75s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 155.65 | loss  4.65 |
| epoch  31 |   200/  475 batches | ms/batch 135.73 | loss  4.57 |
| epoch  31 |   300/  475 batches | ms/batch 130.18 | loss  4.58 |
| epoch  31 |   400/  475 batches | ms/batch 127.22 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 60.30s | training loss  4.63 |
    | end of validation epoch  31 | time: 51.97s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 143.65 | loss  4.40 |
| epoch  32 |   200/  475 batches | ms/batch 130.54 | loss  4.59 |
| epoch  32 |   300/  475 batches | ms/batch 127.59 | loss  4.45 |
| epoch  32 |   400/  475 batches | ms/batch 126.05 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 59.59s | training loss  4.61 |
    | end of validation epoch  32 | time: 51.69s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 144.42 | loss  4.78 |
| epoch  33 |   200/  475 batches | ms/batch 130.59 | loss  4.48 |
| epoch  33 |   300/  475 batches | ms/batch 127.13 | loss  4.32 |
| epoch  33 |   400/  475 batches | ms/batch 125.49 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 59.52s | training loss  4.61 |
    | end of validation epoch  33 | time: 50.68s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 140.66 | loss  4.78 |
| epoch  34 |   200/  475 batches | ms/batch 128.31 | loss  4.44 |
| epoch  34 |   300/  475 batches | ms/batch 125.15 | loss  4.53 |
| epoch  34 |   400/  475 batches | ms/batch 123.76 | loss  4.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 58.78s | training loss  4.60 |
    | end of validation epoch  34 | time: 51.83s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 142.39 | loss  4.13 |
| epoch  35 |   200/  475 batches | ms/batch 130.37 | loss  4.66 |
| epoch  35 |   300/  475 batches | ms/batch 126.46 | loss  4.38 |
| epoch  35 |   400/  475 batches | ms/batch 124.97 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 59.19s | training loss  4.58 |
    | end of validation epoch  35 | time: 51.46s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 142.94 | loss  4.66 |
| epoch  36 |   200/  475 batches | ms/batch 132.07 | loss  4.45 |
| epoch  36 |   300/  475 batches | ms/batch 128.33 | loss  4.67 |
| epoch  36 |   400/  475 batches | ms/batch 125.13 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 59.47s | training loss  4.60 |
    | end of validation epoch  36 | time: 52.36s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 146.53 | loss  4.49 |
| epoch  37 |   200/  475 batches | ms/batch 130.99 | loss  4.86 |
| epoch  37 |   300/  475 batches | ms/batch 126.24 | loss  4.69 |
| epoch  37 |   400/  475 batches | ms/batch 125.31 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 59.23s | training loss  4.59 |
    | end of validation epoch  37 | time: 50.06s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 142.00 | loss  4.43 |
| epoch  38 |   200/  475 batches | ms/batch 129.77 | loss  4.31 |
| epoch  38 |   300/  475 batches | ms/batch 126.49 | loss  4.34 |
| epoch  38 |   400/  475 batches | ms/batch 124.15 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 59.12s | training loss  4.58 |
    | end of validation epoch  38 | time: 51.26s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 141.59 | loss  4.30 |
| epoch  39 |   200/  475 batches | ms/batch 130.94 | loss  4.87 |
| epoch  39 |   300/  475 batches | ms/batch 127.33 | loss  4.64 |
| epoch  39 |   400/  475 batches | ms/batch 125.46 | loss  4.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 59.15s | training loss  4.57 |
    | end of validation epoch  39 | time: 50.98s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 142.34 | loss  4.39 |
| epoch  40 |   200/  475 batches | ms/batch 131.91 | loss  4.50 |
| epoch  40 |   300/  475 batches | ms/batch 126.80 | loss  4.40 |
| epoch  40 |   400/  475 batches | ms/batch 124.53 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 59.16s | training loss  4.57 |
    | end of validation epoch  40 | time: 51.17s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 140.41 | loss  4.60 |
| epoch  41 |   200/  475 batches | ms/batch 128.34 | loss  4.34 |
| epoch  41 |   300/  475 batches | ms/batch 125.60 | loss  4.74 |
| epoch  41 |   400/  475 batches | ms/batch 123.98 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 58.87s | training loss  4.56 |
    | end of validation epoch  41 | time: 51.26s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 143.75 | loss  4.67 |
| epoch  42 |   200/  475 batches | ms/batch 130.38 | loss  4.42 |
| epoch  42 |   300/  475 batches | ms/batch 125.97 | loss  4.58 |
| epoch  42 |   400/  475 batches | ms/batch 126.72 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 60.30s | training loss  4.56 |
    | end of validation epoch  42 | time: 55.63s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 142.26 | loss  4.48 |
| epoch  43 |   200/  475 batches | ms/batch 131.23 | loss  4.60 |
| epoch  43 |   300/  475 batches | ms/batch 127.45 | loss  4.50 |
| epoch  43 |   400/  475 batches | ms/batch 125.03 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 59.29s | training loss  4.55 |
    | end of validation epoch  43 | time: 49.62s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 142.87 | loss  4.35 |
| epoch  44 |   200/  475 batches | ms/batch 131.56 | loss  4.58 |
| epoch  44 |   300/  475 batches | ms/batch 128.69 | loss  4.74 |
| epoch  44 |   400/  475 batches | ms/batch 126.50 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 59.95s | training loss  4.56 |
    | end of validation epoch  44 | time: 51.84s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486]
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 150.53 | loss  4.73 |
| epoch  45 |   200/  475 batches | ms/batch 134.88 | loss  4.42 |
| epoch  45 |   300/  475 batches | ms/batch 128.69 | loss  4.25 |
| epoch  45 |   400/  475 batches | ms/batch 126.64 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 59.85s | training loss  4.56 |
    | end of validation epoch  45 | time: 49.26s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 141.90 | loss  4.10 |
| epoch  46 |   200/  475 batches | ms/batch 130.70 | loss  4.70 |
| epoch  46 |   300/  475 batches | ms/batch 125.83 | loss  4.50 |
| epoch  46 |   400/  475 batches | ms/batch 122.76 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 58.40s | training loss  4.54 |
    | end of validation epoch  46 | time: 50.77s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 143.73 | loss  4.45 |
| epoch  47 |   200/  475 batches | ms/batch 131.32 | loss  4.42 |
| epoch  47 |   300/  475 batches | ms/batch 127.07 | loss  4.63 |
| epoch  47 |   400/  475 batches | ms/batch 125.16 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 59.35s | training loss  4.54 |
    | end of validation epoch  47 | time: 50.41s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381]
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 143.24 | loss  4.69 |
| epoch  48 |   200/  475 batches | ms/batch 130.00 | loss  4.40 |
| epoch  48 |   300/  475 batches | ms/batch 127.28 | loss  4.67 |
| epoch  48 |   400/  475 batches | ms/batch 124.39 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 58.72s | training loss  4.55 |
    | end of validation epoch  48 | time: 51.65s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391]
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 139.88 | loss  4.64 |
| epoch  49 |   200/  475 batches | ms/batch 130.23 | loss  4.60 |
| epoch  49 |   300/  475 batches | ms/batch 124.67 | loss  4.36 |
| epoch  49 |   400/  475 batches | ms/batch 123.16 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 58.33s | training loss  4.53 |
    | end of validation epoch  49 | time: 49.70s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 140.41 | loss  4.71 |
| epoch  50 |   200/  475 batches | ms/batch 130.40 | loss  4.16 |
| epoch  50 |   300/  475 batches | ms/batch 126.07 | loss  5.02 |
| epoch  50 |   400/  475 batches | ms/batch 124.40 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 58.98s | training loss  4.53 |
    | end of validation epoch  50 | time: 50.42s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746]
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 146.51 | loss  4.42 |
| epoch  51 |   200/  475 batches | ms/batch 130.65 | loss  4.57 |
| epoch  51 |   300/  475 batches | ms/batch 126.46 | loss  4.69 |
| epoch  51 |   400/  475 batches | ms/batch 124.21 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 59.26s | training loss  4.54 |
    | end of validation epoch  51 | time: 49.10s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114]
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 141.73 | loss  4.60 |
| epoch  52 |   200/  475 batches | ms/batch 129.83 | loss  4.33 |
| epoch  52 |   300/  475 batches | ms/batch 127.12 | loss  4.48 |
| epoch  52 |   400/  475 batches | ms/batch 124.91 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 58.98s | training loss  4.53 |
    | end of validation epoch  52 | time: 49.85s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908]
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 138.40 | loss  4.72 |
| epoch  53 |   200/  475 batches | ms/batch 128.81 | loss  4.59 |
| epoch  53 |   300/  475 batches | ms/batch 125.77 | loss  4.70 |
| epoch  53 |   400/  475 batches | ms/batch 122.97 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 58.40s | training loss  4.54 |
    | end of validation epoch  53 | time: 49.16s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093]
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 143.22 | loss  4.86 |
| epoch  54 |   200/  475 batches | ms/batch 130.89 | loss  4.68 |
| epoch  54 |   300/  475 batches | ms/batch 126.61 | loss  4.48 |
| epoch  54 |   400/  475 batches | ms/batch 123.79 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 58.56s | training loss  4.53 |
    | end of validation epoch  54 | time: 49.97s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 139.56 | loss  4.38 |
| epoch  55 |   200/  475 batches | ms/batch 128.95 | loss  4.45 |
| epoch  55 |   300/  475 batches | ms/batch 124.68 | loss  3.97 |
| epoch  55 |   400/  475 batches | ms/batch 121.64 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 58.01s | training loss  4.54 |
    | end of validation epoch  55 | time: 50.05s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773]
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 143.16 | loss  4.15 |
| epoch  56 |   200/  475 batches | ms/batch 129.89 | loss  4.60 |
| epoch  56 |   300/  475 batches | ms/batch 125.51 | loss  4.89 |
| epoch  56 |   400/  475 batches | ms/batch 123.86 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 58.80s | training loss  4.53 |
    | end of validation epoch  56 | time: 49.48s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612]
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 141.71 | loss  4.76 |
| epoch  57 |   200/  475 batches | ms/batch 130.01 | loss  4.44 |
| epoch  57 |   300/  475 batches | ms/batch 126.36 | loss  4.26 |
| epoch  57 |   400/  475 batches | ms/batch 124.24 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 58.97s | training loss  4.52 |
    | end of validation epoch  57 | time: 50.31s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 140.46 | loss  4.28 |
| epoch  58 |   200/  475 batches | ms/batch 128.79 | loss  4.47 |
| epoch  58 |   300/  475 batches | ms/batch 125.36 | loss  4.23 |
| epoch  58 |   400/  475 batches | ms/batch 123.14 | loss  4.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 58.44s | training loss  4.53 |
    | end of validation epoch  58 | time: 51.49s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725]
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 140.65 | loss  4.70 |
| epoch  59 |   200/  475 batches | ms/batch 129.80 | loss  4.54 |
| epoch  59 |   300/  475 batches | ms/batch 124.64 | loss  4.54 |
| epoch  59 |   400/  475 batches | ms/batch 122.18 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 58.10s | training loss  4.52 |
    | end of validation epoch  59 | time: 50.85s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946]
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 144.44 | loss  4.50 |
| epoch  60 |   200/  475 batches | ms/batch 131.27 | loss  4.60 |
| epoch  60 |   300/  475 batches | ms/batch 125.69 | loss  4.49 |
| epoch  60 |   400/  475 batches | ms/batch 124.08 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 58.61s | training loss  4.53 |
    | end of validation epoch  60 | time: 50.60s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 140.40 | loss  4.44 |
| epoch  61 |   200/  475 batches | ms/batch 128.42 | loss  4.82 |
| epoch  61 |   300/  475 batches | ms/batch 125.40 | loss  4.41 |
| epoch  61 |   400/  475 batches | ms/batch 122.81 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 58.03s | training loss  4.51 |
    | end of validation epoch  61 | time: 51.49s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 141.43 | loss  4.89 |
| epoch  62 |   200/  475 batches | ms/batch 130.32 | loss  4.50 |
| epoch  62 |   300/  475 batches | ms/batch 125.50 | loss  4.34 |
| epoch  62 |   400/  475 batches | ms/batch 123.76 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 58.64s | training loss  4.53 |
    | end of validation epoch  62 | time: 49.63s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 143.70 | loss  4.66 |
| epoch  63 |   200/  475 batches | ms/batch 129.68 | loss  4.20 |
| epoch  63 |   300/  475 batches | ms/batch 126.34 | loss  4.83 |
| epoch  63 |   400/  475 batches | ms/batch 123.78 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 58.73s | training loss  4.52 |
    | end of validation epoch  63 | time: 49.72s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 145.35 | loss  4.49 |
| epoch  64 |   200/  475 batches | ms/batch 132.15 | loss  4.69 |
| epoch  64 |   300/  475 batches | ms/batch 127.79 | loss  4.32 |
| epoch  64 |   400/  475 batches | ms/batch 124.77 | loss  4.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 58.96s | training loss  4.52 |
    | end of validation epoch  64 | time: 51.01s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 139.18 | loss  4.42 |
| epoch  65 |   200/  475 batches | ms/batch 129.32 | loss  4.46 |
| epoch  65 |   300/  475 batches | ms/batch 124.96 | loss  4.69 |
| epoch  65 |   400/  475 batches | ms/batch 123.07 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 58.54s | training loss  4.51 |
    | end of validation epoch  65 | time: 51.14s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 141.53 | loss  4.20 |
| epoch  66 |   200/  475 batches | ms/batch 128.62 | loss  4.51 |
| epoch  66 |   300/  475 batches | ms/batch 124.98 | loss  4.92 |
| epoch  66 |   400/  475 batches | ms/batch 123.12 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 58.40s | training loss  4.52 |
    | end of validation epoch  66 | time: 48.54s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 141.40 | loss  4.68 |
| epoch  67 |   200/  475 batches | ms/batch 128.35 | loss  4.61 |
| epoch  67 |   300/  475 batches | ms/batch 124.64 | loss  4.41 |
| epoch  67 |   400/  475 batches | ms/batch 122.31 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 58.33s | training loss  4.52 |
    | end of validation epoch  67 | time: 49.33s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782]
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 145.27 | loss  4.64 |
| epoch  68 |   200/  475 batches | ms/batch 131.48 | loss  4.40 |
| epoch  68 |   300/  475 batches | ms/batch 125.74 | loss  4.76 |
| epoch  68 |   400/  475 batches | ms/batch 123.96 | loss  4.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 58.85s | training loss  4.53 |
    | end of validation epoch  68 | time: 50.32s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 140.62 | loss  4.73 |
| epoch  69 |   200/  475 batches | ms/batch 129.77 | loss  4.45 |
| epoch  69 |   300/  475 batches | ms/batch 127.03 | loss  4.45 |
| epoch  69 |   400/  475 batches | ms/batch 125.41 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 58.92s | training loss  4.52 |
    | end of validation epoch  69 | time: 49.06s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 138.56 | loss  4.33 |
| epoch  70 |   200/  475 batches | ms/batch 126.84 | loss  4.45 |
| epoch  70 |   300/  475 batches | ms/batch 123.58 | loss  4.34 |
| epoch  70 |   400/  475 batches | ms/batch 121.91 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 58.06s | training loss  4.50 |
    | end of validation epoch  70 | time: 49.38s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 142.38 | loss  4.23 |
| epoch  71 |   200/  475 batches | ms/batch 130.98 | loss  4.28 |
| epoch  71 |   300/  475 batches | ms/batch 126.32 | loss  4.04 |
| epoch  71 |   400/  475 batches | ms/batch 124.60 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 58.64s | training loss  4.52 |
    | end of validation epoch  71 | time: 51.50s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662]
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 141.76 | loss  4.41 |
| epoch  72 |   200/  475 batches | ms/batch 128.28 | loss  4.36 |
| epoch  72 |   300/  475 batches | ms/batch 125.07 | loss  4.29 |
| epoch  72 |   400/  475 batches | ms/batch 122.63 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 58.32s | training loss  4.52 |
    | end of validation epoch  72 | time: 49.31s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949]
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 144.69 | loss  4.62 |
| epoch  73 |   200/  475 batches | ms/batch 131.67 | loss  4.79 |
| epoch  73 |   300/  475 batches | ms/batch 126.02 | loss  4.57 |
| epoch  73 |   400/  475 batches | ms/batch 123.52 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 58.62s | training loss  4.52 |
    | end of validation epoch  73 | time: 49.48s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 141.92 | loss  4.42 |
| epoch  74 |   200/  475 batches | ms/batch 129.68 | loss  4.55 |
| epoch  74 |   300/  475 batches | ms/batch 125.12 | loss  4.45 |
| epoch  74 |   400/  475 batches | ms/batch 122.69 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 57.98s | training loss  4.51 |
    | end of validation epoch  74 | time: 50.49s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 139.45 | loss  4.38 |
| epoch  75 |   200/  475 batches | ms/batch 130.65 | loss  4.50 |
| epoch  75 |   300/  475 batches | ms/batch 127.37 | loss  4.40 |
| epoch  75 |   400/  475 batches | ms/batch 125.41 | loss  4.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 59.22s | training loss  4.52 |
    | end of validation epoch  75 | time: 49.81s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 143.27 | loss  4.81 |
| epoch  76 |   200/  475 batches | ms/batch 130.07 | loss  4.59 |
| epoch  76 |   300/  475 batches | ms/batch 125.59 | loss  4.47 |
| epoch  76 |   400/  475 batches | ms/batch 123.30 | loss  4.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 58.76s | training loss  4.52 |
    | end of validation epoch  76 | time: 51.04s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 140.91 | loss  4.30 |
| epoch  77 |   200/  475 batches | ms/batch 127.85 | loss  4.87 |
| epoch  77 |   300/  475 batches | ms/batch 125.33 | loss  4.67 |
| epoch  77 |   400/  475 batches | ms/batch 123.84 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 58.70s | training loss  4.51 |
    | end of validation epoch  77 | time: 49.07s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102]
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 142.40 | loss  4.68 |
| epoch  78 |   200/  475 batches | ms/batch 131.50 | loss  4.60 |
| epoch  78 |   300/  475 batches | ms/batch 126.68 | loss  4.45 |
| epoch  78 |   400/  475 batches | ms/batch 124.32 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 59.02s | training loss  4.52 |
    | end of validation epoch  78 | time: 48.67s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 144.04 | loss  4.51 |
| epoch  79 |   200/  475 batches | ms/batch 130.95 | loss  4.48 |
| epoch  79 |   300/  475 batches | ms/batch 125.79 | loss  4.97 |
| epoch  79 |   400/  475 batches | ms/batch 123.13 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 58.12s | training loss  4.52 |
    | end of validation epoch  79 | time: 50.59s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 143.82 | loss  4.64 |
| epoch  80 |   200/  475 batches | ms/batch 129.87 | loss  4.24 |
| epoch  80 |   300/  475 batches | ms/batch 125.21 | loss  4.70 |
| epoch  80 |   400/  475 batches | ms/batch 123.51 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 58.78s | training loss  4.51 |
    | end of validation epoch  80 | time: 51.29s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 143.62 | loss  4.53 |
| epoch  81 |   200/  475 batches | ms/batch 130.93 | loss  4.25 |
| epoch  81 |   300/  475 batches | ms/batch 126.11 | loss  4.46 |
| epoch  81 |   400/  475 batches | ms/batch 124.42 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 58.98s | training loss  4.51 |
    | end of validation epoch  81 | time: 49.17s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135]
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 141.32 | loss  4.21 |
| epoch  82 |   200/  475 batches | ms/batch 129.67 | loss  4.65 |
| epoch  82 |   300/  475 batches | ms/batch 126.65 | loss  4.75 |
| epoch  82 |   400/  475 batches | ms/batch 124.13 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 58.83s | training loss  4.53 |
    | end of validation epoch  82 | time: 50.19s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 142.92 | loss  4.64 |
| epoch  83 |   200/  475 batches | ms/batch 128.66 | loss  4.39 |
| epoch  83 |   300/  475 batches | ms/batch 125.32 | loss  4.49 |
| epoch  83 |   400/  475 batches | ms/batch 123.52 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 58.44s | training loss  4.51 |
    | end of validation epoch  83 | time: 49.46s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 144.07 | loss  4.27 |
| epoch  84 |   200/  475 batches | ms/batch 131.38 | loss  4.26 |
| epoch  84 |   300/  475 batches | ms/batch 126.38 | loss  4.49 |
| epoch  84 |   400/  475 batches | ms/batch 124.06 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 58.81s | training loss  4.50 |
    | end of validation epoch  84 | time: 50.82s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 145.91 | loss  3.93 |
| epoch  85 |   200/  475 batches | ms/batch 132.39 | loss  4.81 |
| epoch  85 |   300/  475 batches | ms/batch 127.29 | loss  4.75 |
| epoch  85 |   400/  475 batches | ms/batch 124.76 | loss  4.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 58.71s | training loss  4.50 |
    | end of validation epoch  85 | time: 50.72s | validation loss  4.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 142.71 | loss  4.41 |
| epoch  86 |   200/  475 batches | ms/batch 130.16 | loss  4.22 |
| epoch  86 |   300/  475 batches | ms/batch 126.28 | loss  4.40 |
| epoch  86 |   400/  475 batches | ms/batch 124.08 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 58.89s | training loss  4.52 |
    | end of validation epoch  86 | time: 50.29s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 143.73 | loss  4.68 |
| epoch  87 |   200/  475 batches | ms/batch 129.87 | loss  4.20 |
| epoch  87 |   300/  475 batches | ms/batch 126.57 | loss  4.38 |
| epoch  87 |   400/  475 batches | ms/batch 123.93 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 58.61s | training loss  4.51 |
    | end of validation epoch  87 | time: 49.59s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446]
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 143.25 | loss  4.30 |
| epoch  88 |   200/  475 batches | ms/batch 130.41 | loss  4.28 |
| epoch  88 |   300/  475 batches | ms/batch 126.11 | loss  4.59 |
| epoch  88 |   400/  475 batches | ms/batch 123.75 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 58.42s | training loss  4.51 |
    | end of validation epoch  88 | time: 49.96s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 141.04 | loss  4.38 |
| epoch  89 |   200/  475 batches | ms/batch 130.48 | loss  5.14 |
| epoch  89 |   300/  475 batches | ms/batch 126.22 | loss  4.55 |
| epoch  89 |   400/  475 batches | ms/batch 124.60 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 58.63s | training loss  4.52 |
    | end of validation epoch  89 | time: 49.66s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 144.76 | loss  4.03 |
| epoch  90 |   200/  475 batches | ms/batch 131.91 | loss  4.81 |
| epoch  90 |   300/  475 batches | ms/batch 126.73 | loss  4.70 |
| epoch  90 |   400/  475 batches | ms/batch 123.91 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 58.56s | training loss  4.52 |
    | end of validation epoch  90 | time: 49.83s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 144.94 | loss  4.57 |
| epoch  91 |   200/  475 batches | ms/batch 131.01 | loss  4.65 |
| epoch  91 |   300/  475 batches | ms/batch 125.62 | loss  4.73 |
| epoch  91 |   400/  475 batches | ms/batch 123.40 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 58.38s | training loss  4.51 |
    | end of validation epoch  91 | time: 49.69s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 141.47 | loss  4.48 |
| epoch  92 |   200/  475 batches | ms/batch 129.18 | loss  4.76 |
| epoch  92 |   300/  475 batches | ms/batch 124.43 | loss  4.54 |
| epoch  92 |   400/  475 batches | ms/batch 122.01 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 58.07s | training loss  4.52 |
    | end of validation epoch  92 | time: 50.29s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 141.02 | loss  4.48 |
| epoch  93 |   200/  475 batches | ms/batch 130.47 | loss  4.41 |
| epoch  93 |   300/  475 batches | ms/batch 125.53 | loss  4.61 |
| epoch  93 |   400/  475 batches | ms/batch 124.15 | loss  4.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 58.83s | training loss  4.52 |
    | end of validation epoch  93 | time: 51.59s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 141.67 | loss  4.45 |
| epoch  94 |   200/  475 batches | ms/batch 129.39 | loss  4.58 |
| epoch  94 |   300/  475 batches | ms/batch 126.13 | loss  4.71 |
| epoch  94 |   400/  475 batches | ms/batch 124.12 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 58.75s | training loss  4.51 |
    | end of validation epoch  94 | time: 49.70s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 142.52 | loss  4.87 |
| epoch  95 |   200/  475 batches | ms/batch 130.84 | loss  4.45 |
| epoch  95 |   300/  475 batches | ms/batch 126.14 | loss  4.51 |
| epoch  95 |   400/  475 batches | ms/batch 123.31 | loss  4.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 58.61s | training loss  4.52 |
    | end of validation epoch  95 | time: 49.52s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319, 4.520887574145668] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087, 3.9794372791001775]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 144.82 | loss  4.64 |
| epoch  96 |   200/  475 batches | ms/batch 129.84 | loss  4.10 |
| epoch  96 |   300/  475 batches | ms/batch 125.35 | loss  4.71 |
| epoch  96 |   400/  475 batches | ms/batch 122.47 | loss  4.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 58.15s | training loss  4.51 |
    | end of validation epoch  96 | time: 49.65s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319, 4.520887574145668, 4.506599574841951] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087, 3.9794372791001775, 3.9909400899871055]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 143.13 | loss  4.78 |
| epoch  97 |   200/  475 batches | ms/batch 131.03 | loss  4.06 |
| epoch  97 |   300/  475 batches | ms/batch 126.39 | loss  4.01 |
| epoch  97 |   400/  475 batches | ms/batch 123.95 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 58.71s | training loss  4.51 |
    | end of validation epoch  97 | time: 50.95s | validation loss  4.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319, 4.520887574145668, 4.506599574841951, 4.508414611314472] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087, 3.9794372791001775, 3.9909400899871055, 4.005824010913112]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 140.66 | loss  4.67 |
| epoch  98 |   200/  475 batches | ms/batch 128.22 | loss  4.59 |
| epoch  98 |   300/  475 batches | ms/batch 125.37 | loss  4.64 |
| epoch  98 |   400/  475 batches | ms/batch 123.47 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 58.91s | training loss  4.51 |
    | end of validation epoch  98 | time: 50.05s | validation loss  3.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319, 4.520887574145668, 4.506599574841951, 4.508414611314472, 4.5084572029113765] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087, 3.9794372791001775, 3.9909400899871055, 4.005824010913112, 3.979683513400935]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 141.63 | loss  4.41 |
| epoch  99 |   200/  475 batches | ms/batch 130.14 | loss  4.60 |
| epoch  99 |   300/  475 batches | ms/batch 126.32 | loss  4.73 |
| epoch  99 |   400/  475 batches | ms/batch 123.97 | loss  4.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 58.73s | training loss  4.51 |
    | end of validation epoch  99 | time: 50.10s | validation loss  3.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.880268483412893, 6.479767758218865, 6.227095420235082, 6.006759504017077, 5.837856306778757, 5.677045527006451, 5.541553875772577, 5.450757742430034, 5.34682603635286, 5.265604253066214, 5.180273526844226, 5.117060856066252, 5.06447467402408, 5.031941593571713, 4.986571966472425, 4.936002527538099, 4.897714288611161, 4.865386905670166, 4.845742105182849, 4.806222134640342, 4.776198041815507, 4.773307407780697, 4.745976923892372, 4.7301584544934725, 4.7079081334565815, 4.687995550256026, 4.666389327802156, 4.652897712305973, 4.649316020764803, 4.635697813535992, 4.6251959529675934, 4.608428651910079, 4.614306237070184, 4.601036752901579, 4.5833644104003906, 4.6005528685921115, 4.585418852755898, 4.575947808717427, 4.570467197016666, 4.56845531463623, 4.562579729682521, 4.559239302183452, 4.554848196631983, 4.55524243706151, 4.558055203086451, 4.5423000019475035, 4.542844755273117, 4.546676278365286, 4.528129691575703, 4.530939627195659, 4.53668256157323, 4.529381486491153, 4.53568059971458, 4.528786113638627, 4.542860494914808, 4.534646805211118, 4.52012466932598, 4.528745325991982, 4.522493790074399, 4.525604955773605, 4.510668738515753, 4.534280088324296, 4.520417029230218, 4.521306638717651, 4.509836079446893, 4.522739887739482, 4.5189506475549, 4.533450382132279, 4.515027306205348, 4.50183330837049, 4.519280976747211, 4.516632203553852, 4.51720270508214, 4.506989643699244, 4.5248042026319, 4.519579794030441, 4.512592537026657, 4.521286242635626, 4.5221983603427285, 4.5062759133389125, 4.50944931231047, 4.529573323099236, 4.511411807411595, 4.504722683555202, 4.501903626793309, 4.519515623293425, 4.513152905012432, 4.513265509354441, 4.520663165042275, 4.516526066629511, 4.506606413690667, 4.517555391913966, 4.5155214063744795, 4.513139020016319, 4.520887574145668, 4.506599574841951, 4.508414611314472, 4.5084572029113765, 4.5138515377044675] validation loss is  [5.627471038273403, 5.405702013929351, 5.213854252791204, 5.076212482292111, 4.9561655060583805, 4.833500132841222, 4.752661620869356, 4.695255848539977, 4.631371546192329, 4.5672343358272265, 4.540574991402506, 4.495047765619614, 4.453462620743182, 4.411489947503354, 4.369493548609629, 4.348386504069096, 4.324108640686805, 4.302312983184302, 4.293509759822814, 4.272218780357297, 4.249020396160478, 4.232666177909915, 4.2084910609141115, 4.197652820779496, 4.194548560791657, 4.182307425667258, 4.173670382058921, 4.148880321438573, 4.148236206599644, 4.128951970268698, 4.122859421898337, 4.1149781591752, 4.103471862167871, 4.105114568181398, 4.12089924251332, 4.101300179457464, 4.100905310206053, 4.069864996341097, 4.066890970999453, 4.082309199982331, 4.076903323165509, 4.067058497116346, 4.069011051113866, 4.071158160682486, 4.044686409605651, 4.048372927834006, 4.052817270535381, 4.049895783432391, 4.032317420013812, 4.056021299682746, 4.041733725732114, 4.03723409195908, 4.039757892865093, 4.031927854073148, 4.034516745254773, 4.032057718068612, 4.021986973385851, 4.0275252526547725, 4.027709037315946, 4.019089730847783, 4.0116758867472155, 4.022072751982873, 4.031342614598635, 3.993994352196445, 4.015167480757256, 4.0028824605861635, 4.012218645640782, 4.013372385201334, 3.985725350740577, 4.010396454514575, 4.009949465759662, 4.008098834702949, 4.005323273794992, 4.001283539443457, 3.996853832437211, 3.9953569203865626, 3.9948542759198102, 3.988916094563588, 4.016860364865856, 3.9837627130396225, 3.996695516490135, 4.001608273562263, 3.9962974055474545, 3.986305711650047, 3.9978622809177686, 3.9946104057696687, 3.994992244143446, 4.005833786074855, 3.9762901097786525, 3.9840455736432756, 3.993861494945879, 3.9858925683157787, 3.9808279506298674, 3.9827859081140087, 3.9794372791001775, 3.9909400899871055, 4.005824010913112, 3.979683513400935, 3.992743309806375]
