/run/nvme/job_2955852/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 309, 'ckp_path': './checkpoint/', 'vgg_path': '/vgg-sound/', 'unwanted_files_path': '../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64}
use_cude True
all together the number of training files is 169683
total number of training files is 38007
total number of training files is 38007
Let's use 4 GPUs!
Directory  ./checkpoint/  already exists
this is epoch 0
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   0 |   100/  123 batches | ms/batch 5411.93 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   0 | time: 629.49s | training loss  5.73 |
[5.731328685109208]
this is epoch 1
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   1 |   100/  123 batches | ms/batch 5625.21 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 644.16s | training loss  5.66 |
[5.731328685109208, 5.657885380876743]
this is epoch 2
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   2 |   100/  123 batches | ms/batch 5419.97 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 617.66s | training loss  5.59 |
[5.731328685109208, 5.657885380876743, 5.588132804002219]
this is epoch 3
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   3 |   100/  123 batches | ms/batch 5369.55 | loss  5.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 618.83s | training loss  5.54 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012]
this is epoch 4
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   4 |   100/  123 batches | ms/batch 5395.89 | loss  5.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 613.93s | training loss  5.50 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107]
this is epoch 5
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   5 |   100/  123 batches | ms/batch 5358.12 | loss  5.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 621.14s | training loss  5.46 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529]
this is epoch 6
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   6 |   100/  123 batches | ms/batch 5366.81 | loss  5.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 615.63s | training loss  5.42 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118]
this is epoch 7
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   7 |   100/  123 batches | ms/batch 5321.18 | loss  5.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 624.39s | training loss  5.39 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348]
this is epoch 8
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   8 |   100/  123 batches | ms/batch 5304.30 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 627.07s | training loss  5.35 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314]
this is epoch 9
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   9 |   100/  123 batches | ms/batch 5516.28 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 628.04s | training loss  5.31 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032]
this is epoch 10
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  10 |   100/  123 batches | ms/batch 5315.74 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 608.01s | training loss  5.27 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486]
this is epoch 11
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  11 |   100/  123 batches | ms/batch 5397.61 | loss  5.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 615.86s | training loss  5.23 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475]
this is epoch 12
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  12 |   100/  123 batches | ms/batch 5283.26 | loss  5.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 617.75s | training loss  5.18 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939]
this is epoch 13
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  13 |   100/  123 batches | ms/batch 5521.96 | loss  5.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 624.19s | training loss  5.14 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469]
this is epoch 14
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  14 |   100/  123 batches | ms/batch 5353.73 | loss  5.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 618.16s | training loss  5.10 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705]
this is epoch 15
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  15 |   100/  123 batches | ms/batch 5673.03 | loss  5.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 647.71s | training loss  5.07 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523]
this is epoch 16
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  16 |   100/  123 batches | ms/batch 5398.88 | loss  5.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 622.78s | training loss  5.02 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573]
this is epoch 17
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  17 |   100/  123 batches | ms/batch 5376.54 | loss  4.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 619.73s | training loss  4.97 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685]
this is epoch 18
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  18 |   100/  123 batches | ms/batch 5407.90 | loss  4.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 617.49s | training loss  4.92 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685]
this is epoch 19
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  19 |   100/  123 batches | ms/batch 5277.78 | loss  4.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 615.34s | training loss  4.88 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216]
this is epoch 20
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  20 |   100/  123 batches | ms/batch 5391.11 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 611.75s | training loss  4.84 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795]
this is epoch 21
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  21 |   100/  123 batches | ms/batch 5496.22 | loss  4.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 625.95s | training loss  4.79 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511]
this is epoch 22
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  22 |   100/  123 batches | ms/batch 5366.26 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 609.72s | training loss  4.73 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715]
this is epoch 23
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  23 |   100/  123 batches | ms/batch 5343.52 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 611.68s | training loss  4.67 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396]
this is epoch 24
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  24 |   100/  123 batches | ms/batch 5436.57 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 617.94s | training loss  4.62 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261]
this is epoch 25
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  25 |   100/  123 batches | ms/batch 5547.34 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 634.56s | training loss  4.56 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964]
this is epoch 26
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  26 |   100/  123 batches | ms/batch 5404.16 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 632.28s | training loss  4.51 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888]
this is epoch 27
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  27 |   100/  123 batches | ms/batch 5489.84 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 620.66s | training loss  4.45 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369]
this is epoch 28
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  28 |   100/  123 batches | ms/batch 5321.63 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 635.07s | training loss  4.40 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569]
this is epoch 29
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  29 |   100/  123 batches | ms/batch 5403.13 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 621.68s | training loss  4.34 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895]
this is epoch 30
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  30 |   100/  123 batches | ms/batch 5327.59 | loss  4.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 626.16s | training loss  4.28 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549]
this is epoch 31
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  31 |   100/  123 batches | ms/batch 5483.90 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 628.45s | training loss  4.20 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458]
this is epoch 32
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  32 |   100/  123 batches | ms/batch 5453.21 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 626.57s | training loss  4.16 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651]
this is epoch 33
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  33 |   100/  123 batches | ms/batch 5448.97 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 627.42s | training loss  4.09 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144]
this is epoch 34
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  34 |   100/  123 batches | ms/batch 5303.06 | loss  4.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 612.16s | training loss  4.04 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114]
this is epoch 35
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  35 |   100/  123 batches | ms/batch 5341.01 | loss  4.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 612.19s | training loss  3.98 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959]
this is epoch 36
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  36 |   100/  123 batches | ms/batch 5334.68 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 616.70s | training loss  3.92 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166]
this is epoch 37
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  37 |   100/  123 batches | ms/batch 5461.89 | loss  3.82 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 620.84s | training loss  3.87 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846]
this is epoch 38
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  38 |   100/  123 batches | ms/batch 5441.56 | loss  3.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 623.28s | training loss  3.79 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423]
this is epoch 39
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  39 |   100/  123 batches | ms/batch 5394.96 | loss  3.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 621.59s | training loss  3.75 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827]
this is epoch 40
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  40 |   100/  123 batches | ms/batch 5440.20 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 625.55s | training loss  3.70 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985]
this is epoch 41
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  41 |   100/  123 batches | ms/batch 5515.98 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 632.99s | training loss  3.63 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833]
this is epoch 42
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  42 |   100/  123 batches | ms/batch 5346.45 | loss  3.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 623.03s | training loss  3.58 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986]
this is epoch 43
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  43 |   100/  123 batches | ms/batch 5326.71 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 632.95s | training loss  3.54 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476]
this is epoch 44
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  44 |   100/  123 batches | ms/batch 5418.73 | loss  3.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 619.29s | training loss  3.48 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326]
this is epoch 45
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  45 |   100/  123 batches | ms/batch 5523.48 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 625.50s | training loss  3.43 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145]
this is epoch 46
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  46 |   100/  123 batches | ms/batch 5406.18 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 617.50s | training loss  3.40 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732]
this is epoch 47
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  47 |   100/  123 batches | ms/batch 5380.13 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 628.56s | training loss  3.33 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523]
this is epoch 48
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  48 |   100/  123 batches | ms/batch 5361.43 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 619.28s | training loss  3.29 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178]
this is epoch 49
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  49 |   100/  123 batches | ms/batch 5594.32 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 635.13s | training loss  3.24 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754]
this is epoch 50
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  50 |   100/  123 batches | ms/batch 5474.71 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 621.99s | training loss  3.20 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996]
this is epoch 51
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  51 |   100/  123 batches | ms/batch 5492.77 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 623.87s | training loss  3.17 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132]
this is epoch 52
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  52 |   100/  123 batches | ms/batch 5334.94 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 618.08s | training loss  3.13 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583]
this is epoch 53
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  53 |   100/  123 batches | ms/batch 5430.92 | loss  3.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 619.06s | training loss  3.08 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576]
this is epoch 54
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  54 |   100/  123 batches | ms/batch 5358.92 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 611.35s | training loss  3.04 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767]
this is epoch 55
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  55 |   100/  123 batches | ms/batch 5398.07 | loss  2.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 622.08s | training loss  3.01 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374]
this is epoch 56
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  56 |   100/  123 batches | ms/batch 5408.92 | loss  2.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 621.12s | training loss  2.96 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161]
this is epoch 57
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  57 |   100/  123 batches | ms/batch 5417.01 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 618.52s | training loss  2.93 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735]
this is epoch 58
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  58 |   100/  123 batches | ms/batch 5351.05 | loss  3.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 612.57s | training loss  2.90 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767]
this is epoch 59
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  59 |   100/  123 batches | ms/batch 5502.18 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 622.97s | training loss  2.86 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433]
this is epoch 60
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  60 |   100/  123 batches | ms/batch 5393.77 | loss  2.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 619.62s | training loss  2.81 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543]
this is epoch 61
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  61 |   100/  123 batches | ms/batch 5335.97 | loss  2.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 615.01s | training loss  2.77 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714]
this is epoch 62
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  62 |   100/  123 batches | ms/batch 5396.93 | loss  2.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 616.86s | training loss  2.75 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146]
this is epoch 63
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  63 |   100/  123 batches | ms/batch 5449.40 | loss  2.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 619.77s | training loss  2.71 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243]
this is epoch 64
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  64 |   100/  123 batches | ms/batch 5355.99 | loss  2.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 625.54s | training loss  2.68 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157]
this is epoch 65
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  65 |   100/  123 batches | ms/batch 5528.23 | loss  2.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 620.72s | training loss  2.65 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094]
this is epoch 66
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  66 |   100/  123 batches | ms/batch 5415.77 | loss  2.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 623.02s | training loss  2.61 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216]
this is epoch 67
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  67 |   100/  123 batches | ms/batch 5496.07 | loss  2.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 630.60s | training loss  2.58 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503]
this is epoch 68
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  68 |   100/  123 batches | ms/batch 5357.87 | loss  2.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 617.90s | training loss  2.55 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705]
this is epoch 69
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  69 |   100/  123 batches | ms/batch 5438.46 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 641.25s | training loss  2.53 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544]
this is epoch 70
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  70 |   100/  123 batches | ms/batch 5432.75 | loss  2.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 621.86s | training loss  2.49 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156]
this is epoch 71
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  71 |   100/  123 batches | ms/batch 5328.93 | loss  2.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 615.84s | training loss  2.48 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253]
this is epoch 72
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  72 |   100/  123 batches | ms/batch 5297.25 | loss  2.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 617.40s | training loss  2.45 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484]
this is epoch 73
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  73 |   100/  123 batches | ms/batch 5385.54 | loss  2.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 616.89s | training loss  2.42 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355]
this is epoch 74
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  74 |   100/  123 batches | ms/batch 5340.10 | loss  2.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 613.95s | training loss  2.40 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024]
this is epoch 75
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  75 |   100/  123 batches | ms/batch 5311.45 | loss  2.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 619.28s | training loss  2.37 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536]
this is epoch 76
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  76 |   100/  123 batches | ms/batch 5355.85 | loss  2.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 608.95s | training loss  2.34 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198]
this is epoch 77
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  77 |   100/  123 batches | ms/batch 5362.10 | loss  2.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 621.41s | training loss  2.33 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658]
this is epoch 78
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  78 |   100/  123 batches | ms/batch 5306.07 | loss  2.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 625.84s | training loss  2.30 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172]
this is epoch 79
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  79 |   100/  123 batches | ms/batch 5475.91 | loss  2.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 620.87s | training loss  2.28 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008]
this is epoch 80
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  80 |   100/  123 batches | ms/batch 5348.56 | loss  2.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 620.66s | training loss  2.26 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807]
this is epoch 81
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  81 |   100/  123 batches | ms/batch 5525.13 | loss  2.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 631.37s | training loss  2.23 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779]
this is epoch 82
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  82 |   100/  123 batches | ms/batch 5360.39 | loss  2.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 615.12s | training loss  2.22 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662]
this is epoch 83
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  83 |   100/  123 batches | ms/batch 5481.72 | loss  2.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 630.23s | training loss  2.19 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374]
this is epoch 84
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  84 |   100/  123 batches | ms/batch 5505.27 | loss  2.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 636.33s | training loss  2.16 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765]
this is epoch 85
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  85 |   100/  123 batches | ms/batch 5357.56 | loss  2.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 617.88s | training loss  2.16 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217]
this is epoch 86
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  86 |   100/  123 batches | ms/batch 5417.17 | loss  2.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 613.92s | training loss  2.13 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517]
this is epoch 87
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  87 |   100/  123 batches | ms/batch 5303.82 | loss  2.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 619.77s | training loss  2.11 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985]
this is epoch 88
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  88 |   100/  123 batches | ms/batch 5484.43 | loss  2.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 630.15s | training loss  2.09 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628]
this is epoch 89
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  89 |   100/  123 batches | ms/batch 5412.80 | loss  2.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 617.91s | training loss  2.07 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382]
this is epoch 90
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  90 |   100/  123 batches | ms/batch 5323.97 | loss  2.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 609.09s | training loss  2.05 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157]
this is epoch 91
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  91 |   100/  123 batches | ms/batch 5259.26 | loss  2.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 617.57s | training loss  2.03 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767]
this is epoch 92
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  92 |   100/  123 batches | ms/batch 5483.29 | loss  1.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 636.70s | training loss  2.01 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735]
this is epoch 93
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  93 |   100/  123 batches | ms/batch 5341.34 | loss  2.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 622.69s | training loss  2.00 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867]
this is epoch 94
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  94 |   100/  123 batches | ms/batch 5260.42 | loss  2.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 621.66s | training loss  1.98 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748]
this is epoch 95
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  95 |   100/  123 batches | ms/batch 5385.57 | loss  1.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 620.35s | training loss  1.95 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748, 1.9525058027205429]
this is epoch 96
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  96 |   100/  123 batches | ms/batch 5228.38 | loss  1.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 617.15s | training loss  1.95 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748, 1.9525058027205429, 1.9478037890379991]
this is epoch 97
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  97 |   100/  123 batches | ms/batch 5405.84 | loss  1.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 613.79s | training loss  1.93 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748, 1.9525058027205429, 1.9478037890379991, 1.9274435498850133]
this is epoch 98
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  98 |   100/  123 batches | ms/batch 5464.70 | loss  1.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 615.93s | training loss  1.91 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748, 1.9525058027205429, 1.9478037890379991, 1.9274435498850133, 1.9141852923525058]
this is epoch 99
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  99 |   100/  123 batches | ms/batch 5537.35 | loss  1.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 626.38s | training loss  1.90 |
[5.731328685109208, 5.657885380876743, 5.588132804002219, 5.544346685332012, 5.497940943493107, 5.460344070341529, 5.424301004022118, 5.390209884178348, 5.3452387980329314, 5.305534700068032, 5.269594099463486, 5.2319343652182475, 5.183984415317939, 5.144705299439469, 5.098128768486705, 5.066871162352523, 5.022588741488573, 4.9740283159705685, 4.923074121397685, 4.877054881274216, 4.8420444426497795, 4.790285013555511, 4.733378856162715, 4.674255502902396, 4.616688267002261, 4.564887779514964, 4.508871012586888, 4.446371555328369, 4.400387953936569, 4.33902188432895, 4.275391349947549, 4.204093510542458, 4.157286514111651, 4.092183845799144, 4.040055701403114, 3.981700197467959, 3.9193319906064166, 3.868286599957846, 3.7925530720532423, 3.7467855050311827, 3.6956031748918985, 3.6318381879387833, 3.5838917968718986, 3.5415096515562476, 3.4797570472810326, 3.4328318146186145, 3.395511169743732, 3.331464598818523, 3.2868750211669178, 3.239274738280754, 3.2040582788668996, 3.1667679112132, 3.1285234738171583, 3.078628152366576, 3.0390344228201767, 3.0055577444836374, 2.957943629443161, 2.9293007501741735, 2.8996460844830767, 2.8582307749647433, 2.8073750511417543, 2.7746316843885714, 2.7538895064253146, 2.709140356963243, 2.677041487965157, 2.651693824830094, 2.6140208631996216, 2.577346714531503, 2.5520155604292705, 2.5297176043192544, 2.4931054231597156, 2.477712142758253, 2.4521318005352484, 2.416192198187355, 2.3970261763751024, 2.371187822605536, 2.3423378293107198, 2.326411884974658, 2.2980324097765172, 2.279026244714008, 2.2558155660706807, 2.233910918235779, 2.2186278230775662, 2.1931315573250374, 2.163931665381765, 2.1554105107377217, 2.1273973511486517, 2.111287922393985, 2.091979006441628, 2.0702395623292382, 2.052638017065157, 2.0286286342434767, 2.011692553031735, 2.001696618591867, 1.9775630196904748, 1.9525058027205429, 1.9478037890379991, 1.9274435498850133, 1.9141852923525058, 1.9018366986173925]
