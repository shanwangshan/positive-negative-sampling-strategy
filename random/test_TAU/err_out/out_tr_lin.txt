/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1157.34 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 124.18s | training loss  2.83 |
    | end of validation epoch   1 | time: 75.79s | validation loss  2.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.8343637827280403] validation loss is  [2.014051139354706]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1141.31 | loss  2.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 122.56s | training loss  2.53 |
    | end of validation epoch   2 | time: 72.13s | validation loss  1.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.8343637827280403, 2.534661967475135] validation loss is  [2.014051139354706, 1.805315613746643]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 1131.67 | loss  2.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 121.45s | training loss  2.32 |
    | end of validation epoch   3 | time: 71.55s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 1121.66 | loss  1.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 120.78s | training loss  2.19 |
    | end of validation epoch   4 | time: 71.97s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 1111.45 | loss  2.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 120.17s | training loss  2.06 |
    | end of validation epoch   5 | time: 72.17s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 1115.10 | loss  1.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 119.79s | training loss  1.96 |
    | end of validation epoch   6 | time: 72.27s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1108.88 | loss  1.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 120.21s | training loss  1.90 |
    | end of validation epoch   7 | time: 72.14s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1121.29 | loss  1.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 120.35s | training loss  1.84 |
    | end of validation epoch   8 | time: 76.50s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1128.69 | loss  1.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 121.25s | training loss  1.78 |
    | end of validation epoch   9 | time: 73.62s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1106.90 | loss  1.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 119.46s | training loss  1.73 |
    | end of validation epoch  10 | time: 72.71s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1103.64 | loss  1.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 119.27s | training loss  1.70 |
    | end of validation epoch  11 | time: 71.50s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1112.05 | loss  1.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 120.89s | training loss  1.68 |
    | end of validation epoch  12 | time: 74.82s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1118.31 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 120.05s | training loss  1.64 |
    | end of validation epoch  13 | time: 73.76s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1112.48 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 119.80s | training loss  1.63 |
    | end of validation epoch  14 | time: 72.44s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1111.00 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 119.31s | training loss  1.59 |
    | end of validation epoch  15 | time: 72.40s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1270.03 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 135.67s | training loss  1.58 |
    | end of validation epoch  16 | time: 74.25s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1107.02 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 119.79s | training loss  1.56 |
    | end of validation epoch  17 | time: 73.37s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1111.85 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 119.73s | training loss  1.54 |
    | end of validation epoch  18 | time: 72.37s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1126.53 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 121.22s | training loss  1.52 |
    | end of validation epoch  19 | time: 72.28s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 1140.99 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 122.88s | training loss  1.50 |
    | end of validation epoch  20 | time: 72.72s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 1132.83 | loss  1.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 121.63s | training loss  1.51 |
    | end of validation epoch  21 | time: 74.02s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1130.74 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 121.10s | training loss  1.48 |
    | end of validation epoch  22 | time: 72.22s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1126.20 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 121.28s | training loss  1.47 |
    | end of validation epoch  23 | time: 73.23s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1136.42 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 121.84s | training loss  1.45 |
    | end of validation epoch  24 | time: 74.63s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1127.67 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 121.28s | training loss  1.44 |
    | end of validation epoch  25 | time: 74.03s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1115.81 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 120.65s | training loss  1.44 |
    | end of validation epoch  26 | time: 74.56s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286]
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1121.64 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 120.33s | training loss  1.43 |
    | end of validation epoch  27 | time: 74.26s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1126.37 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 120.89s | training loss  1.40 |
    | end of validation epoch  28 | time: 71.41s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1123.67 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 121.61s | training loss  1.41 |
    | end of validation epoch  29 | time: 75.38s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1119.60 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 122.22s | training loss  1.39 |
    | end of validation epoch  30 | time: 73.49s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1124.34 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 120.86s | training loss  1.39 |
    | end of validation epoch  31 | time: 72.53s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1116.35 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 119.81s | training loss  1.40 |
    | end of validation epoch  32 | time: 72.56s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136]
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1131.21 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 121.31s | training loss  1.39 |
    | end of validation epoch  33 | time: 72.78s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1117.93 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 119.86s | training loss  1.38 |
    | end of validation epoch  34 | time: 74.42s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1123.69 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 120.75s | training loss  1.38 |
    | end of validation epoch  35 | time: 73.14s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1119.39 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 120.23s | training loss  1.38 |
    | end of validation epoch  36 | time: 73.59s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1115.83 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 120.74s | training loss  1.36 |
    | end of validation epoch  37 | time: 74.76s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1132.51 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 122.06s | training loss  1.37 |
    | end of validation epoch  38 | time: 73.97s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594]
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 1127.68 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 120.92s | training loss  1.36 |
    | end of validation epoch  39 | time: 75.38s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874]
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 1131.76 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 121.44s | training loss  1.35 |
    | end of validation epoch  40 | time: 73.58s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 1114.53 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 119.97s | training loss  1.35 |
    | end of validation epoch  41 | time: 74.37s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 1117.97 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 120.53s | training loss  1.36 |
    | end of validation epoch  42 | time: 72.15s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 1118.73 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 120.06s | training loss  1.35 |
    | end of validation epoch  43 | time: 72.97s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 1117.13 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 120.18s | training loss  1.36 |
    | end of validation epoch  44 | time: 75.34s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206]
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 1125.36 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 121.15s | training loss  1.37 |
    | end of validation epoch  45 | time: 73.23s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1110.04 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 119.41s | training loss  1.36 |
    | end of validation epoch  46 | time: 73.86s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 1126.52 | loss  1.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 120.87s | training loss  1.35 |
    | end of validation epoch  47 | time: 71.09s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 1125.27 | loss  1.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 120.75s | training loss  1.36 |
    | end of validation epoch  48 | time: 70.95s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 1120.15 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 120.25s | training loss  1.34 |
    | end of validation epoch  49 | time: 72.24s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 1122.70 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 120.72s | training loss  1.34 |
    | end of validation epoch  50 | time: 72.31s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 1117.43 | loss  1.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 120.17s | training loss  1.35 |
    | end of validation epoch  51 | time: 74.64s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 1159.30 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 124.65s | training loss  1.34 |
    | end of validation epoch  52 | time: 78.17s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 1207.96 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 129.34s | training loss  1.35 |
    | end of validation epoch  53 | time: 74.16s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 1153.52 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 124.35s | training loss  1.34 |
    | end of validation epoch  54 | time: 74.08s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1164.87 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 125.07s | training loss  1.35 |
    | end of validation epoch  55 | time: 73.71s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 1144.26 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 124.59s | training loss  1.34 |
    | end of validation epoch  56 | time: 80.93s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 1153.57 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 123.86s | training loss  1.33 |
    | end of validation epoch  57 | time: 76.59s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 1156.72 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 124.52s | training loss  1.32 |
    | end of validation epoch  58 | time: 74.79s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 1156.40 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 124.73s | training loss  1.33 |
    | end of validation epoch  59 | time: 72.98s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 1164.45 | loss  1.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 124.94s | training loss  1.34 |
    | end of validation epoch  60 | time: 72.87s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 1164.25 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 125.31s | training loss  1.35 |
    | end of validation epoch  61 | time: 73.42s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 1154.29 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 123.84s | training loss  1.34 |
    | end of validation epoch  62 | time: 73.42s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 1159.50 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 125.17s | training loss  1.34 |
    | end of validation epoch  63 | time: 75.67s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443]
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 1162.35 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 125.00s | training loss  1.33 |
    | end of validation epoch  64 | time: 76.23s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 1161.61 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 125.06s | training loss  1.33 |
    | end of validation epoch  65 | time: 75.25s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 1151.75 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 124.18s | training loss  1.34 |
    | end of validation epoch  66 | time: 73.88s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 1167.75 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 125.21s | training loss  1.33 |
    | end of validation epoch  67 | time: 72.24s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 1148.50 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 124.17s | training loss  1.33 |
    | end of validation epoch  68 | time: 76.63s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 1156.06 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 124.35s | training loss  1.33 |
    | end of validation epoch  69 | time: 73.47s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 1164.41 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 124.78s | training loss  1.34 |
    | end of validation epoch  70 | time: 74.99s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 1283.53 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 137.14s | training loss  1.34 |
    | end of validation epoch  71 | time: 80.44s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 1167.36 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 126.64s | training loss  1.32 |
    | end of validation epoch  72 | time: 77.50s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 1173.64 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 126.10s | training loss  1.33 |
    | end of validation epoch  73 | time: 74.68s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 1183.29 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 126.80s | training loss  1.32 |
    | end of validation epoch  74 | time: 77.02s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 1174.00 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 125.92s | training loss  1.32 |
    | end of validation epoch  75 | time: 74.92s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 1180.23 | loss  1.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 126.72s | training loss  1.34 |
    | end of validation epoch  76 | time: 76.92s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 1178.93 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 126.48s | training loss  1.34 |
    | end of validation epoch  77 | time: 78.94s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 1180.08 | loss  1.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 126.91s | training loss  1.32 |
    | end of validation epoch  78 | time: 75.05s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 1179.52 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 126.48s | training loss  1.33 |
    | end of validation epoch  79 | time: 75.55s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 1179.20 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 126.64s | training loss  1.33 |
    | end of validation epoch  80 | time: 76.86s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 1177.45 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 126.33s | training loss  1.32 |
    | end of validation epoch  81 | time: 77.89s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 1176.11 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 126.90s | training loss  1.33 |
    | end of validation epoch  82 | time: 78.11s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 1165.49 | loss  1.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 126.02s | training loss  1.33 |
    | end of validation epoch  83 | time: 74.85s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 1171.25 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 125.76s | training loss  1.35 |
    | end of validation epoch  84 | time: 75.00s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 1184.12 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 126.98s | training loss  1.33 |
    | end of validation epoch  85 | time: 75.95s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 1168.11 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 125.85s | training loss  1.34 |
    | end of validation epoch  86 | time: 78.07s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1170.91 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 125.74s | training loss  1.32 |
    | end of validation epoch  87 | time: 77.96s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 1192.17 | loss  1.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 128.02s | training loss  1.31 |
    | end of validation epoch  88 | time: 75.13s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 1178.45 | loss  1.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 126.97s | training loss  1.34 |
    | end of validation epoch  89 | time: 76.78s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 1247.87 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 133.99s | training loss  1.34 |
    | end of validation epoch  90 | time: 78.72s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 1171.80 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 126.24s | training loss  1.33 |
    | end of validation epoch  91 | time: 82.73s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 1183.64 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 127.70s | training loss  1.34 |
    | end of validation epoch  92 | time: 74.47s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 1172.31 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 126.89s | training loss  1.32 |
    | end of validation epoch  93 | time: 74.84s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 1173.05 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 126.21s | training loss  1.32 |
    | end of validation epoch  94 | time: 76.81s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1175.01 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 126.46s | training loss  1.32 |
    | end of validation epoch  95 | time: 76.76s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186, 1.3234833554104641] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789, 1.1056048258518179]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 1186.79 | loss  1.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 127.29s | training loss  1.32 |
    | end of validation epoch  96 | time: 73.78s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186, 1.3234833554104641, 1.3218434093234774] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789, 1.1056048258518179, 1.097523083910346]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 1177.45 | loss  1.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 126.21s | training loss  1.32 |
    | end of validation epoch  97 | time: 74.21s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186, 1.3234833554104641, 1.3218434093234774, 1.3194914207802162] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789, 1.1056048258518179, 1.097523083910346, 1.091365948629876]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 1174.45 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 126.20s | training loss  1.33 |
    | end of validation epoch  98 | time: 76.15s | validation loss  1.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186, 1.3234833554104641, 1.3218434093234774, 1.3194914207802162, 1.33033870684134] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789, 1.1056048258518179, 1.097523083910346, 1.091365948629876, 1.1126442818591993]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 1175.71 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 126.25s | training loss  1.33 |
    | end of validation epoch  99 | time: 78.57s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.8343637827280403, 2.534661967475135, 2.3215980884191154, 2.1902430852254233, 2.0577706796628936, 1.9567875121090863, 1.8993631096573564, 1.8356293536521293, 1.781585759944744, 1.733361487990027, 1.6987056001886591, 1.6757518772606377, 1.63643462593491, 1.6323739193581246, 1.5935124234036282, 1.5833176750320572, 1.5582424241143304, 1.541089491801219, 1.5187654237489443, 1.5042157602739763, 1.5058718071327553, 1.4822020133336384, 1.4737926109417065, 1.4460032995756682, 1.4393714236783552, 1.4395060388891547, 1.4322090191884085, 1.4029622496785343, 1.4053887740985767, 1.3887393786026552, 1.3908028484464765, 1.398288430394353, 1.3853027090296015, 1.3810920908644393, 1.3829281620077185, 1.38048914853517, 1.359467143411035, 1.3698952853142679, 1.3643546899159749, 1.354146595473762, 1.348448083207414, 1.3558939394650158, 1.3501639817212079, 1.364677263809754, 1.3717394229528066, 1.3560529378083375, 1.351186382877934, 1.3577779187812462, 1.3441478905377087, 1.343586522179681, 1.347754753387726, 1.3405672263454746, 1.3519831412547343, 1.3399633673934248, 1.3454622406143326, 1.3440653128666922, 1.3278953706895984, 1.3244746139457635, 1.3295835900951076, 1.3433538106110718, 1.3456406238916758, 1.3360628877674137, 1.3391886358862524, 1.3267658218607172, 1.3318532017974165, 1.3394385458112836, 1.3342395988670555, 1.332609125622758, 1.3322182496388753, 1.3388680122994088, 1.3394765477996688, 1.320624006761087, 1.3323535274814915, 1.3194121755995192, 1.3247445521053967, 1.3386937487232793, 1.3404213920369878, 1.3197224505312808, 1.3315215395377562, 1.3347121090502352, 1.3223967391091425, 1.3263058844987337, 1.3251850229125839, 1.3458766582849864, 1.3250552405108202, 1.3397021299009924, 1.3184147622134235, 1.31103228233956, 1.3368896933289263, 1.3409878004778613, 1.3340456872373014, 1.3387841222522494, 1.3151768628541414, 1.3235559635334186, 1.3234833554104641, 1.3218434093234774, 1.3194914207802162, 1.33033870684134, 1.3269534755397487] validation loss is  [2.014051139354706, 1.805315613746643, 1.6796793614824612, 1.5653176307678223, 1.4934801533818245, 1.4397917079428832, 1.4006056264042854, 1.3738678799321253, 1.3413973078131676, 1.3108826167881489, 1.3077930087844531, 1.2985723509142797, 1.2609943933784962, 1.2542288191616535, 1.2466515333702166, 1.226646554345886, 1.2158121895045042, 1.2110617011785507, 1.2101006802792351, 1.2015442146609228, 1.1951587917283177, 1.2062280410900712, 1.1831237295021613, 1.1744576633597414, 1.1650125067681074, 1.1733513716608286, 1.180328629290064, 1.1634280470510323, 1.1623183044915397, 1.1465583241855104, 1.1432563768078883, 1.152390838911136, 1.1514557929088671, 1.1420024524753292, 1.1343412436544895, 1.142927313533922, 1.1396164211134117, 1.147703006863594, 1.1396809599051874, 1.1253008075679343, 1.1336070985222857, 1.1308630562076967, 1.117671428558727, 1.1417106684918206, 1.1179741574451327, 1.1241673709203799, 1.1241179341450334, 1.1153672635555267, 1.1221766248345375, 1.1323472373187542, 1.1241375956063469, 1.1119427988305688, 1.1252123937010765, 1.1211349874113996, 1.1263896947105725, 1.121653470210731, 1.107781193840007, 1.1150597259402275, 1.112013874265055, 1.112061142611007, 1.1108396121611197, 1.1058380367855232, 1.1068226229399443, 1.1126481912409265, 1.1334611711402733, 1.108648587949574, 1.1108175770690043, 1.1212433502078056, 1.109654809969167, 1.1179554450015228, 1.114732747276624, 1.1005528798947732, 1.1161151562506955, 1.108670004333059, 1.099013444967568, 1.106171103194356, 1.107205342501402, 1.1075027162830036, 1.1136822073409955, 1.1161414921904604, 1.0988086154684424, 1.1046803550173838, 1.1089409810180466, 1.0919860539336999, 1.1034165288632114, 1.1012643001352747, 1.107314287374417, 1.096485425097247, 1.1031355165566008, 1.0985019135599334, 1.1107847886160016, 1.092022832793494, 1.0901467225824792, 1.101618333098789, 1.1056048258518179, 1.097523083910346, 1.091365948629876, 1.1126442818591993, 1.097083115639786]
