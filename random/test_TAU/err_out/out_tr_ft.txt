/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is False
Directory  ./audio_model_ft/  already exists
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 997.51 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 107.81s | training loss  1.63 |
    | end of validation epoch   1 | time: 72.22s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [1.6273042495186265] validation loss is  [1.0484119756923367]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 945.05 | loss  1.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 101.90s | training loss  1.10 |
    | end of validation epoch   2 | time: 61.55s | validation loss  0.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [1.6273042495186265, 1.1033582939757958] validation loss is  [1.0484119756923367, 0.9369165604778876]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 934.54 | loss  0.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 100.96s | training loss  0.93 |
    | end of validation epoch   3 | time: 61.58s | validation loss  0.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 929.69 | loss  1.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 99.83s | training loss  0.80 |
    | end of validation epoch   4 | time: 67.90s | validation loss  0.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 936.39 | loss  0.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 101.84s | training loss  0.70 |
    | end of validation epoch   5 | time: 62.56s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 948.22 | loss  0.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 101.90s | training loss  0.62 |
    | end of validation epoch   6 | time: 66.02s | validation loss  0.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 926.10 | loss  0.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 99.45s | training loss  0.59 |
    | end of validation epoch   7 | time: 63.84s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 943.45 | loss  0.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 101.42s | training loss  0.53 |
    | end of validation epoch   8 | time: 63.72s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 918.20 | loss  0.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 98.91s | training loss  0.49 |
    | end of validation epoch   9 | time: 65.17s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 985.70 | loss  0.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 105.42s | training loss  0.44 |
    | end of validation epoch  10 | time: 61.98s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 922.69 | loss  0.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 99.58s | training loss  0.42 |
    | end of validation epoch  11 | time: 65.60s | validation loss  1.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 914.70 | loss  0.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 98.44s | training loss  0.40 |
    | end of validation epoch  12 | time: 63.46s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 943.43 | loss  0.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 101.43s | training loss  0.37 |
    | end of validation epoch  13 | time: 64.45s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 931.06 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 100.16s | training loss  0.36 |
    | end of validation epoch  14 | time: 65.74s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 936.17 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 100.72s | training loss  0.31 |
    | end of validation epoch  15 | time: 63.24s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 933.97 | loss  0.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 100.56s | training loss  0.31 |
    | end of validation epoch  16 | time: 66.34s | validation loss  1.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 929.93 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 100.17s | training loss  0.30 |
    | end of validation epoch  17 | time: 64.15s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 936.69 | loss  0.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 102.02s | training loss  0.27 |
    | end of validation epoch  18 | time: 64.33s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 922.73 | loss  0.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 99.88s | training loss  0.27 |
    | end of validation epoch  19 | time: 66.19s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866]
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 954.30 | loss  0.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 102.71s | training loss  0.24 |
    | end of validation epoch  20 | time: 62.53s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 921.69 | loss  0.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 99.18s | training loss  0.24 |
    | end of validation epoch  21 | time: 65.56s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 921.88 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 99.29s | training loss  0.24 |
    | end of validation epoch  22 | time: 63.03s | validation loss  1.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554]
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 963.62 | loss  0.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 106.33s | training loss  0.23 |
    | end of validation epoch  23 | time: 65.16s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 962.10 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 104.41s | training loss  0.19 |
    | end of validation epoch  24 | time: 65.27s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 970.08 | loss  0.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 104.11s | training loss  0.20 |
    | end of validation epoch  25 | time: 63.41s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415]
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 945.86 | loss  0.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 101.82s | training loss  0.21 |
    | end of validation epoch  26 | time: 67.84s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239]
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 944.28 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 101.65s | training loss  0.19 |
    | end of validation epoch  27 | time: 64.02s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 947.15 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 103.04s | training loss  0.18 |
    | end of validation epoch  28 | time: 65.57s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 935.65 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 100.59s | training loss  0.17 |
    | end of validation epoch  29 | time: 65.02s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 942.92 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 101.94s | training loss  0.17 |
    | end of validation epoch  30 | time: 63.40s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 935.79 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 100.81s | training loss  0.17 |
    | end of validation epoch  31 | time: 66.20s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 941.57 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 101.10s | training loss  0.17 |
    | end of validation epoch  32 | time: 63.03s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 947.76 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 101.84s | training loss  0.16 |
    | end of validation epoch  33 | time: 65.84s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 929.47 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 100.11s | training loss  0.15 |
    | end of validation epoch  34 | time: 64.75s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 955.71 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 102.97s | training loss  0.14 |
    | end of validation epoch  35 | time: 63.94s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 933.74 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 100.36s | training loss  0.15 |
    | end of validation epoch  36 | time: 65.99s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303]
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 939.88 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 100.95s | training loss  0.13 |
    | end of validation epoch  37 | time: 62.65s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 940.00 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 101.29s | training loss  0.14 |
    | end of validation epoch  38 | time: 66.51s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878]
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 934.73 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 100.38s | training loss  0.12 |
    | end of validation epoch  39 | time: 64.29s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 945.30 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 101.65s | training loss  0.12 |
    | end of validation epoch  40 | time: 64.60s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725]
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 934.88 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 100.90s | training loss  0.13 |
    | end of validation epoch  41 | time: 65.94s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 942.68 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 101.18s | training loss  0.12 |
    | end of validation epoch  42 | time: 62.07s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 930.87 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 100.12s | training loss  0.12 |
    | end of validation epoch  43 | time: 65.72s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 924.11 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 99.37s | training loss  0.12 |
    | end of validation epoch  44 | time: 63.48s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 929.11 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 101.78s | training loss  0.12 |
    | end of validation epoch  45 | time: 64.04s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 917.64 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 98.75s | training loss  0.11 |
    | end of validation epoch  46 | time: 65.79s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 938.27 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 100.70s | training loss  0.11 |
    | end of validation epoch  47 | time: 62.87s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 935.22 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 100.41s | training loss  0.11 |
    | end of validation epoch  48 | time: 66.40s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 931.83 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 100.17s | training loss  0.09 |
    | end of validation epoch  49 | time: 64.19s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 938.21 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 101.44s | training loss  0.10 |
    | end of validation epoch  50 | time: 64.59s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 928.39 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 100.03s | training loss  0.10 |
    | end of validation epoch  51 | time: 65.91s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 945.34 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 101.81s | training loss  0.09 |
    | end of validation epoch  52 | time: 63.00s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 931.91 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 100.80s | training loss  0.09 |
    | end of validation epoch  53 | time: 66.32s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 928.61 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 99.78s | training loss  0.09 |
    | end of validation epoch  54 | time: 64.04s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 938.94 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 100.88s | training loss  0.10 |
    | end of validation epoch  55 | time: 65.14s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 922.47 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 100.28s | training loss  0.09 |
    | end of validation epoch  56 | time: 65.73s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 943.66 | loss  0.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 101.68s | training loss  0.08 |
    | end of validation epoch  57 | time: 63.28s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 931.66 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 100.21s | training loss  0.09 |
    | end of validation epoch  58 | time: 66.35s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 931.64 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 100.26s | training loss  0.08 |
    | end of validation epoch  59 | time: 63.83s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 938.53 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 100.93s | training loss  0.08 |
    | end of validation epoch  60 | time: 65.04s | validation loss  1.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 932.87 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 100.59s | training loss  0.08 |
    | end of validation epoch  61 | time: 65.88s | validation loss  1.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 935.77 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 100.83s | training loss  0.08 |
    | end of validation epoch  62 | time: 63.36s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 934.24 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 100.27s | training loss  0.08 |
    | end of validation epoch  63 | time: 66.43s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 947.47 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 101.75s | training loss  0.08 |
    | end of validation epoch  64 | time: 63.85s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 932.38 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 100.54s | training loss  0.07 |
    | end of validation epoch  65 | time: 65.16s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 927.66 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 99.67s | training loss  0.07 |
    | end of validation epoch  66 | time: 69.70s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 936.17 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 101.06s | training loss  0.08 |
    | end of validation epoch  67 | time: 63.94s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 925.73 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 99.49s | training loss  0.06 |
    | end of validation epoch  68 | time: 65.77s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 915.43 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 99.28s | training loss  0.07 |
    | end of validation epoch  69 | time: 62.22s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 928.06 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 99.79s | training loss  0.07 |
    | end of validation epoch  70 | time: 65.60s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 911.87 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 98.17s | training loss  0.06 |
    | end of validation epoch  71 | time: 64.13s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 929.10 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 99.83s | training loss  0.07 |
    | end of validation epoch  72 | time: 64.16s | validation loss  1.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 924.72 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 99.30s | training loss  0.07 |
    | end of validation epoch  73 | time: 66.20s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 929.39 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 99.80s | training loss  0.07 |
    | end of validation epoch  74 | time: 62.37s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 929.34 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 100.27s | training loss  0.05 |
    | end of validation epoch  75 | time: 66.33s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 923.06 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 100.29s | training loss  0.06 |
    | end of validation epoch  76 | time: 64.56s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 939.74 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 101.02s | training loss  0.06 |
    | end of validation epoch  77 | time: 64.03s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 926.91 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 99.54s | training loss  0.06 |
    | end of validation epoch  78 | time: 66.46s | validation loss  1.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 934.67 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 100.44s | training loss  0.06 |
    | end of validation epoch  79 | time: 62.64s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 936.10 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 100.70s | training loss  0.07 |
    | end of validation epoch  80 | time: 65.90s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 959.44 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 102.84s | training loss  0.06 |
    | end of validation epoch  81 | time: 63.31s | validation loss  1.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 928.87 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 100.20s | training loss  0.06 |
    | end of validation epoch  82 | time: 64.22s | validation loss  1.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 911.39 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 98.03s | training loss  0.06 |
    | end of validation epoch  83 | time: 65.29s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 923.07 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 99.17s | training loss  0.06 |
    | end of validation epoch  84 | time: 61.77s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 929.17 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 99.85s | training loss  0.06 |
    | end of validation epoch  85 | time: 66.37s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 925.92 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 99.42s | training loss  0.06 |
    | end of validation epoch  86 | time: 64.13s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 934.13 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 100.56s | training loss  0.05 |
    | end of validation epoch  87 | time: 64.56s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 924.15 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 99.32s | training loss  0.05 |
    | end of validation epoch  88 | time: 65.74s | validation loss  1.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 935.12 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 100.48s | training loss  0.06 |
    | end of validation epoch  89 | time: 62.48s | validation loss  1.90 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 931.60 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 100.17s | training loss  0.06 |
    | end of validation epoch  90 | time: 66.25s | validation loss  1.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 925.94 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 99.76s | training loss  0.06 |
    | end of validation epoch  91 | time: 63.73s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 936.96 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 100.67s | training loss  0.06 |
    | end of validation epoch  92 | time: 64.66s | validation loss  1.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 919.05 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 99.20s | training loss  0.07 |
    | end of validation epoch  93 | time: 66.22s | validation loss  1.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 931.47 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 99.97s | training loss  0.06 |
    | end of validation epoch  94 | time: 62.27s | validation loss  1.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 927.91 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 99.98s | training loss  0.04 |
    | end of validation epoch  95 | time: 65.51s | validation loss  1.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964, 0.04480791234973449] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774, 1.7665633579017594]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 911.67 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 98.33s | training loss  0.05 |
    | end of validation epoch  96 | time: 63.46s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964, 0.04480791234973449, 0.04501307770811223] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774, 1.7665633579017594, 1.5928821981748722]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 925.53 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 99.99s | training loss  0.06 |
    | end of validation epoch  97 | time: 64.02s | validation loss  1.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964, 0.04480791234973449, 0.04501307770811223, 0.05922581819263664] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774, 1.7665633579017594, 1.5928821981748722, 1.6656135159525245]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 920.15 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 99.03s | training loss  0.05 |
    | end of validation epoch  98 | time: 65.24s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964, 0.04480791234973449, 0.04501307770811223, 0.05922581819263664, 0.05165172017993288] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774, 1.7665633579017594, 1.5928821981748722, 1.6656135159525245, 1.4503142482038431]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 925.72 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 99.41s | training loss  0.05 |
    | end of validation epoch  99 | time: 61.97s | validation loss  1.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [1.6273042495186265, 1.1033582939757958, 0.9308422578347696, 0.7987281787502873, 0.6950644018413784, 0.617218609060253, 0.5909106084355363, 0.5346583820678092, 0.4943598801488275, 0.443330485675786, 0.42128842599220107, 0.3956729532093615, 0.3665508684274313, 0.3580199884133296, 0.31266988008408936, 0.3098629593580693, 0.29938308485187926, 0.2699949395683435, 0.2717615182738046, 0.23875663295253977, 0.23668039053141535, 0.240965163989647, 0.2280471377947309, 0.19274891912937164, 0.20161321108137165, 0.21039976870959942, 0.18898394750850694, 0.1834293087390629, 0.17454915836050705, 0.17184023364438666, 0.17093613038997393, 0.16764013563190494, 0.160239222686033, 0.15030202172226734, 0.1424861449983206, 0.15297441214725777, 0.1348038375176288, 0.1400562542918566, 0.12071420754062699, 0.12267353061821547, 0.12544509750027377, 0.12375720377239557, 0.1234082316157517, 0.11852078396524932, 0.12175626307725906, 0.107082144785169, 0.10908347334679183, 0.10619300268254839, 0.08545536422044844, 0.09800017609990933, 0.09893414780900285, 0.09146396446603912, 0.0913116093892772, 0.08619914207238334, 0.09768882083396117, 0.08702212066163083, 0.08163728393815659, 0.08794858873830186, 0.07644415640079223, 0.08106647561899982, 0.0824172245422462, 0.07735918184557745, 0.07631663526702034, 0.07772167263602889, 0.0738342560424998, 0.06921949977608952, 0.08184535582424016, 0.06409756682376873, 0.07102250666902946, 0.06671472222686887, 0.06243706850737736, 0.06569373222101513, 0.0697979517868376, 0.06909716622652234, 0.054359474777336325, 0.058926670124066306, 0.05582253850560199, 0.057030141789902435, 0.06156773303073269, 0.07006283144693116, 0.060379426695641364, 0.05617071285441115, 0.05542591886242499, 0.06193972230880513, 0.06493104064652512, 0.061454539287936046, 0.04742305854394159, 0.0455539924268787, 0.05754725429489537, 0.058441070447516466, 0.06082465573054579, 0.055637555402443487, 0.06659007976083336, 0.05829547565463964, 0.04480791234973449, 0.04501307770811223, 0.05922581819263664, 0.05165172017993288, 0.05305456717473429] validation loss is  [1.0484119756923367, 0.9369165604778876, 0.9819237341095383, 0.9415962926577777, 1.1032641887431964, 0.9813234562558742, 1.0999132134262861, 1.1384636527557934, 1.1225327368689857, 1.126625101780519, 0.9994418712643286, 1.1205736901029013, 1.0849524091463536, 1.0406499417246475, 1.273786142274427, 1.0227000591888402, 1.3401561504773174, 1.3652974542540808, 1.2318141058979866, 1.2258991071551766, 1.1167652595565112, 1.0450953006511554, 1.089231170869122, 1.2728000175751124, 1.1490824756086415, 1.1191833299599239, 1.277807821056437, 1.2054352166790825, 1.3660402939713094, 1.2636646752386393, 1.2766665725891169, 1.0658595947509941, 1.329179015941918, 1.3008293016658474, 1.2815507509900879, 1.3244653161382303, 1.2899350865094068, 1.1372509005207878, 1.3297288973117247, 1.2142116664132725, 1.2197876709105913, 1.2425373455043882, 1.5136639999303345, 1.372088101823465, 1.5431828192328492, 1.4245582509902306, 1.1919045314425603, 1.1616802422019343, 1.3911837585037574, 1.382481660771494, 1.4354172436481651, 1.3856211366025188, 1.458504713397512, 1.5244966327251557, 1.5667753963231614, 1.442441132933406, 1.3529907585664962, 1.2749940660432912, 1.347662140770505, 1.6554700332938712, 1.4807708969262119, 1.3790362926799087, 1.3015729447264068, 1.4675406312647585, 1.530872695070381, 1.4535349019958328, 1.423753295413917, 1.3778628623113036, 1.5600550374559436, 1.5543541247655714, 1.500186837821578, 1.6561805632906423, 1.4960197864663012, 1.385147986341811, 1.6048634236600872, 1.4356254135685351, 1.5285723530735897, 1.6472420811769553, 1.6240187170915306, 1.574063628523921, 1.637825499868389, 1.7392763919354668, 1.5360399029159453, 1.428035229560919, 1.5242960339577014, 1.546945086098276, 1.5563315017304074, 1.7604580456585002, 1.9037581276691828, 1.6584371547214687, 1.781056170492472, 1.6969035419169813, 1.7116772474643465, 1.7553138015791774, 1.7665633579017594, 1.5928821981748722, 1.6656135159525245, 1.4503142482038431, 1.792425119499967]
