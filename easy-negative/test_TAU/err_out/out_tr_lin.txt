/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1364.87 | loss  2.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 145.96s | training loss  2.88 |
    | end of validation epoch   1 | time: 86.83s | validation loss  2.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.8755685939445152] validation loss is  [2.0624763617912927]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 1202.22 | loss  2.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 129.34s | training loss  2.57 |
    | end of validation epoch   2 | time: 73.79s | validation loss  1.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.8755685939445152, 2.5727600321039423] validation loss is  [2.0624763617912927, 1.8687467575073242]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 1216.36 | loss  2.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 130.80s | training loss  2.38 |
    | end of validation epoch   3 | time: 73.98s | validation loss  1.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 1183.86 | loss  2.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 127.50s | training loss  2.23 |
    | end of validation epoch   4 | time: 78.23s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 1204.18 | loss  2.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 129.02s | training loss  2.09 |
    | end of validation epoch   5 | time: 75.27s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 1198.47 | loss  2.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 128.80s | training loss  2.03 |
    | end of validation epoch   6 | time: 74.97s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1178.10 | loss  1.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 126.90s | training loss  1.99 |
    | end of validation epoch   7 | time: 78.19s | validation loss  1.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1186.96 | loss  1.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 127.71s | training loss  1.89 |
    | end of validation epoch   8 | time: 75.14s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1189.56 | loss  1.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 127.70s | training loss  1.83 |
    | end of validation epoch   9 | time: 74.54s | validation loss  1.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1182.54 | loss  1.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 127.41s | training loss  1.82 |
    | end of validation epoch  10 | time: 75.75s | validation loss  1.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1191.15 | loss  1.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 128.11s | training loss  1.75 |
    | end of validation epoch  11 | time: 74.64s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1181.75 | loss  1.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 127.05s | training loss  1.73 |
    | end of validation epoch  12 | time: 75.17s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1181.27 | loss  2.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 127.31s | training loss  1.71 |
    | end of validation epoch  13 | time: 77.67s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1195.55 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 128.34s | training loss  1.68 |
    | end of validation epoch  14 | time: 74.31s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1187.10 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 128.43s | training loss  1.65 |
    | end of validation epoch  15 | time: 74.07s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1172.74 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 128.68s | training loss  1.63 |
    | end of validation epoch  16 | time: 76.96s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1195.50 | loss  1.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 128.78s | training loss  1.62 |
    | end of validation epoch  17 | time: 74.02s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1167.07 | loss  1.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 125.97s | training loss  1.60 |
    | end of validation epoch  18 | time: 75.78s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 1177.58 | loss  1.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 126.29s | training loss  1.57 |
    | end of validation epoch  19 | time: 76.92s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 1168.00 | loss  1.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 126.38s | training loss  1.57 |
    | end of validation epoch  20 | time: 74.21s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 1186.41 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 128.78s | training loss  1.56 |
    | end of validation epoch  21 | time: 76.57s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1222.11 | loss  1.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 131.57s | training loss  1.53 |
    | end of validation epoch  22 | time: 78.67s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1211.20 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 131.13s | training loss  1.53 |
    | end of validation epoch  23 | time: 75.91s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1193.39 | loss  1.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 129.51s | training loss  1.53 |
    | end of validation epoch  24 | time: 75.73s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433]
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1190.97 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 128.36s | training loss  1.50 |
    | end of validation epoch  25 | time: 78.17s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1207.63 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 129.56s | training loss  1.49 |
    | end of validation epoch  26 | time: 73.21s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1196.64 | loss  1.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 128.77s | training loss  1.46 |
    | end of validation epoch  27 | time: 74.94s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1189.94 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 128.02s | training loss  1.48 |
    | end of validation epoch  28 | time: 76.53s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937]
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1181.73 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 126.99s | training loss  1.49 |
    | end of validation epoch  29 | time: 74.23s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1204.55 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 128.99s | training loss  1.47 |
    | end of validation epoch  30 | time: 75.67s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069]
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1178.69 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 126.44s | training loss  1.46 |
    | end of validation epoch  31 | time: 76.17s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1189.06 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 127.53s | training loss  1.45 |
    | end of validation epoch  32 | time: 73.51s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1174.73 | loss  1.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 126.40s | training loss  1.47 |
    | end of validation epoch  33 | time: 74.86s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1210.09 | loss  1.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 130.41s | training loss  1.47 |
    | end of validation epoch  34 | time: 75.18s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1187.18 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 128.93s | training loss  1.44 |
    | end of validation epoch  35 | time: 74.82s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1168.51 | loss  1.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 125.47s | training loss  1.43 |
    | end of validation epoch  36 | time: 75.17s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1175.70 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 126.40s | training loss  1.45 |
    | end of validation epoch  37 | time: 75.11s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413]
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1173.34 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 125.80s | training loss  1.43 |
    | end of validation epoch  38 | time: 74.51s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 1176.52 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 126.41s | training loss  1.43 |
    | end of validation epoch  39 | time: 73.96s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 1171.27 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 125.90s | training loss  1.43 |
    | end of validation epoch  40 | time: 76.70s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551]
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 1173.47 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 125.91s | training loss  1.41 |
    | end of validation epoch  41 | time: 75.81s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 1175.21 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 126.22s | training loss  1.41 |
    | end of validation epoch  42 | time: 75.39s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 1178.35 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 126.44s | training loss  1.42 |
    | end of validation epoch  43 | time: 76.76s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 1166.45 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 126.28s | training loss  1.42 |
    | end of validation epoch  44 | time: 74.30s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 1168.54 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 125.36s | training loss  1.41 |
    | end of validation epoch  45 | time: 74.70s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1173.14 | loss  1.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 126.05s | training loss  1.41 |
    | end of validation epoch  46 | time: 76.78s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 1160.21 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 124.83s | training loss  1.41 |
    | end of validation epoch  47 | time: 73.98s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 1166.56 | loss  1.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 125.53s | training loss  1.41 |
    | end of validation epoch  48 | time: 76.11s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 1175.31 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 126.14s | training loss  1.40 |
    | end of validation epoch  49 | time: 76.36s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 1168.57 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 126.77s | training loss  1.42 |
    | end of validation epoch  50 | time: 75.01s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 1187.78 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 127.64s | training loss  1.42 |
    | end of validation epoch  51 | time: 75.48s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 1188.21 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 127.64s | training loss  1.39 |
    | end of validation epoch  52 | time: 76.45s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 1188.86 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 127.67s | training loss  1.41 |
    | end of validation epoch  53 | time: 75.99s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 1169.83 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 126.04s | training loss  1.39 |
    | end of validation epoch  54 | time: 70.44s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1088.27 | loss  1.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 117.45s | training loss  1.41 |
    | end of validation epoch  55 | time: 72.41s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 1094.54 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 117.81s | training loss  1.42 |
    | end of validation epoch  56 | time: 71.67s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 1100.47 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 118.18s | training loss  1.40 |
    | end of validation epoch  57 | time: 70.98s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 1098.95 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 117.78s | training loss  1.39 |
    | end of validation epoch  58 | time: 73.64s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 1095.46 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 117.84s | training loss  1.40 |
    | end of validation epoch  59 | time: 71.31s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 1094.31 | loss  1.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 118.68s | training loss  1.40 |
    | end of validation epoch  60 | time: 71.36s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 1093.28 | loss  1.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 117.68s | training loss  1.40 |
    | end of validation epoch  61 | time: 73.12s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 1099.44 | loss  1.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 117.69s | training loss  1.41 |
    | end of validation epoch  62 | time: 72.12s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 1091.76 | loss  1.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 119.24s | training loss  1.40 |
    | end of validation epoch  63 | time: 70.50s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237]
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 1100.24 | loss  1.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 118.17s | training loss  1.41 |
    | end of validation epoch  64 | time: 73.35s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 1112.85 | loss  1.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 119.19s | training loss  1.39 |
    | end of validation epoch  65 | time: 71.43s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 1102.18 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 118.55s | training loss  1.38 |
    | end of validation epoch  66 | time: 70.34s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 1081.82 | loss  1.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 117.62s | training loss  1.39 |
    | end of validation epoch  67 | time: 71.74s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 1099.28 | loss  1.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 118.19s | training loss  1.40 |
    | end of validation epoch  68 | time: 71.46s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 1099.33 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 118.02s | training loss  1.39 |
    | end of validation epoch  69 | time: 70.68s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 1091.36 | loss  1.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 117.99s | training loss  1.38 |
    | end of validation epoch  70 | time: 74.59s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 1240.94 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 132.45s | training loss  1.40 |
    | end of validation epoch  71 | time: 71.66s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 1097.33 | loss  1.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 117.74s | training loss  1.41 |
    | end of validation epoch  72 | time: 71.06s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 1091.83 | loss  1.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 117.52s | training loss  1.40 |
    | end of validation epoch  73 | time: 71.59s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 1093.10 | loss  1.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 117.69s | training loss  1.40 |
    | end of validation epoch  74 | time: 72.38s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 1098.90 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 118.57s | training loss  1.39 |
    | end of validation epoch  75 | time: 72.00s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 1191.13 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 128.60s | training loss  1.39 |
    | end of validation epoch  76 | time: 72.53s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 1100.81 | loss  1.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 118.48s | training loss  1.40 |
    | end of validation epoch  77 | time: 71.35s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 1103.90 | loss  1.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 118.70s | training loss  1.40 |
    | end of validation epoch  78 | time: 70.56s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 1091.50 | loss  1.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 117.33s | training loss  1.38 |
    | end of validation epoch  79 | time: 72.86s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 1094.46 | loss  1.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 117.70s | training loss  1.39 |
    | end of validation epoch  80 | time: 72.25s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 1101.99 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 118.21s | training loss  1.39 |
    | end of validation epoch  81 | time: 72.14s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 1096.75 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 117.63s | training loss  1.41 |
    | end of validation epoch  82 | time: 72.40s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 1103.18 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 118.75s | training loss  1.40 |
    | end of validation epoch  83 | time: 72.03s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 1098.44 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 118.14s | training loss  1.39 |
    | end of validation epoch  84 | time: 70.63s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 1093.16 | loss  1.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 117.36s | training loss  1.38 |
    | end of validation epoch  85 | time: 71.81s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 1085.24 | loss  1.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 118.22s | training loss  1.38 |
    | end of validation epoch  86 | time: 72.06s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1095.78 | loss  1.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 117.82s | training loss  1.39 |
    | end of validation epoch  87 | time: 70.64s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 1091.35 | loss  1.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 116.97s | training loss  1.39 |
    | end of validation epoch  88 | time: 72.62s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 1091.53 | loss  1.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 117.93s | training loss  1.38 |
    | end of validation epoch  89 | time: 72.93s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 1098.16 | loss  1.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 117.64s | training loss  1.39 |
    | end of validation epoch  90 | time: 71.38s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 1096.06 | loss  1.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 118.46s | training loss  1.40 |
    | end of validation epoch  91 | time: 71.73s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 1092.53 | loss  1.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 118.02s | training loss  1.39 |
    | end of validation epoch  92 | time: 71.37s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 1093.98 | loss  1.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 118.03s | training loss  1.39 |
    | end of validation epoch  93 | time: 71.65s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 1090.17 | loss  1.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 117.15s | training loss  1.38 |
    | end of validation epoch  94 | time: 71.83s | validation loss  1.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1088.21 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 117.13s | training loss  1.39 |
    | end of validation epoch  95 | time: 71.46s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502, 1.3888420672030062] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415, 1.1529655974979203]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 1139.85 | loss  1.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 122.28s | training loss  1.39 |
    | end of validation epoch  96 | time: 70.25s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502, 1.3888420672030062, 1.3881670676910125] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415, 1.1529655974979203, 1.1758097472290199]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 1104.31 | loss  1.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 118.67s | training loss  1.39 |
    | end of validation epoch  97 | time: 71.50s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502, 1.3888420672030062, 1.3881670676910125, 1.3913818973678727] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415, 1.1529655974979203, 1.1758097472290199, 1.1763278941313426]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 1094.78 | loss  1.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 117.77s | training loss  1.39 |
    | end of validation epoch  98 | time: 72.40s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502, 1.3888420672030062, 1.3881670676910125, 1.3913818973678727, 1.390781714035584] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415, 1.1529655974979203, 1.1758097472290199, 1.1763278941313426, 1.1817878689616919]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 1099.19 | loss  1.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 118.75s | training loss  1.38 |
    | end of validation epoch  99 | time: 70.89s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.8755685939445152, 2.5727600321039423, 2.378971645423958, 2.2335058107032433, 2.092255554757677, 2.032723802703995, 1.9901037710207004, 1.8858730213062183, 1.8349105914433796, 1.8175722972766772, 1.7525559653033007, 1.7315804539500057, 1.7075043347504761, 1.6809503913999677, 1.6542900972538166, 1.6259019643336803, 1.6161164603791796, 1.5982257750657227, 1.5684402483003634, 1.5729676409884616, 1.5574026977693713, 1.5259829501847963, 1.526946750847069, 1.5273157648138098, 1.5023743955938667, 1.4875944584339589, 1.4649372712985889, 1.481946636964609, 1.4878434254242494, 1.4692393423200727, 1.455891428767024, 1.4505328444747236, 1.467204670648317, 1.4695118343507922, 1.4438177970078614, 1.4335924838040326, 1.4457982359705746, 1.4314750196697477, 1.4306227699056402, 1.4347476572603792, 1.4081176830841615, 1.4060420764459145, 1.423535864632409, 1.4221010476619274, 1.4102888612059858, 1.4127610374141384, 1.4139010487376034, 1.4079326831542693, 1.4043503774178994, 1.4169660312635404, 1.4159777497386072, 1.3941402800448306, 1.4081049588349488, 1.3927056080586202, 1.409264584919354, 1.420617005846522, 1.4046965203843675, 1.3901024326547846, 1.4044905256580662, 1.4049913002564027, 1.3984712877789058, 1.4113984720127002, 1.400024778134114, 1.4078917943679534, 1.393189532262785, 1.382944594632398, 1.390036157659582, 1.3970380065677401, 1.3876797830736316, 1.3848768859296232, 1.3950956415485691, 1.4132690580041558, 1.3979663913314406, 1.3956806455646549, 1.390571359041575, 1.3874794835442896, 1.4009459824175448, 1.3981766389296935, 1.382726731601062, 1.388628319577054, 1.393218858821972, 1.409621497532269, 1.395152893152323, 1.3944892722207147, 1.3804325187528454, 1.3833755351401664, 1.3859487213529982, 1.3881950142147306, 1.3779114903630436, 1.3939149938188158, 1.4007292996655714, 1.3870476183590588, 1.3879145287178658, 1.3835099280417502, 1.3888420672030062, 1.3881670676910125, 1.3913818973678727, 1.390781714035584, 1.379136417363141] validation loss is  [2.0624763617912927, 1.8687467575073242, 1.734134629368782, 1.6283444439371426, 1.5748709328472614, 1.5009905397891998, 1.476985141634941, 1.4376342470447223, 1.3996710119148095, 1.381595400472482, 1.3716965429484844, 1.3500432732204597, 1.3342848047614098, 1.3210980109870434, 1.3091502549747627, 1.3077158915499847, 1.2942924251159031, 1.281071190411846, 1.2856222012390692, 1.2680279432485502, 1.2743537134180467, 1.2593670686086018, 1.247961130614082, 1.249953044578433, 1.243725998327136, 1.246237799525261, 1.222876553113262, 1.243510941353937, 1.220940303678314, 1.2334518606464069, 1.2278524904201429, 1.2306976926823456, 1.2225429241855938, 1.213550517335534, 1.2119804347554843, 1.2074234429746866, 1.2336175901194413, 1.1977840509886544, 1.2102478152761857, 1.209877518626551, 1.2093157038713496, 1.2080060293277104, 1.1989957025895517, 1.184920343880852, 1.1875348171840112, 1.1946514137089252, 1.2050244292865198, 1.1924748836706083, 1.1903329839309056, 1.1958032219360273, 1.1916034392391641, 1.1984250533084075, 1.2069697100669146, 1.1859861379489303, 1.1890466992432873, 1.1753706131130457, 1.1840658740450938, 1.1957489134122927, 1.1953184405962627, 1.1782078131412466, 1.1775764028231304, 1.183490173580746, 1.176953725827237, 1.1857651136815548, 1.1773482641826074, 1.1639244783048828, 1.1894155517220497, 1.1762390440950792, 1.1666033609459798, 1.1750358908126752, 1.1765801950047414, 1.19216021305571, 1.1670912482465308, 1.1644830266013741, 1.1700779392073553, 1.1760129273558657, 1.176224620391925, 1.1837141563495, 1.1758589847013354, 1.1858604475855827, 1.16088407424589, 1.1774051214257877, 1.1659203823655844, 1.16783847194165, 1.1683552054067452, 1.1738302918771903, 1.176974789549907, 1.176447922674318, 1.1665388752395909, 1.1754195084795356, 1.1754243082056444, 1.158813654134671, 1.1711503760889173, 1.1622318293278415, 1.1529655974979203, 1.1758097472290199, 1.1763278941313426, 1.1817878689616919, 1.1739514839525025]
