/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 8, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 1260.13 | loss  1.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 139.15s | training loss  1.62 |
    | end of validation epoch   1 | time: 77.71s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [1.623363066363979] validation loss is  [1.1276272661052644]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 2114.94 | loss  1.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 227.27s | training loss  1.12 |
    | end of validation epoch   2 | time: 93.21s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [1.623363066363979, 1.116403447078155] validation loss is  [1.1276272661052644, 1.0862976759672165]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 2390.59 | loss  0.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 254.85s | training loss  0.95 |
    | end of validation epoch   3 | time: 91.53s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 2066.24 | loss  0.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 220.60s | training loss  0.83 |
    | end of validation epoch   4 | time: 107.92s | validation loss  1.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 2367.38 | loss  0.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 252.06s | training loss  0.76 |
    | end of validation epoch   5 | time: 98.36s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 2466.53 | loss  0.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 261.75s | training loss  0.67 |
    | end of validation epoch   6 | time: 113.24s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 1913.84 | loss  0.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 199.66s | training loss  0.61 |
    | end of validation epoch   7 | time: 77.24s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 1152.67 | loss  0.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 123.84s | training loss  0.56 |
    | end of validation epoch   8 | time: 72.82s | validation loss  1.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 1148.72 | loss  0.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 124.18s | training loss  0.53 |
    | end of validation epoch   9 | time: 76.90s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 1152.86 | loss  0.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 123.89s | training loss  0.47 |
    | end of validation epoch  10 | time: 76.75s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 1152.81 | loss  0.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 124.06s | training loss  0.44 |
    | end of validation epoch  11 | time: 75.46s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 1141.76 | loss  0.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 123.20s | training loss  0.40 |
    | end of validation epoch  12 | time: 77.40s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 1154.25 | loss  0.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 123.82s | training loss  0.39 |
    | end of validation epoch  13 | time: 75.11s | validation loss  1.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 1152.84 | loss  0.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 124.07s | training loss  0.35 |
    | end of validation epoch  14 | time: 76.60s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 1156.17 | loss  0.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 123.94s | training loss  0.34 |
    | end of validation epoch  15 | time: 73.74s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 1148.39 | loss  0.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 123.17s | training loss  0.33 |
    | end of validation epoch  16 | time: 75.74s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 1162.95 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 124.85s | training loss  0.30 |
    | end of validation epoch  17 | time: 74.77s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 1723.31 | loss  0.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 187.42s | training loss  0.31 |
    | end of validation epoch  18 | time: 99.86s | validation loss  1.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163]
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 2435.18 | loss  0.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 260.69s | training loss  0.28 |
    | end of validation epoch  19 | time: 95.23s | validation loss  1.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 2353.76 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 251.49s | training loss  0.26 |
    | end of validation epoch  20 | time: 102.26s | validation loss  1.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 2426.64 | loss  0.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 257.12s | training loss  0.28 |
    | end of validation epoch  21 | time: 85.21s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548]
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 1166.58 | loss  0.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 125.71s | training loss  0.25 |
    | end of validation epoch  22 | time: 74.84s | validation loss  1.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 1173.65 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 125.98s | training loss  0.24 |
    | end of validation epoch  23 | time: 72.94s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 1170.97 | loss  0.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 126.16s | training loss  0.23 |
    | end of validation epoch  24 | time: 78.51s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 1166.66 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 125.14s | training loss  0.22 |
    | end of validation epoch  25 | time: 74.27s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 1157.47 | loss  0.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 125.70s | training loss  0.22 |
    | end of validation epoch  26 | time: 74.24s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819]
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 1162.78 | loss  0.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 124.75s | training loss  0.20 |
    | end of validation epoch  27 | time: 76.66s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 1160.12 | loss  0.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 124.59s | training loss  0.19 |
    | end of validation epoch  28 | time: 74.30s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 1221.20 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 131.80s | training loss  0.20 |
    | end of validation epoch  29 | time: 77.19s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297]
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 1153.76 | loss  0.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 124.36s | training loss  0.20 |
    | end of validation epoch  30 | time: 74.20s | validation loss  1.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278]
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 1162.01 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 125.33s | training loss  0.17 |
    | end of validation epoch  31 | time: 77.28s | validation loss  1.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 1154.50 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 124.48s | training loss  0.16 |
    | end of validation epoch  32 | time: 76.58s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 1157.89 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 124.45s | training loss  0.16 |
    | end of validation epoch  33 | time: 74.36s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 1154.07 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 124.18s | training loss  0.16 |
    | end of validation epoch  34 | time: 75.06s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924]
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 1144.15 | loss  0.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 123.37s | training loss  0.15 |
    | end of validation epoch  35 | time: 73.80s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 1159.87 | loss  0.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 124.53s | training loss  0.14 |
    | end of validation epoch  36 | time: 78.58s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 1154.86 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 124.17s | training loss  0.16 |
    | end of validation epoch  37 | time: 74.75s | validation loss  1.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686]
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 1346.44 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 150.27s | training loss  0.15 |
    | end of validation epoch  38 | time: 100.46s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194]
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 2444.39 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 258.12s | training loss  0.14 |
    | end of validation epoch  39 | time: 98.70s | validation loss  1.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 2391.35 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 258.09s | training loss  0.14 |
    | end of validation epoch  40 | time: 100.46s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802]
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 1174.94 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 126.11s | training loss  0.13 |
    | end of validation epoch  41 | time: 76.45s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 1184.92 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 127.17s | training loss  0.14 |
    | end of validation epoch  42 | time: 74.66s | validation loss  1.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 1163.55 | loss  0.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 125.63s | training loss  0.13 |
    | end of validation epoch  43 | time: 79.23s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 1181.11 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 127.75s | training loss  0.13 |
    | end of validation epoch  44 | time: 75.36s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 1167.01 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 125.88s | training loss  0.13 |
    | end of validation epoch  45 | time: 77.22s | validation loss  1.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 1162.25 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 126.08s | training loss  0.12 |
    | end of validation epoch  46 | time: 76.07s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 1169.36 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 126.02s | training loss  0.11 |
    | end of validation epoch  47 | time: 73.91s | validation loss  1.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 1163.80 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 125.03s | training loss  0.12 |
    | end of validation epoch  48 | time: 77.01s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 2023.46 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 216.97s | training loss  0.12 |
    | end of validation epoch  49 | time: 105.86s | validation loss  1.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645]
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 2217.66 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 236.68s | training loss  0.11 |
    | end of validation epoch  50 | time: 106.48s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 1809.03 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 189.50s | training loss  0.11 |
    | end of validation epoch  51 | time: 75.74s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 1167.03 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 126.39s | training loss  0.11 |
    | end of validation epoch  52 | time: 75.80s | validation loss  1.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 1163.43 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 125.61s | training loss  0.11 |
    | end of validation epoch  53 | time: 77.84s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 1170.30 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 125.63s | training loss  0.10 |
    | end of validation epoch  54 | time: 75.35s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 1176.11 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 126.06s | training loss  0.08 |
    | end of validation epoch  55 | time: 78.50s | validation loss  1.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 1157.33 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 124.79s | training loss  0.09 |
    | end of validation epoch  56 | time: 75.51s | validation loss  1.29 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891]
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 1167.73 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 125.66s | training loss  0.10 |
    | end of validation epoch  57 | time: 75.60s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 1180.78 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 126.63s | training loss  0.10 |
    | end of validation epoch  58 | time: 76.53s | validation loss  1.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 1165.77 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 125.84s | training loss  0.09 |
    | end of validation epoch  59 | time: 74.42s | validation loss  1.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 1156.83 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 124.45s | training loss  0.08 |
    | end of validation epoch  60 | time: 78.97s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 1158.73 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 124.36s | training loss  0.08 |
    | end of validation epoch  61 | time: 74.00s | validation loss  1.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 1169.72 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 127.01s | training loss  0.08 |
    | end of validation epoch  62 | time: 78.66s | validation loss  1.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 1803.13 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 197.81s | training loss  0.09 |
    | end of validation epoch  63 | time: 99.57s | validation loss  1.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365]
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 2469.18 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 262.69s | training loss  0.08 |
    | end of validation epoch  64 | time: 106.26s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 2146.82 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 229.81s | training loss  0.08 |
    | end of validation epoch  65 | time: 105.71s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 2489.98 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 265.48s | training loss  0.08 |
    | end of validation epoch  66 | time: 103.45s | validation loss  1.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 2178.25 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 226.67s | training loss  0.08 |
    | end of validation epoch  67 | time: 76.46s | validation loss  1.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 1168.74 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 125.46s | training loss  0.07 |
    | end of validation epoch  68 | time: 73.43s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 1173.53 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 125.81s | training loss  0.07 |
    | end of validation epoch  69 | time: 76.23s | validation loss  1.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 1161.35 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 125.08s | training loss  0.08 |
    | end of validation epoch  70 | time: 76.13s | validation loss  1.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196]
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 1165.41 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 125.87s | training loss  0.07 |
    | end of validation epoch  71 | time: 76.62s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 1160.01 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 124.77s | training loss  0.07 |
    | end of validation epoch  72 | time: 76.86s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 1167.48 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 125.72s | training loss  0.06 |
    | end of validation epoch  73 | time: 81.82s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 2453.80 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 263.27s | training loss  0.07 |
    | end of validation epoch  74 | time: 101.43s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 2161.75 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 233.77s | training loss  0.08 |
    | end of validation epoch  75 | time: 104.21s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 2482.92 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 264.35s | training loss  0.06 |
    | end of validation epoch  76 | time: 100.80s | validation loss  1.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 2347.68 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 243.23s | training loss  0.07 |
    | end of validation epoch  77 | time: 76.87s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 1215.54 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 130.29s | training loss  0.06 |
    | end of validation epoch  78 | time: 76.64s | validation loss  1.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 1161.91 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 125.67s | training loss  0.06 |
    | end of validation epoch  79 | time: 76.68s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 1168.19 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 125.64s | training loss  0.07 |
    | end of validation epoch  80 | time: 76.95s | validation loss  1.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 1165.54 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 125.22s | training loss  0.07 |
    | end of validation epoch  81 | time: 76.71s | validation loss  1.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 1177.41 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 126.18s | training loss  0.07 |
    | end of validation epoch  82 | time: 73.79s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 1169.59 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 126.00s | training loss  0.06 |
    | end of validation epoch  83 | time: 76.04s | validation loss  1.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 1162.91 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 124.93s | training loss  0.08 |
    | end of validation epoch  84 | time: 77.88s | validation loss  1.36 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 1168.02 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 125.51s | training loss  0.06 |
    | end of validation epoch  85 | time: 74.98s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 1167.55 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 125.39s | training loss  0.06 |
    | end of validation epoch  86 | time: 77.45s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 1173.61 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 125.63s | training loss  0.05 |
    | end of validation epoch  87 | time: 76.81s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 1175.55 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 126.32s | training loss  0.07 |
    | end of validation epoch  88 | time: 77.56s | validation loss  1.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 1169.46 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 125.56s | training loss  0.07 |
    | end of validation epoch  89 | time: 77.39s | validation loss  1.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689]
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 1178.33 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 126.52s | training loss  0.06 |
    | end of validation epoch  90 | time: 73.61s | validation loss  1.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 1201.07 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 132.51s | training loss  0.06 |
    | end of validation epoch  91 | time: 75.08s | validation loss  1.48 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 1184.90 | loss  0.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 127.16s | training loss  0.05 |
    | end of validation epoch  92 | time: 75.48s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 1174.74 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 125.91s | training loss  0.04 |
    | end of validation epoch  93 | time: 76.72s | validation loss  1.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 1164.88 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 125.41s | training loss  0.05 |
    | end of validation epoch  94 | time: 77.17s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 1168.26 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 125.63s | training loss  0.06 |
    | end of validation epoch  95 | time: 73.76s | validation loss  1.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692, 0.05700861996979461] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688, 1.5197171786421677]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 1160.51 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 127.48s | training loss  0.05 |
    | end of validation epoch  96 | time: 76.89s | validation loss  1.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692, 0.05700861996979461, 0.05407559752531417] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688, 1.5197171786421677, 1.5690756545385132]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 1169.46 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 125.69s | training loss  0.05 |
    | end of validation epoch  97 | time: 75.60s | validation loss  1.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692, 0.05700861996979461, 0.05407559752531417, 0.05028885196231574] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688, 1.5197171786421677, 1.5690756545385132, 1.9613372903428778]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 1170.48 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 126.54s | training loss  0.05 |
    | end of validation epoch  98 | time: 76.18s | validation loss  1.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692, 0.05700861996979461, 0.05407559752531417, 0.05028885196231574, 0.054617888185086554] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688, 1.5197171786421677, 1.5690756545385132, 1.9613372903428778, 1.6695243914694327]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 1167.42 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 125.90s | training loss  0.05 |
    | end of validation epoch  99 | time: 79.48s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [1.623363066363979, 1.116403447078155, 0.953104346722096, 0.8277945636628984, 0.7597188611288328, 0.6730385895247932, 0.6132186417644089, 0.5598272050286198, 0.5274070608723271, 0.4658240133577639, 0.44256228584427015, 0.40351961955830856, 0.38655920944235345, 0.3489727481259956, 0.3435345674688752, 0.330784234579082, 0.3025322180610519, 0.3054407742377874, 0.28084061449175485, 0.26069140219473624, 0.279823616281286, 0.25164911694623326, 0.2423770527044932, 0.2254954758393872, 0.21897802787186862, 0.2238305129849159, 0.196856484488324, 0.18990708860728117, 0.19879314122167793, 0.1978359613064173, 0.17340911092521907, 0.16069604602415818, 0.15743262569109598, 0.16224616390090804, 0.14918662906364277, 0.14223111024847976, 0.15538031238693376, 0.1452678264294927, 0.13707846446751473, 0.14248473855020763, 0.12801893337352857, 0.13875666433559344, 0.13320915458035898, 0.12723068973502596, 0.13195836488660928, 0.11777373896660032, 0.10917410680705363, 0.11629507306392665, 0.12208897751328107, 0.11193527059780585, 0.10679179535725632, 0.10544037953153387, 0.10628899649993793, 0.09870368967185149, 0.08491791118574035, 0.09181403122036844, 0.09887858716821349, 0.0971443210602612, 0.09004150430025819, 0.07953948755790521, 0.08238114285710696, 0.08453194570501109, 0.09092008166417882, 0.0790437758807932, 0.0794935156717091, 0.08394671112366088, 0.08150503936282417, 0.07180809727156753, 0.06950540803708471, 0.08138180493905738, 0.07043736516120466, 0.07138412399637001, 0.06456586701778678, 0.07038792473608041, 0.07747238869401249, 0.061683583987859995, 0.0659230260413375, 0.05856262607214687, 0.058758900429516495, 0.07027771785385437, 0.07089624140696886, 0.07076970174875077, 0.06075600520414007, 0.08037775733113826, 0.06279570968435691, 0.05783474030443006, 0.04717004005923062, 0.06623364187072257, 0.06587253677918836, 0.059970709409245905, 0.05760863412859606, 0.054207451542973656, 0.04401008630809081, 0.04644687280377692, 0.05700861996979461, 0.05407559752531417, 0.05028885196231574, 0.054617888185086554, 0.05343639940392595] validation loss is  [1.1276272661052644, 1.0862976759672165, 1.0384703056576352, 1.0911940566341702, 1.2065877209582443, 1.078786941167588, 1.0749630815116689, 1.0981022204311255, 1.1261609942885116, 1.1853107739783202, 1.070694033425146, 1.1312341169978026, 1.1460944686938699, 1.2524355876957998, 1.1223761473277893, 1.1364425507199485, 1.0730800452026112, 1.115528402326163, 1.2186135203034307, 1.0842804682906717, 1.1875852006487548, 1.041607111189175, 1.2050517385359854, 1.1756806503951036, 1.1977049796356976, 1.2918894950028819, 1.230160797600547, 1.3871981628083934, 1.2477810685086297, 1.1959464610554278, 1.129313499553973, 1.2585551352240145, 1.1866928791520575, 1.2769530817846924, 1.302624881715019, 1.1680301671440247, 1.0743481297346686, 1.2883010470541194, 1.5483911127618437, 1.268642866925802, 1.1776669395088295, 1.1363643842972426, 1.3110529908687265, 1.428100030689772, 1.18376950796907, 1.2448950455485222, 1.4084924503331422, 1.5554170841351151, 1.3255251279624645, 1.2525851631119924, 1.6245381166202908, 1.295906470502814, 1.341796681037522, 1.2356516119665077, 1.31155796600918, 1.290390921877891, 1.2076975913563122, 1.2841746223712107, 1.477014296668737, 1.5186912900632403, 1.2474695850551143, 1.3455048518953845, 1.4676475923964365, 1.4468068523904851, 1.6255093906753852, 1.3223521634160231, 1.6917325842938833, 1.5132166198357784, 1.6191972313948402, 1.4424413438828196, 1.4576303876334957, 1.501454815188481, 1.6775670756954544, 1.540487144278207, 1.5286302592606564, 1.5125330064523343, 1.58750002184388, 1.53165390752838, 1.784287066189184, 1.4592781313500989, 1.4521409066310298, 1.5939430665554635, 1.9357705189128562, 1.3616414728567179, 1.4887492751634757, 1.566439855499387, 1.6032199528538815, 1.757172240584623, 1.6905812106730689, 1.3873760050046258, 1.4835380768169368, 1.5957718013814883, 1.5005756058471889, 1.6317930564012688, 1.5197171786421677, 1.5690756545385132, 1.9613372903428778, 1.6695243914694327, 1.5909828386114289]
