{'debug': False, 'num_workers': 4, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'data_path': '../../../TAU-urban-audio-visual-scenes-2021-development/', 'video_clip_duration': 10, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 10, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'model_type': 'audio', 'num_classes': 10, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
use_cude True
Directory  ./audio_model/  Created 
use_cude True
-----------start training
this is epoch 1
| epoch   1 |   100/  111 batches | ms/batch 6262.24 | loss  1.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 684.40s | training loss  2.02 |
    | end of validation epoch   1 | time: 144.36s | validation loss  1.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [2.022422927993912] validation loss is  [1.5445711699624856]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  111 batches | ms/batch 4606.46 | loss  1.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 513.87s | training loss  1.34 |
    | end of validation epoch   2 | time: 119.54s | validation loss  1.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [2.022422927993912, 1.3430059482385446] validation loss is  [1.5445711699624856, 1.260857407624523]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  111 batches | ms/batch 4378.73 | loss  0.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 489.32s | training loss  1.15 |
    | end of validation epoch   3 | time: 116.33s | validation loss  1.34 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  111 batches | ms/batch 4509.04 | loss  0.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 508.17s | training loss  1.03 |
    | end of validation epoch   4 | time: 128.44s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  111 batches | ms/batch 5166.44 | loss  0.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 578.89s | training loss  0.90 |
    | end of validation epoch   5 | time: 164.17s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  111 batches | ms/batch 4712.33 | loss  0.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 518.02s | training loss  0.81 |
    | end of validation epoch   6 | time: 107.18s | validation loss  1.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  111 batches | ms/batch 4575.94 | loss  0.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 512.17s | training loss  0.70 |
    | end of validation epoch   7 | time: 127.62s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  111 batches | ms/batch 4494.61 | loss  0.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 503.79s | training loss  0.64 |
    | end of validation epoch   8 | time: 126.58s | validation loss  1.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  111 batches | ms/batch 4884.93 | loss  0.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 548.80s | training loss  0.57 |
    | end of validation epoch   9 | time: 130.10s | validation loss  1.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  111 batches | ms/batch 5405.05 | loss  0.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 606.34s | training loss  0.50 |
    | end of validation epoch  10 | time: 157.74s | validation loss  1.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  111 batches | ms/batch 5114.30 | loss  0.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 582.49s | training loss  0.42 |
    | end of validation epoch  11 | time: 154.20s | validation loss  1.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  111 batches | ms/batch 5423.20 | loss  0.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 601.39s | training loss  0.37 |
    | end of validation epoch  12 | time: 124.92s | validation loss  1.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  111 batches | ms/batch 5162.58 | loss  0.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 588.08s | training loss  0.35 |
    | end of validation epoch  13 | time: 112.94s | validation loss  1.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  111 batches | ms/batch 5099.07 | loss  0.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 581.71s | training loss  0.29 |
    | end of validation epoch  14 | time: 150.08s | validation loss  1.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  111 batches | ms/batch 5253.05 | loss  0.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 584.68s | training loss  0.27 |
    | end of validation epoch  15 | time: 152.52s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  111 batches | ms/batch 5294.89 | loss  0.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 582.13s | training loss  0.23 |
    | end of validation epoch  16 | time: 130.94s | validation loss  1.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  111 batches | ms/batch 4963.94 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 556.39s | training loss  0.22 |
    | end of validation epoch  17 | time: 135.12s | validation loss  1.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  111 batches | ms/batch 4816.39 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 544.27s | training loss  0.17 |
    | end of validation epoch  18 | time: 125.46s | validation loss  1.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  111 batches | ms/batch 4957.75 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 545.06s | training loss  0.16 |
    | end of validation epoch  19 | time: 114.79s | validation loss  1.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  111 batches | ms/batch 4765.17 | loss  0.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 522.16s | training loss  0.12 |
    | end of validation epoch  20 | time: 125.00s | validation loss  1.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  111 batches | ms/batch 4718.21 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 529.47s | training loss  0.13 |
    | end of validation epoch  21 | time: 127.31s | validation loss  1.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575]
this is epoch 22
| epoch  22 |   100/  111 batches | ms/batch 4901.51 | loss  0.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 546.93s | training loss  0.12 |
    | end of validation epoch  22 | time: 113.29s | validation loss  1.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  111 batches | ms/batch 4932.84 | loss  0.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 546.81s | training loss  0.11 |
    | end of validation epoch  23 | time: 129.45s | validation loss  1.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  111 batches | ms/batch 4910.41 | loss  0.16 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 547.37s | training loss  0.09 |
    | end of validation epoch  24 | time: 123.34s | validation loss  1.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  111 batches | ms/batch 5576.47 | loss  0.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 601.12s | training loss  0.08 |
    | end of validation epoch  25 | time: 122.30s | validation loss  1.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  111 batches | ms/batch 4993.14 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 559.85s | training loss  0.08 |
    | end of validation epoch  26 | time: 149.80s | validation loss  1.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626]
this is epoch 27
| epoch  27 |   100/  111 batches | ms/batch 5522.74 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 606.11s | training loss  0.07 |
    | end of validation epoch  27 | time: 150.08s | validation loss  1.92 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  111 batches | ms/batch 5635.65 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 627.02s | training loss  0.07 |
    | end of validation epoch  28 | time: 134.38s | validation loss  1.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  111 batches | ms/batch 4966.73 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 556.55s | training loss  0.07 |
    | end of validation epoch  29 | time: 143.89s | validation loss  2.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  111 batches | ms/batch 5121.49 | loss  0.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 569.25s | training loss  0.07 |
    | end of validation epoch  30 | time: 140.24s | validation loss  2.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  111 batches | ms/batch 5128.80 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 572.74s | training loss  0.07 |
    | end of validation epoch  31 | time: 136.09s | validation loss  1.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  111 batches | ms/batch 6137.39 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 675.56s | training loss  0.08 |
    | end of validation epoch  32 | time: 127.55s | validation loss  1.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267]
this is epoch 33
| epoch  33 |   100/  111 batches | ms/batch 4966.67 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 554.12s | training loss  0.07 |
    | end of validation epoch  33 | time: 129.91s | validation loss  1.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317]
this is epoch 34
| epoch  34 |   100/  111 batches | ms/batch 5021.01 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 560.49s | training loss  0.06 |
    | end of validation epoch  34 | time: 147.86s | validation loss  1.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  111 batches | ms/batch 5458.56 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 601.18s | training loss  0.05 |
    | end of validation epoch  35 | time: 133.39s | validation loss  1.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  111 batches | ms/batch 5494.86 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 613.80s | training loss  0.05 |
    | end of validation epoch  36 | time: 142.92s | validation loss  2.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  111 batches | ms/batch 5132.41 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 578.66s | training loss  0.05 |
    | end of validation epoch  37 | time: 138.23s | validation loss  2.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165]
this is epoch 38
| epoch  38 |   100/  111 batches | ms/batch 5187.11 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 579.01s | training loss  0.05 |
    | end of validation epoch  38 | time: 141.53s | validation loss  1.94 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  111 batches | ms/batch 5698.05 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 628.88s | training loss  0.04 |
    | end of validation epoch  39 | time: 130.11s | validation loss  2.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  111 batches | ms/batch 5147.71 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 574.17s | training loss  0.03 |
    | end of validation epoch  40 | time: 133.94s | validation loss  2.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  111 batches | ms/batch 5095.78 | loss  0.05 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 574.06s | training loss  0.04 |
    | end of validation epoch  41 | time: 144.54s | validation loss  1.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216]
this is epoch 42
| epoch  42 |   100/  111 batches | ms/batch 5428.00 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 600.93s | training loss  0.04 |
    | end of validation epoch  42 | time: 144.34s | validation loss  2.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471]
this is epoch 43
| epoch  43 |   100/  111 batches | ms/batch 5447.28 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 609.00s | training loss  0.04 |
    | end of validation epoch  43 | time: 148.96s | validation loss  2.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692]
this is epoch 44
| epoch  44 |   100/  111 batches | ms/batch 5207.10 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 570.90s | training loss  0.03 |
    | end of validation epoch  44 | time: 138.99s | validation loss  1.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  111 batches | ms/batch 5093.10 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 570.73s | training loss  0.04 |
    | end of validation epoch  45 | time: 136.32s | validation loss  2.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644]
this is epoch 46
| epoch  46 |   100/  111 batches | ms/batch 5482.34 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 603.24s | training loss  0.06 |
    | end of validation epoch  46 | time: 128.74s | validation loss  1.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564]
this is epoch 47
| epoch  47 |   100/  111 batches | ms/batch 5517.28 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 617.40s | training loss  0.05 |
    | end of validation epoch  47 | time: 142.22s | validation loss  2.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267]
this is epoch 48
| epoch  48 |   100/  111 batches | ms/batch 5525.81 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 615.38s | training loss  0.04 |
    | end of validation epoch  48 | time: 141.75s | validation loss  2.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742]
this is epoch 49
| epoch  49 |   100/  111 batches | ms/batch 5210.29 | loss  0.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 580.75s | training loss  0.04 |
    | end of validation epoch  49 | time: 138.85s | validation loss  2.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067]
this is epoch 50
| epoch  50 |   100/  111 batches | ms/batch 5520.73 | loss  0.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 620.11s | training loss  0.05 |
    | end of validation epoch  50 | time: 141.80s | validation loss  2.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236]
this is epoch 51
| epoch  51 |   100/  111 batches | ms/batch 5520.83 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 620.05s | training loss  0.04 |
    | end of validation epoch  51 | time: 142.22s | validation loss  1.84 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825]
this is epoch 52
| epoch  52 |   100/  111 batches | ms/batch 5489.18 | loss  0.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 615.42s | training loss  0.04 |
    | end of validation epoch  52 | time: 142.64s | validation loss  2.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975]
this is epoch 53
| epoch  53 |   100/  111 batches | ms/batch 5511.86 | loss  0.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 605.71s | training loss  0.03 |
    | end of validation epoch  53 | time: 136.38s | validation loss  2.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264]
this is epoch 54
| epoch  54 |   100/  111 batches | ms/batch 5531.88 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 602.29s | training loss  0.03 |
    | end of validation epoch  54 | time: 141.69s | validation loss  1.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893]
this is epoch 55
| epoch  55 |   100/  111 batches | ms/batch 5175.12 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 584.02s | training loss  0.03 |
    | end of validation epoch  55 | time: 158.06s | validation loss  1.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454]
this is epoch 56
| epoch  56 |   100/  111 batches | ms/batch 5125.87 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 581.32s | training loss  0.02 |
    | end of validation epoch  56 | time: 150.70s | validation loss  2.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  111 batches | ms/batch 5359.93 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 605.70s | training loss  0.03 |
    | end of validation epoch  57 | time: 127.35s | validation loss  2.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265]
this is epoch 58
| epoch  58 |   100/  111 batches | ms/batch 5397.11 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 595.83s | training loss  0.03 |
    | end of validation epoch  58 | time: 129.85s | validation loss  2.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106]
this is epoch 59
| epoch  59 |   100/  111 batches | ms/batch 5455.51 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 591.41s | training loss  0.02 |
    | end of validation epoch  59 | time: 140.35s | validation loss  2.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719]
this is epoch 60
| epoch  60 |   100/  111 batches | ms/batch 5424.85 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 604.60s | training loss  0.06 |
    | end of validation epoch  60 | time: 147.15s | validation loss  2.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084]
this is epoch 61
| epoch  61 |   100/  111 batches | ms/batch 5812.08 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 639.39s | training loss  0.04 |
    | end of validation epoch  61 | time: 146.22s | validation loss  2.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534]
this is epoch 62
| epoch  62 |   100/  111 batches | ms/batch 5513.50 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 610.35s | training loss  0.02 |
    | end of validation epoch  62 | time: 137.90s | validation loss  2.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695]
this is epoch 63
| epoch  63 |   100/  111 batches | ms/batch 5272.16 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 597.30s | training loss  0.01 |
    | end of validation epoch  63 | time: 150.32s | validation loss  2.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  111 batches | ms/batch 5491.82 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 617.53s | training loss  0.03 |
    | end of validation epoch  64 | time: 151.38s | validation loss  2.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349]
this is epoch 65
| epoch  65 |   100/  111 batches | ms/batch 5909.04 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 644.98s | training loss  0.06 |
    | end of validation epoch  65 | time: 132.99s | validation loss  2.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399]
this is epoch 66
| epoch  66 |   100/  111 batches | ms/batch 5705.36 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 633.32s | training loss  0.03 |
    | end of validation epoch  66 | time: 143.66s | validation loss  2.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904]
this is epoch 67
| epoch  67 |   100/  111 batches | ms/batch 5359.06 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 595.02s | training loss  0.02 |
    | end of validation epoch  67 | time: 161.61s | validation loss  2.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393]
this is epoch 68
| epoch  68 |   100/  111 batches | ms/batch 5649.85 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 627.49s | training loss  0.02 |
    | end of validation epoch  68 | time: 163.50s | validation loss  2.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786]
this is epoch 69
| epoch  69 |   100/  111 batches | ms/batch 6050.06 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 671.51s | training loss  0.02 |
    | end of validation epoch  69 | time: 134.10s | validation loss  2.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644]
this is epoch 70
| epoch  70 |   100/  111 batches | ms/batch 5312.08 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 595.59s | training loss  0.01 |
    | end of validation epoch  70 | time: 160.91s | validation loss  2.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  111 batches | ms/batch 5363.18 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 590.57s | training loss  0.01 |
    | end of validation epoch  71 | time: 152.73s | validation loss  2.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004]
this is epoch 72
| epoch  72 |   100/  111 batches | ms/batch 5680.28 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 626.28s | training loss  0.03 |
    | end of validation epoch  72 | time: 137.56s | validation loss  3.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876]
this is epoch 73
| epoch  73 |   100/  111 batches | ms/batch 5483.27 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 619.83s | training loss  0.04 |
    | end of validation epoch  73 | time: 137.11s | validation loss  2.27 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247]
this is epoch 74
| epoch  74 |   100/  111 batches | ms/batch 5369.07 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 590.42s | training loss  0.03 |
    | end of validation epoch  74 | time: 134.10s | validation loss  2.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575]
this is epoch 75
| epoch  75 |   100/  111 batches | ms/batch 5339.34 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 598.13s | training loss  0.03 |
    | end of validation epoch  75 | time: 163.62s | validation loss  2.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048]
this is epoch 76
| epoch  76 |   100/  111 batches | ms/batch 5620.46 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 624.25s | training loss  0.02 |
    | end of validation epoch  76 | time: 139.98s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266]
this is epoch 77
| epoch  77 |   100/  111 batches | ms/batch 5894.04 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 644.47s | training loss  0.03 |
    | end of validation epoch  77 | time: 135.26s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797]
this is epoch 78
| epoch  78 |   100/  111 batches | ms/batch 5614.66 | loss  0.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 627.99s | training loss  0.03 |
    | end of validation epoch  78 | time: 143.99s | validation loss  2.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835]
this is epoch 79
| epoch  79 |   100/  111 batches | ms/batch 5439.60 | loss  0.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 603.27s | training loss  0.03 |
    | end of validation epoch  79 | time: 153.65s | validation loss  2.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948]
this is epoch 80
| epoch  80 |   100/  111 batches | ms/batch 5694.47 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 639.32s | training loss  0.03 |
    | end of validation epoch  80 | time: 140.75s | validation loss  2.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363]
this is epoch 81
| epoch  81 |   100/  111 batches | ms/batch 5748.10 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 638.77s | training loss  0.02 |
    | end of validation epoch  81 | time: 142.10s | validation loss  2.21 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796]
this is epoch 82
| epoch  82 |   100/  111 batches | ms/batch 5515.94 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 629.09s | training loss  0.02 |
    | end of validation epoch  82 | time: 157.50s | validation loss  2.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792]
this is epoch 83
| epoch  83 |   100/  111 batches | ms/batch 5398.48 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 611.85s | training loss  0.02 |
    | end of validation epoch  83 | time: 141.30s | validation loss  2.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641]
this is epoch 84
| epoch  84 |   100/  111 batches | ms/batch 5681.45 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 626.11s | training loss  0.02 |
    | end of validation epoch  84 | time: 147.18s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273]
this is epoch 85
| epoch  85 |   100/  111 batches | ms/batch 5556.85 | loss  0.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 617.56s | training loss  0.02 |
    | end of validation epoch  85 | time: 133.40s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265]
this is epoch 86
| epoch  86 |   100/  111 batches | ms/batch 5263.77 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 595.96s | training loss  0.04 |
    | end of validation epoch  86 | time: 135.05s | validation loss  2.46 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982]
this is epoch 87
| epoch  87 |   100/  111 batches | ms/batch 5571.83 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 623.59s | training loss  0.03 |
    | end of validation epoch  87 | time: 150.44s | validation loss  2.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187]
this is epoch 88
| epoch  88 |   100/  111 batches | ms/batch 5563.97 | loss  0.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 629.37s | training loss  0.02 |
    | end of validation epoch  88 | time: 137.93s | validation loss  2.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743]
this is epoch 89
| epoch  89 |   100/  111 batches | ms/batch 5430.64 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 607.15s | training loss  0.01 |
    | end of validation epoch  89 | time: 150.36s | validation loss  2.33 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  111 batches | ms/batch 5113.18 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 576.19s | training loss  0.02 |
    | end of validation epoch  90 | time: 136.81s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923]
this is epoch 91
| epoch  91 |   100/  111 batches | ms/batch 5194.93 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 577.43s | training loss  0.02 |
    | end of validation epoch  91 | time: 137.64s | validation loss  1.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198]
this is epoch 92
| epoch  92 |   100/  111 batches | ms/batch 5452.84 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 606.89s | training loss  0.02 |
    | end of validation epoch  92 | time: 127.08s | validation loss  2.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008]
this is epoch 93
| epoch  93 |   100/  111 batches | ms/batch 5940.55 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 656.45s | training loss  0.01 |
    | end of validation epoch  93 | time: 152.26s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166]
this is epoch 94
| epoch  94 |   100/  111 batches | ms/batch 5431.30 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 599.94s | training loss  0.01 |
    | end of validation epoch  94 | time: 154.64s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578]
this is epoch 95
| epoch  95 |   100/  111 batches | ms/batch 5849.35 | loss  0.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 655.76s | training loss  0.01 |
    | end of validation epoch  95 | time: 160.92s | validation loss  2.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801, 0.011770545172259665] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578, 2.796414067668896]
this is epoch 96
| epoch  96 |   100/  111 batches | ms/batch 5732.65 | loss  0.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 626.55s | training loss  0.01 |
    | end of validation epoch  96 | time: 137.72s | validation loss  2.30 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801, 0.011770545172259665, 0.012509657248334438] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578, 2.796414067668896, 2.3020892977237963]
this is epoch 97
| epoch  97 |   100/  111 batches | ms/batch 6269.51 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 696.19s | training loss  0.03 |
    | end of validation epoch  97 | time: 148.29s | validation loss  2.43 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801, 0.011770545172259665, 0.012509657248334438, 0.03291020421222112] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578, 2.796414067668896, 2.3020892977237963, 2.4265875588243944]
this is epoch 98
| epoch  98 |   100/  111 batches | ms/batch 5503.18 | loss  0.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 628.44s | training loss  0.05 |
    | end of validation epoch  98 | time: 153.50s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801, 0.011770545172259665, 0.012509657248334438, 0.03291020421222112, 0.04534196011986382] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578, 2.796414067668896, 2.3020892977237963, 2.4265875588243944, 2.58771424826773]
this is epoch 99
| epoch  99 |   100/  111 batches | ms/batch 5538.92 | loss  0.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 613.93s | training loss  0.03 |
    | end of validation epoch  99 | time: 150.96s | validation loss  2.73 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [2.022422927993912, 1.3430059482385446, 1.1523217054100725, 1.025113570260572, 0.9038670621476732, 0.8090104060667055, 0.7032164031857843, 0.6439937200095203, 0.5675543769522831, 0.5003344450179521, 0.4217775355319719, 0.36577969534440086, 0.3503993636852986, 0.2910831789444159, 0.2695373712225003, 0.23447891995982006, 0.22145866689918278, 0.170973360605605, 0.1605140196646119, 0.12126786671243273, 0.13180399462916292, 0.11905072082404618, 0.10822137601270869, 0.09409050039342932, 0.07793313851328315, 0.07993312639830348, 0.07424409726114424, 0.07246730988425715, 0.07098189735499856, 0.07095324432242427, 0.06607976665737124, 0.08375123770790058, 0.06746557112028068, 0.05951153066613384, 0.051698137859201375, 0.04847694126268228, 0.04932641264823106, 0.04620949619844019, 0.035600064330742705, 0.03168114136300377, 0.0388990377185044, 0.03891913935768645, 0.042723579361598506, 0.027545626636023994, 0.03895689187537845, 0.05915179983125412, 0.049113524945553495, 0.036300051026046276, 0.036832696028255126, 0.04716567754711922, 0.03972119153351397, 0.03900212750493272, 0.03175644437060901, 0.03491069861637378, 0.030416251316473627, 0.015821153294349608, 0.02574550905181011, 0.03113662315268271, 0.023145219508290023, 0.05926719496681078, 0.03828844019455147, 0.018138642799893714, 0.012979030400946702, 0.02544489115694756, 0.05734609198328611, 0.031188544303363375, 0.021980191685829882, 0.02162192819056915, 0.015384179932224724, 0.011047863770645481, 0.011339373029726516, 0.026432892666848562, 0.03832504154824828, 0.027486833209918626, 0.026036507955559337, 0.02271503973689333, 0.03213929536968453, 0.03388943489817147, 0.03251108729179848, 0.02691659529332642, 0.018268061417410872, 0.01869502177499738, 0.016690962514756165, 0.019603115266035857, 0.01726631506562216, 0.04440014444234957, 0.032534423426867604, 0.015376642899617116, 0.009243510873791098, 0.02442011632773027, 0.023212057827271287, 0.02453562603353078, 0.0123718969123008, 0.012734640284277801, 0.011770545172259665, 0.012509657248334438, 0.03291020421222112, 0.04534196011986382, 0.034913002180883675] validation loss is  [1.5445711699624856, 1.260857407624523, 1.3406102202522259, 1.2724592403198283, 1.2061102067430813, 1.2446616347879171, 1.2720012896655437, 1.1863627439985673, 1.1720334989076946, 1.2128475670081873, 1.234600365103688, 1.2655118987895548, 1.4343456936379273, 1.4208684511637937, 1.6031271912749314, 1.4935428104751434, 1.3703349705222838, 1.675548562197946, 1.6329454884398729, 1.5909993608171742, 1.560283260497575, 1.6891128959056612, 1.5966450065704219, 1.697267457707009, 1.8744925796054304, 1.7927197608987626, 1.9154783183087905, 1.6984157608821988, 2.025164171932071, 2.0788870129035786, 1.879414515919052, 1.8318017881635267, 1.76082190706317, 1.8747094899978645, 1.9287555581230056, 1.9994598333748097, 2.0102677296163165, 1.9422045799049859, 2.155540917747809, 2.0226923479155325, 1.7186579685658216, 2.159277594415471, 2.119498937749692, 1.7460826526682165, 2.1799071810455644, 1.9949066912716564, 2.053420197296267, 2.174688171078742, 2.17120659945067, 2.1432670954879236, 1.8430633825676825, 2.159072884617975, 2.4250666056626264, 1.7837517445877893, 1.8876559263056454, 2.1214575143518837, 2.2078997485514265, 2.397541819829106, 2.035594964666719, 2.2113886266170084, 2.0837454800639534, 2.1255676285072695, 2.4709757937816903, 2.420206567311349, 2.075116961481399, 2.256564717996904, 2.175603470299393, 2.1343774450263786, 2.1134669692934644, 2.2544369060876, 2.1606621137956004, 3.1422580699149876, 2.268019786764247, 2.29544589536575, 2.0350338276512048, 2.5324589659285266, 2.6102412484275797, 2.299081057590835, 2.3937138778273948, 2.1781589423141363, 2.209005444807796, 2.369457824184792, 2.398544167283641, 2.5636352710274273, 2.524412789614265, 2.459814156247982, 2.3056995586230187, 2.066500161054743, 2.3286510289459934, 2.5945376155298923, 1.9921155006935198, 2.313656580605008, 2.5007774008627166, 2.531857887360578, 2.796414067668896, 2.3020892977237963, 2.4265875588243944, 2.58771424826773, 2.7348880583977007]
