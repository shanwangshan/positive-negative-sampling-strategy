/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/negative_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 150.88 | loss  5.74 |
| epoch   1 |   200/  475 batches | ms/batch 140.56 | loss  5.70 |
| epoch   1 |   300/  475 batches | ms/batch 136.03 | loss  5.42 |
| epoch   1 |   400/  475 batches | ms/batch 133.93 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 63.73s | training loss  5.69 |
    | end of validation epoch   1 | time: 53.54s | validation loss  5.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.692523019690262] validation loss is  [5.116441894980038]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 146.19 | loss  5.24 |
| epoch   2 |   200/  475 batches | ms/batch 134.37 | loss  5.20 |
| epoch   2 |   300/  475 batches | ms/batch 130.48 | loss  5.08 |
| epoch   2 |   400/  475 batches | ms/batch 128.03 | loss  5.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 60.46s | training loss  5.19 |
    | end of validation epoch   2 | time: 50.70s | validation loss  4.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.692523019690262, 5.192255317286441] validation loss is  [5.116441894980038, 4.616448562686183]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 148.66 | loss  5.16 |
| epoch   3 |   200/  475 batches | ms/batch 134.68 | loss  4.99 |
| epoch   3 |   300/  475 batches | ms/batch 128.93 | loss  5.14 |
| epoch   3 |   400/  475 batches | ms/batch 128.98 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 61.75s | training loss  4.90 |
    | end of validation epoch   3 | time: 50.86s | validation loss  4.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 150.74 | loss  4.82 |
| epoch   4 |   200/  475 batches | ms/batch 134.87 | loss  4.82 |
| epoch   4 |   300/  475 batches | ms/batch 131.11 | loss  4.82 |
| epoch   4 |   400/  475 batches | ms/batch 129.36 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 61.66s | training loss  4.71 |
    | end of validation epoch   4 | time: 50.40s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 146.45 | loss  4.93 |
| epoch   5 |   200/  475 batches | ms/batch 133.99 | loss  4.71 |
| epoch   5 |   300/  475 batches | ms/batch 131.32 | loss  4.63 |
| epoch   5 |   400/  475 batches | ms/batch 129.46 | loss  4.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 61.26s | training loss  4.57 |
    | end of validation epoch   5 | time: 51.37s | validation loss  3.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 146.29 | loss  4.47 |
| epoch   6 |   200/  475 batches | ms/batch 133.95 | loss  4.56 |
| epoch   6 |   300/  475 batches | ms/batch 130.12 | loss  4.34 |
| epoch   6 |   400/  475 batches | ms/batch 128.80 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 61.04s | training loss  4.46 |
    | end of validation epoch   6 | time: 51.37s | validation loss  3.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 143.54 | loss  4.03 |
| epoch   7 |   200/  475 batches | ms/batch 134.86 | loss  4.43 |
| epoch   7 |   300/  475 batches | ms/batch 131.68 | loss  4.26 |
| epoch   7 |   400/  475 batches | ms/batch 129.18 | loss  4.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 61.36s | training loss  4.37 |
    | end of validation epoch   7 | time: 52.26s | validation loss  3.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 146.12 | loss  4.17 |
| epoch   8 |   200/  475 batches | ms/batch 132.50 | loss  4.35 |
| epoch   8 |   300/  475 batches | ms/batch 129.08 | loss  4.61 |
| epoch   8 |   400/  475 batches | ms/batch 127.20 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 60.43s | training loss  4.29 |
    | end of validation epoch   8 | time: 52.40s | validation loss  3.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 145.75 | loss  4.52 |
| epoch   9 |   200/  475 batches | ms/batch 134.01 | loss  4.26 |
| epoch   9 |   300/  475 batches | ms/batch 130.03 | loss  4.21 |
| epoch   9 |   400/  475 batches | ms/batch 128.18 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 61.03s | training loss  4.21 |
    | end of validation epoch   9 | time: 52.25s | validation loss  3.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 148.44 | loss  3.95 |
| epoch  10 |   200/  475 batches | ms/batch 134.77 | loss  4.09 |
| epoch  10 |   300/  475 batches | ms/batch 129.28 | loss  4.45 |
| epoch  10 |   400/  475 batches | ms/batch 127.06 | loss  4.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 59.97s | training loss  4.15 |
    | end of validation epoch  10 | time: 51.54s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 145.96 | loss  4.01 |
| epoch  11 |   200/  475 batches | ms/batch 135.71 | loss  4.13 |
| epoch  11 |   300/  475 batches | ms/batch 130.40 | loss  3.94 |
| epoch  11 |   400/  475 batches | ms/batch 128.34 | loss  3.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 61.21s | training loss  4.09 |
    | end of validation epoch  11 | time: 50.27s | validation loss  3.47 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 145.39 | loss  3.78 |
| epoch  12 |   200/  475 batches | ms/batch 132.41 | loss  3.81 |
| epoch  12 |   300/  475 batches | ms/batch 129.56 | loss  4.12 |
| epoch  12 |   400/  475 batches | ms/batch 127.33 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 60.34s | training loss  4.06 |
    | end of validation epoch  12 | time: 50.35s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 150.26 | loss  4.04 |
| epoch  13 |   200/  475 batches | ms/batch 134.12 | loss  3.98 |
| epoch  13 |   300/  475 batches | ms/batch 129.64 | loss  4.32 |
| epoch  13 |   400/  475 batches | ms/batch 127.41 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 60.49s | training loss  4.00 |
    | end of validation epoch  13 | time: 50.99s | validation loss  3.37 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 144.91 | loss  4.20 |
| epoch  14 |   200/  475 batches | ms/batch 132.20 | loss  4.19 |
| epoch  14 |   300/  475 batches | ms/batch 128.62 | loss  3.99 |
| epoch  14 |   400/  475 batches | ms/batch 127.25 | loss  3.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 60.60s | training loss  3.96 |
    | end of validation epoch  14 | time: 51.74s | validation loss  3.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 148.62 | loss  4.18 |
| epoch  15 |   200/  475 batches | ms/batch 134.55 | loss  4.11 |
| epoch  15 |   300/  475 batches | ms/batch 131.03 | loss  3.76 |
| epoch  15 |   400/  475 batches | ms/batch 129.23 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 60.64s | training loss  3.93 |
    | end of validation epoch  15 | time: 49.67s | validation loss  3.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 148.88 | loss  3.86 |
| epoch  16 |   200/  475 batches | ms/batch 136.24 | loss  4.04 |
| epoch  16 |   300/  475 batches | ms/batch 131.14 | loss  4.13 |
| epoch  16 |   400/  475 batches | ms/batch 128.70 | loss  4.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 61.13s | training loss  3.89 |
    | end of validation epoch  16 | time: 51.10s | validation loss  3.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 146.44 | loss  3.87 |
| epoch  17 |   200/  475 batches | ms/batch 134.02 | loss  3.58 |
| epoch  17 |   300/  475 batches | ms/batch 129.61 | loss  3.77 |
| epoch  17 |   400/  475 batches | ms/batch 128.18 | loss  4.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 60.91s | training loss  3.85 |
    | end of validation epoch  17 | time: 51.99s | validation loss  3.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 143.54 | loss  3.97 |
| epoch  18 |   200/  475 batches | ms/batch 134.44 | loss  4.27 |
| epoch  18 |   300/  475 batches | ms/batch 129.57 | loss  3.60 |
| epoch  18 |   400/  475 batches | ms/batch 127.34 | loss  3.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 60.38s | training loss  3.83 |
    | end of validation epoch  18 | time: 51.72s | validation loss  3.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 146.12 | loss  3.67 |
| epoch  19 |   200/  475 batches | ms/batch 132.20 | loss  3.69 |
| epoch  19 |   300/  475 batches | ms/batch 127.90 | loss  3.79 |
| epoch  19 |   400/  475 batches | ms/batch 126.32 | loss  3.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 60.09s | training loss  3.78 |
    | end of validation epoch  19 | time: 51.23s | validation loss  3.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 148.50 | loss  3.52 |
| epoch  20 |   200/  475 batches | ms/batch 135.77 | loss  3.78 |
| epoch  20 |   300/  475 batches | ms/batch 131.31 | loss  3.76 |
| epoch  20 |   400/  475 batches | ms/batch 129.38 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 61.27s | training loss  3.76 |
    | end of validation epoch  20 | time: 51.23s | validation loss  3.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 149.53 | loss  3.57 |
| epoch  21 |   200/  475 batches | ms/batch 134.35 | loss  3.68 |
| epoch  21 |   300/  475 batches | ms/batch 128.65 | loss  3.90 |
| epoch  21 |   400/  475 batches | ms/batch 126.46 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 59.97s | training loss  3.73 |
    | end of validation epoch  21 | time: 50.73s | validation loss  3.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 146.47 | loss  3.70 |
| epoch  22 |   200/  475 batches | ms/batch 132.70 | loss  3.34 |
| epoch  22 |   300/  475 batches | ms/batch 128.43 | loss  3.85 |
| epoch  22 |   400/  475 batches | ms/batch 127.00 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 60.58s | training loss  3.70 |
    | end of validation epoch  22 | time: 50.99s | validation loss  3.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 144.57 | loss  3.69 |
| epoch  23 |   200/  475 batches | ms/batch 132.79 | loss  3.59 |
| epoch  23 |   300/  475 batches | ms/batch 128.91 | loss  4.23 |
| epoch  23 |   400/  475 batches | ms/batch 127.40 | loss  3.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 60.80s | training loss  3.68 |
    | end of validation epoch  23 | time: 49.89s | validation loss  3.01 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 146.22 | loss  3.74 |
| epoch  24 |   200/  475 batches | ms/batch 133.38 | loss  3.51 |
| epoch  24 |   300/  475 batches | ms/batch 129.47 | loss  3.58 |
| epoch  24 |   400/  475 batches | ms/batch 127.07 | loss  3.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 60.40s | training loss  3.66 |
    | end of validation epoch  24 | time: 51.20s | validation loss  3.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 145.23 | loss  4.01 |
| epoch  25 |   200/  475 batches | ms/batch 132.96 | loss  3.84 |
| epoch  25 |   300/  475 batches | ms/batch 129.17 | loss  3.74 |
| epoch  25 |   400/  475 batches | ms/batch 127.97 | loss  3.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 60.61s | training loss  3.63 |
    | end of validation epoch  25 | time: 51.85s | validation loss  3.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 143.10 | loss  3.32 |
| epoch  26 |   200/  475 batches | ms/batch 134.62 | loss  3.63 |
| epoch  26 |   300/  475 batches | ms/batch 130.20 | loss  3.57 |
| epoch  26 |   400/  475 batches | ms/batch 127.92 | loss  3.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 60.40s | training loss  3.61 |
    | end of validation epoch  26 | time: 51.81s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 144.99 | loss  3.86 |
| epoch  27 |   200/  475 batches | ms/batch 134.26 | loss  3.72 |
| epoch  27 |   300/  475 batches | ms/batch 129.40 | loss  3.35 |
| epoch  27 |   400/  475 batches | ms/batch 127.07 | loss  3.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 60.68s | training loss  3.61 |
    | end of validation epoch  27 | time: 50.43s | validation loss  2.93 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 149.06 | loss  3.77 |
| epoch  28 |   200/  475 batches | ms/batch 134.88 | loss  3.55 |
| epoch  28 |   300/  475 batches | ms/batch 131.01 | loss  4.19 |
| epoch  28 |   400/  475 batches | ms/batch 128.53 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 60.85s | training loss  3.58 |
    | end of validation epoch  28 | time: 52.49s | validation loss  2.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 147.63 | loss  3.18 |
| epoch  29 |   200/  475 batches | ms/batch 137.50 | loss  3.44 |
| epoch  29 |   300/  475 batches | ms/batch 131.61 | loss  3.77 |
| epoch  29 |   400/  475 batches | ms/batch 129.46 | loss  3.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 61.28s | training loss  3.55 |
    | end of validation epoch  29 | time: 51.94s | validation loss  2.96 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 145.80 | loss  3.28 |
| epoch  30 |   200/  475 batches | ms/batch 133.52 | loss  3.31 |
| epoch  30 |   300/  475 batches | ms/batch 129.76 | loss  3.72 |
| epoch  30 |   400/  475 batches | ms/batch 127.89 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 60.86s | training loss  3.56 |
    | end of validation epoch  30 | time: 50.76s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 142.92 | loss  3.83 |
| epoch  31 |   200/  475 batches | ms/batch 132.99 | loss  3.38 |
| epoch  31 |   300/  475 batches | ms/batch 129.08 | loss  3.76 |
| epoch  31 |   400/  475 batches | ms/batch 126.97 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 60.40s | training loss  3.54 |
    | end of validation epoch  31 | time: 50.62s | validation loss  2.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 148.53 | loss  3.52 |
| epoch  32 |   200/  475 batches | ms/batch 135.49 | loss  3.84 |
| epoch  32 |   300/  475 batches | ms/batch 131.54 | loss  3.56 |
| epoch  32 |   400/  475 batches | ms/batch 128.66 | loss  3.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 61.08s | training loss  3.52 |
    | end of validation epoch  32 | time: 52.74s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 150.75 | loss  3.76 |
| epoch  33 |   200/  475 batches | ms/batch 137.24 | loss  3.52 |
| epoch  33 |   300/  475 batches | ms/batch 131.61 | loss  3.36 |
| epoch  33 |   400/  475 batches | ms/batch 129.73 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 61.77s | training loss  3.50 |
    | end of validation epoch  33 | time: 52.92s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 147.53 | loss  3.25 |
| epoch  34 |   200/  475 batches | ms/batch 134.32 | loss  3.65 |
| epoch  34 |   300/  475 batches | ms/batch 129.71 | loss  3.27 |
| epoch  34 |   400/  475 batches | ms/batch 127.52 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 60.44s | training loss  3.49 |
    | end of validation epoch  34 | time: 50.72s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 147.19 | loss  3.29 |
| epoch  35 |   200/  475 batches | ms/batch 132.96 | loss  3.65 |
| epoch  35 |   300/  475 batches | ms/batch 129.15 | loss  3.69 |
| epoch  35 |   400/  475 batches | ms/batch 126.79 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 60.36s | training loss  3.47 |
    | end of validation epoch  35 | time: 50.06s | validation loss  2.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 147.15 | loss  3.45 |
| epoch  36 |   200/  475 batches | ms/batch 135.30 | loss  3.35 |
| epoch  36 |   300/  475 batches | ms/batch 130.50 | loss  3.65 |
| epoch  36 |   400/  475 batches | ms/batch 128.80 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 61.06s | training loss  3.46 |
    | end of validation epoch  36 | time: 51.06s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 146.17 | loss  3.45 |
| epoch  37 |   200/  475 batches | ms/batch 133.41 | loss  3.35 |
| epoch  37 |   300/  475 batches | ms/batch 129.73 | loss  3.45 |
| epoch  37 |   400/  475 batches | ms/batch 126.65 | loss  3.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 60.48s | training loss  3.44 |
    | end of validation epoch  37 | time: 52.41s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 143.25 | loss  3.29 |
| epoch  38 |   200/  475 batches | ms/batch 132.14 | loss  3.55 |
| epoch  38 |   300/  475 batches | ms/batch 128.67 | loss  3.35 |
| epoch  38 |   400/  475 batches | ms/batch 126.62 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 60.15s | training loss  3.42 |
    | end of validation epoch  38 | time: 50.97s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 144.53 | loss  3.58 |
| epoch  39 |   200/  475 batches | ms/batch 134.17 | loss  3.27 |
| epoch  39 |   300/  475 batches | ms/batch 130.35 | loss  3.21 |
| epoch  39 |   400/  475 batches | ms/batch 128.62 | loss  3.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 61.01s | training loss  3.41 |
    | end of validation epoch  39 | time: 51.27s | validation loss  2.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 146.25 | loss  2.74 |
| epoch  40 |   200/  475 batches | ms/batch 134.79 | loss  3.44 |
| epoch  40 |   300/  475 batches | ms/batch 129.54 | loss  3.47 |
| epoch  40 |   400/  475 batches | ms/batch 127.41 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 60.51s | training loss  3.41 |
    | end of validation epoch  40 | time: 51.15s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 143.99 | loss  3.17 |
| epoch  41 |   200/  475 batches | ms/batch 134.89 | loss  3.34 |
| epoch  41 |   300/  475 batches | ms/batch 129.88 | loss  4.17 |
| epoch  41 |   400/  475 batches | ms/batch 127.47 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 60.71s | training loss  3.39 |
    | end of validation epoch  41 | time: 51.10s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 145.89 | loss  3.37 |
| epoch  42 |   200/  475 batches | ms/batch 133.67 | loss  3.30 |
| epoch  42 |   300/  475 batches | ms/batch 128.84 | loss  3.66 |
| epoch  42 |   400/  475 batches | ms/batch 127.21 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 60.78s | training loss  3.38 |
    | end of validation epoch  42 | time: 52.16s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 144.12 | loss  3.56 |
| epoch  43 |   200/  475 batches | ms/batch 132.77 | loss  3.24 |
| epoch  43 |   300/  475 batches | ms/batch 128.87 | loss  3.53 |
| epoch  43 |   400/  475 batches | ms/batch 126.50 | loss  3.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 60.34s | training loss  3.37 |
    | end of validation epoch  43 | time: 53.04s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 147.31 | loss  3.02 |
| epoch  44 |   200/  475 batches | ms/batch 133.81 | loss  3.98 |
| epoch  44 |   300/  475 batches | ms/batch 129.64 | loss  3.28 |
| epoch  44 |   400/  475 batches | ms/batch 128.24 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 60.95s | training loss  3.36 |
    | end of validation epoch  44 | time: 51.62s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 146.68 | loss  3.25 |
| epoch  45 |   200/  475 batches | ms/batch 132.61 | loss  3.28 |
| epoch  45 |   300/  475 batches | ms/batch 128.79 | loss  3.10 |
| epoch  45 |   400/  475 batches | ms/batch 126.49 | loss  3.44 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 60.20s | training loss  3.34 |
    | end of validation epoch  45 | time: 50.89s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 147.86 | loss  3.46 |
| epoch  46 |   200/  475 batches | ms/batch 133.30 | loss  3.37 |
| epoch  46 |   300/  475 batches | ms/batch 128.92 | loss  3.27 |
| epoch  46 |   400/  475 batches | ms/batch 127.44 | loss  3.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 60.42s | training loss  3.34 |
    | end of validation epoch  46 | time: 51.70s | validation loss  2.80 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153]
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 143.29 | loss  2.84 |
| epoch  47 |   200/  475 batches | ms/batch 132.25 | loss  2.87 |
| epoch  47 |   300/  475 batches | ms/batch 129.14 | loss  3.10 |
| epoch  47 |   400/  475 batches | ms/batch 126.74 | loss  3.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 60.37s | training loss  3.31 |
    | end of validation epoch  47 | time: 51.65s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 144.82 | loss  2.98 |
| epoch  48 |   200/  475 batches | ms/batch 133.68 | loss  3.78 |
| epoch  48 |   300/  475 batches | ms/batch 128.33 | loss  3.68 |
| epoch  48 |   400/  475 batches | ms/batch 126.29 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 60.39s | training loss  3.31 |
    | end of validation epoch  48 | time: 51.47s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 145.90 | loss  3.73 |
| epoch  49 |   200/  475 batches | ms/batch 132.79 | loss  3.63 |
| epoch  49 |   300/  475 batches | ms/batch 128.80 | loss  3.45 |
| epoch  49 |   400/  475 batches | ms/batch 126.90 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 60.33s | training loss  3.31 |
    | end of validation epoch  49 | time: 49.96s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 145.83 | loss  3.36 |
| epoch  50 |   200/  475 batches | ms/batch 135.36 | loss  3.07 |
| epoch  50 |   300/  475 batches | ms/batch 129.28 | loss  3.40 |
| epoch  50 |   400/  475 batches | ms/batch 127.26 | loss  2.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 60.45s | training loss  3.31 |
    | end of validation epoch  50 | time: 52.01s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 142.85 | loss  3.02 |
| epoch  51 |   200/  475 batches | ms/batch 132.41 | loss  3.17 |
| epoch  51 |   300/  475 batches | ms/batch 129.44 | loss  3.12 |
| epoch  51 |   400/  475 batches | ms/batch 126.87 | loss  3.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 60.36s | training loss  3.29 |
    | end of validation epoch  51 | time: 52.12s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 149.19 | loss  3.47 |
| epoch  52 |   200/  475 batches | ms/batch 135.77 | loss  3.53 |
| epoch  52 |   300/  475 batches | ms/batch 131.06 | loss  3.13 |
| epoch  52 |   400/  475 batches | ms/batch 128.61 | loss  3.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 60.80s | training loss  3.28 |
    | end of validation epoch  52 | time: 51.94s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 144.13 | loss  3.43 |
| epoch  53 |   200/  475 batches | ms/batch 131.37 | loss  3.21 |
| epoch  53 |   300/  475 batches | ms/batch 127.64 | loss  3.29 |
| epoch  53 |   400/  475 batches | ms/batch 126.59 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 60.26s | training loss  3.27 |
    | end of validation epoch  53 | time: 51.13s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 144.13 | loss  2.85 |
| epoch  54 |   200/  475 batches | ms/batch 133.07 | loss  3.00 |
| epoch  54 |   300/  475 batches | ms/batch 129.05 | loss  3.16 |
| epoch  54 |   400/  475 batches | ms/batch 127.81 | loss  3.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 60.77s | training loss  3.25 |
    | end of validation epoch  54 | time: 51.51s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 142.70 | loss  3.71 |
| epoch  55 |   200/  475 batches | ms/batch 131.23 | loss  3.30 |
| epoch  55 |   300/  475 batches | ms/batch 128.42 | loss  3.44 |
| epoch  55 |   400/  475 batches | ms/batch 127.50 | loss  3.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 60.61s | training loss  3.24 |
    | end of validation epoch  55 | time: 51.04s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 145.63 | loss  3.25 |
| epoch  56 |   200/  475 batches | ms/batch 133.23 | loss  2.88 |
| epoch  56 |   300/  475 batches | ms/batch 128.94 | loss  3.46 |
| epoch  56 |   400/  475 batches | ms/batch 127.47 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 60.78s | training loss  3.25 |
    | end of validation epoch  56 | time: 50.73s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 151.18 | loss  3.05 |
| epoch  57 |   200/  475 batches | ms/batch 136.24 | loss  3.22 |
| epoch  57 |   300/  475 batches | ms/batch 130.56 | loss  2.97 |
| epoch  57 |   400/  475 batches | ms/batch 128.94 | loss  3.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 61.06s | training loss  3.24 |
    | end of validation epoch  57 | time: 50.59s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 145.31 | loss  2.93 |
| epoch  58 |   200/  475 batches | ms/batch 132.90 | loss  3.10 |
| epoch  58 |   300/  475 batches | ms/batch 130.01 | loss  3.25 |
| epoch  58 |   400/  475 batches | ms/batch 128.23 | loss  3.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 60.93s | training loss  3.21 |
    | end of validation epoch  58 | time: 51.71s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 147.52 | loss  3.46 |
| epoch  59 |   200/  475 batches | ms/batch 136.61 | loss  3.36 |
| epoch  59 |   300/  475 batches | ms/batch 130.64 | loss  3.14 |
| epoch  59 |   400/  475 batches | ms/batch 128.61 | loss  2.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 60.93s | training loss  3.22 |
    | end of validation epoch  59 | time: 52.25s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 144.02 | loss  3.19 |
| epoch  60 |   200/  475 batches | ms/batch 133.89 | loss  2.97 |
| epoch  60 |   300/  475 batches | ms/batch 130.34 | loss  3.44 |
| epoch  60 |   400/  475 batches | ms/batch 127.97 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 60.88s | training loss  3.20 |
    | end of validation epoch  60 | time: 51.30s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 144.40 | loss  3.19 |
| epoch  61 |   200/  475 batches | ms/batch 135.22 | loss  3.31 |
| epoch  61 |   300/  475 batches | ms/batch 131.09 | loss  2.94 |
| epoch  61 |   400/  475 batches | ms/batch 128.98 | loss  2.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 60.66s | training loss  3.20 |
    | end of validation epoch  61 | time: 51.78s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 147.47 | loss  3.21 |
| epoch  62 |   200/  475 batches | ms/batch 133.42 | loss  2.81 |
| epoch  62 |   300/  475 batches | ms/batch 128.77 | loss  3.33 |
| epoch  62 |   400/  475 batches | ms/batch 126.72 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 60.52s | training loss  3.20 |
    | end of validation epoch  62 | time: 50.76s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 146.07 | loss  2.67 |
| epoch  63 |   200/  475 batches | ms/batch 132.20 | loss  2.83 |
| epoch  63 |   300/  475 batches | ms/batch 127.88 | loss  3.58 |
| epoch  63 |   400/  475 batches | ms/batch 127.16 | loss  3.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 60.50s | training loss  3.18 |
    | end of validation epoch  63 | time: 52.39s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 147.00 | loss  3.41 |
| epoch  64 |   200/  475 batches | ms/batch 133.23 | loss  3.55 |
| epoch  64 |   300/  475 batches | ms/batch 130.13 | loss  3.08 |
| epoch  64 |   400/  475 batches | ms/batch 127.79 | loss  2.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 60.59s | training loss  3.17 |
    | end of validation epoch  64 | time: 52.77s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 145.86 | loss  3.00 |
| epoch  65 |   200/  475 batches | ms/batch 133.05 | loss  3.39 |
| epoch  65 |   300/  475 batches | ms/batch 127.97 | loss  3.60 |
| epoch  65 |   400/  475 batches | ms/batch 125.88 | loss  3.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 60.32s | training loss  3.17 |
    | end of validation epoch  65 | time: 51.32s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 145.57 | loss  3.21 |
| epoch  66 |   200/  475 batches | ms/batch 133.41 | loss  3.15 |
| epoch  66 |   300/  475 batches | ms/batch 129.49 | loss  3.26 |
| epoch  66 |   400/  475 batches | ms/batch 128.29 | loss  2.99 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 60.85s | training loss  3.16 |
    | end of validation epoch  66 | time: 51.25s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 145.84 | loss  3.31 |
| epoch  67 |   200/  475 batches | ms/batch 131.53 | loss  3.46 |
| epoch  67 |   300/  475 batches | ms/batch 128.76 | loss  2.87 |
| epoch  67 |   400/  475 batches | ms/batch 127.01 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 60.41s | training loss  3.15 |
    | end of validation epoch  67 | time: 50.87s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 145.50 | loss  2.94 |
| epoch  68 |   200/  475 batches | ms/batch 132.77 | loss  3.21 |
| epoch  68 |   300/  475 batches | ms/batch 128.76 | loss  3.26 |
| epoch  68 |   400/  475 batches | ms/batch 126.63 | loss  2.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 60.21s | training loss  3.16 |
    | end of validation epoch  68 | time: 51.22s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 141.83 | loss  3.49 |
| epoch  69 |   200/  475 batches | ms/batch 132.50 | loss  3.69 |
| epoch  69 |   300/  475 batches | ms/batch 128.92 | loss  3.21 |
| epoch  69 |   400/  475 batches | ms/batch 127.72 | loss  3.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 60.48s | training loss  3.14 |
    | end of validation epoch  69 | time: 50.84s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 146.29 | loss  2.61 |
| epoch  70 |   200/  475 batches | ms/batch 133.46 | loss  3.36 |
| epoch  70 |   300/  475 batches | ms/batch 129.13 | loss  2.79 |
| epoch  70 |   400/  475 batches | ms/batch 127.57 | loss  2.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 60.64s | training loss  3.13 |
    | end of validation epoch  70 | time: 50.81s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 144.37 | loss  3.59 |
| epoch  71 |   200/  475 batches | ms/batch 133.13 | loss  2.74 |
| epoch  71 |   300/  475 batches | ms/batch 129.55 | loss  3.53 |
| epoch  71 |   400/  475 batches | ms/batch 126.50 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 60.24s | training loss  3.11 |
    | end of validation epoch  71 | time: 51.32s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 147.67 | loss  3.18 |
| epoch  72 |   200/  475 batches | ms/batch 132.10 | loss  2.70 |
| epoch  72 |   300/  475 batches | ms/batch 128.49 | loss  3.00 |
| epoch  72 |   400/  475 batches | ms/batch 126.81 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 60.21s | training loss  3.11 |
    | end of validation epoch  72 | time: 50.16s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 149.47 | loss  3.44 |
| epoch  73 |   200/  475 batches | ms/batch 135.88 | loss  2.69 |
| epoch  73 |   300/  475 batches | ms/batch 130.41 | loss  3.44 |
| epoch  73 |   400/  475 batches | ms/batch 129.20 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 61.14s | training loss  3.11 |
    | end of validation epoch  73 | time: 50.83s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 145.73 | loss  2.75 |
| epoch  74 |   200/  475 batches | ms/batch 133.12 | loss  2.85 |
| epoch  74 |   300/  475 batches | ms/batch 128.58 | loss  3.42 |
| epoch  74 |   400/  475 batches | ms/batch 126.89 | loss  3.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 60.34s | training loss  3.10 |
    | end of validation epoch  74 | time: 51.17s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 149.61 | loss  3.15 |
| epoch  75 |   200/  475 batches | ms/batch 134.97 | loss  3.10 |
| epoch  75 |   300/  475 batches | ms/batch 130.29 | loss  2.56 |
| epoch  75 |   400/  475 batches | ms/batch 128.45 | loss  3.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 61.10s | training loss  3.09 |
    | end of validation epoch  75 | time: 52.62s | validation loss  2.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 145.89 | loss  3.16 |
| epoch  76 |   200/  475 batches | ms/batch 132.15 | loss  2.60 |
| epoch  76 |   300/  475 batches | ms/batch 129.36 | loss  3.12 |
| epoch  76 |   400/  475 batches | ms/batch 126.99 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 60.65s | training loss  3.09 |
    | end of validation epoch  76 | time: 50.85s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 145.76 | loss  2.90 |
| epoch  77 |   200/  475 batches | ms/batch 131.74 | loss  2.82 |
| epoch  77 |   300/  475 batches | ms/batch 128.73 | loss  3.53 |
| epoch  77 |   400/  475 batches | ms/batch 127.23 | loss  2.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 60.64s | training loss  3.07 |
    | end of validation epoch  77 | time: 51.45s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 147.06 | loss  3.34 |
| epoch  78 |   200/  475 batches | ms/batch 132.98 | loss  3.15 |
| epoch  78 |   300/  475 batches | ms/batch 129.37 | loss  3.32 |
| epoch  78 |   400/  475 batches | ms/batch 127.26 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 60.38s | training loss  3.08 |
    | end of validation epoch  78 | time: 51.31s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 147.30 | loss  3.19 |
| epoch  79 |   200/  475 batches | ms/batch 134.57 | loss  2.74 |
| epoch  79 |   300/  475 batches | ms/batch 129.77 | loss  2.75 |
| epoch  79 |   400/  475 batches | ms/batch 127.98 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 60.73s | training loss  3.06 |
    | end of validation epoch  79 | time: 51.61s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 144.16 | loss  3.25 |
| epoch  80 |   200/  475 batches | ms/batch 133.10 | loss  3.12 |
| epoch  80 |   300/  475 batches | ms/batch 129.88 | loss  2.67 |
| epoch  80 |   400/  475 batches | ms/batch 127.90 | loss  3.22 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 60.87s | training loss  3.06 |
    | end of validation epoch  80 | time: 52.95s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 143.94 | loss  2.94 |
| epoch  81 |   200/  475 batches | ms/batch 132.13 | loss  3.18 |
| epoch  81 |   300/  475 batches | ms/batch 128.44 | loss  2.79 |
| epoch  81 |   400/  475 batches | ms/batch 126.39 | loss  2.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 59.90s | training loss  3.05 |
    | end of validation epoch  81 | time: 51.34s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 146.52 | loss  3.23 |
| epoch  82 |   200/  475 batches | ms/batch 133.30 | loss  3.14 |
| epoch  82 |   300/  475 batches | ms/batch 129.10 | loss  2.61 |
| epoch  82 |   400/  475 batches | ms/batch 127.94 | loss  3.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 60.76s | training loss  3.05 |
    | end of validation epoch  82 | time: 51.01s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 144.06 | loss  2.58 |
| epoch  83 |   200/  475 batches | ms/batch 131.22 | loss  3.01 |
| epoch  83 |   300/  475 batches | ms/batch 127.48 | loss  3.16 |
| epoch  83 |   400/  475 batches | ms/batch 125.66 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 59.77s | training loss  3.04 |
    | end of validation epoch  83 | time: 51.68s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 146.43 | loss  3.21 |
| epoch  84 |   200/  475 batches | ms/batch 133.47 | loss  3.10 |
| epoch  84 |   300/  475 batches | ms/batch 130.83 | loss  3.04 |
| epoch  84 |   400/  475 batches | ms/batch 128.74 | loss  2.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 60.88s | training loss  3.04 |
    | end of validation epoch  84 | time: 50.35s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 147.84 | loss  2.67 |
| epoch  85 |   200/  475 batches | ms/batch 133.29 | loss  3.12 |
| epoch  85 |   300/  475 batches | ms/batch 129.15 | loss  2.97 |
| epoch  85 |   400/  475 batches | ms/batch 127.56 | loss  2.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 61.15s | training loss  3.02 |
    | end of validation epoch  85 | time: 52.69s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 145.95 | loss  2.98 |
| epoch  86 |   200/  475 batches | ms/batch 134.65 | loss  2.91 |
| epoch  86 |   300/  475 batches | ms/batch 130.70 | loss  3.18 |
| epoch  86 |   400/  475 batches | ms/batch 127.79 | loss  2.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 60.42s | training loss  3.03 |
    | end of validation epoch  86 | time: 52.46s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 147.96 | loss  2.70 |
| epoch  87 |   200/  475 batches | ms/batch 134.00 | loss  3.20 |
| epoch  87 |   300/  475 batches | ms/batch 130.05 | loss  3.40 |
| epoch  87 |   400/  475 batches | ms/batch 127.60 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 60.51s | training loss  3.00 |
    | end of validation epoch  87 | time: 51.62s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 146.05 | loss  3.03 |
| epoch  88 |   200/  475 batches | ms/batch 133.79 | loss  2.94 |
| epoch  88 |   300/  475 batches | ms/batch 129.68 | loss  3.31 |
| epoch  88 |   400/  475 batches | ms/batch 128.22 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 61.23s | training loss  3.02 |
    | end of validation epoch  88 | time: 52.53s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 147.22 | loss  2.53 |
| epoch  89 |   200/  475 batches | ms/batch 133.92 | loss  2.70 |
| epoch  89 |   300/  475 batches | ms/batch 129.39 | loss  3.57 |
| epoch  89 |   400/  475 batches | ms/batch 127.81 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 60.72s | training loss  3.00 |
    | end of validation epoch  89 | time: 51.43s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 145.28 | loss  3.53 |
| epoch  90 |   200/  475 batches | ms/batch 132.51 | loss  2.79 |
| epoch  90 |   300/  475 batches | ms/batch 128.47 | loss  3.37 |
| epoch  90 |   400/  475 batches | ms/batch 127.23 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 60.89s | training loss  3.00 |
    | end of validation epoch  90 | time: 51.44s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 145.75 | loss  3.06 |
| epoch  91 |   200/  475 batches | ms/batch 132.59 | loss  2.64 |
| epoch  91 |   300/  475 batches | ms/batch 130.10 | loss  2.93 |
| epoch  91 |   400/  475 batches | ms/batch 127.99 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 60.87s | training loss  2.99 |
    | end of validation epoch  91 | time: 50.49s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 144.30 | loss  2.85 |
| epoch  92 |   200/  475 batches | ms/batch 132.52 | loss  2.61 |
| epoch  92 |   300/  475 batches | ms/batch 128.40 | loss  2.81 |
| epoch  92 |   400/  475 batches | ms/batch 126.43 | loss  2.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 60.60s | training loss  3.00 |
    | end of validation epoch  92 | time: 51.61s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 149.59 | loss  2.72 |
| epoch  93 |   200/  475 batches | ms/batch 134.59 | loss  3.20 |
| epoch  93 |   300/  475 batches | ms/batch 129.19 | loss  3.18 |
| epoch  93 |   400/  475 batches | ms/batch 126.73 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 60.30s | training loss  2.98 |
    | end of validation epoch  93 | time: 52.63s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 150.12 | loss  2.92 |
| epoch  94 |   200/  475 batches | ms/batch 136.01 | loss  3.09 |
| epoch  94 |   300/  475 batches | ms/batch 130.84 | loss  3.21 |
| epoch  94 |   400/  475 batches | ms/batch 128.32 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 60.81s | training loss  2.98 |
    | end of validation epoch  94 | time: 53.26s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 145.90 | loss  2.97 |
| epoch  95 |   200/  475 batches | ms/batch 133.15 | loss  2.94 |
| epoch  95 |   300/  475 batches | ms/batch 129.68 | loss  3.23 |
| epoch  95 |   400/  475 batches | ms/batch 126.57 | loss  3.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 60.34s | training loss  2.96 |
    | end of validation epoch  95 | time: 51.76s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183, 2.9570474704943206] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674, 2.5563925865317594]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 145.11 | loss  2.95 |
| epoch  96 |   200/  475 batches | ms/batch 135.27 | loss  3.10 |
| epoch  96 |   300/  475 batches | ms/batch 130.64 | loss  2.91 |
| epoch  96 |   400/  475 batches | ms/batch 128.87 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 61.18s | training loss  2.96 |
    | end of validation epoch  96 | time: 52.66s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183, 2.9570474704943206, 2.959228152726826] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674, 2.5563925865317594, 2.5211555597161044]
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 150.88 | loss  3.03 |
| epoch  97 |   200/  475 batches | ms/batch 138.97 | loss  2.96 |
| epoch  97 |   300/  475 batches | ms/batch 133.73 | loss  2.71 |
| epoch  97 |   400/  475 batches | ms/batch 130.71 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 61.98s | training loss  2.97 |
    | end of validation epoch  97 | time: 51.65s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183, 2.9570474704943206, 2.959228152726826, 2.96718548222592] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674, 2.5563925865317594, 2.5211555597161044, 2.533853563941827]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 149.51 | loss  2.85 |
| epoch  98 |   200/  475 batches | ms/batch 135.01 | loss  2.70 |
| epoch  98 |   300/  475 batches | ms/batch 131.29 | loss  3.06 |
| epoch  98 |   400/  475 batches | ms/batch 128.75 | loss  2.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 60.98s | training loss  2.95 |
    | end of validation epoch  98 | time: 52.25s | validation loss  2.54 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183, 2.9570474704943206, 2.959228152726826, 2.96718548222592, 2.9485872404198896] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674, 2.5563925865317594, 2.5211555597161044, 2.533853563941827, 2.5441449730336165]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 145.86 | loss  2.71 |
| epoch  99 |   200/  475 batches | ms/batch 133.23 | loss  2.79 |
| epoch  99 |   300/  475 batches | ms/batch 129.77 | loss  2.70 |
| epoch  99 |   400/  475 batches | ms/batch 126.98 | loss  3.08 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 60.24s | training loss  2.94 |
    | end of validation epoch  99 | time: 52.40s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.692523019690262, 5.192255317286441, 4.9039327500995835, 4.714223671963341, 4.569415796179521, 4.461839796367444, 4.372322403255262, 4.290752782821655, 4.210499905536049, 4.147474161951165, 4.087914793616847, 4.058652125910709, 4.002597504164044, 3.9565083604109916, 3.925481187920821, 3.8924279338435124, 3.848118049220035, 3.830951771485178, 3.784193033921091, 3.7631653790724906, 3.7267747060876144, 3.6955560759494177, 3.678657801778693, 3.6599192679555794, 3.628126509315089, 3.6123062831477113, 3.6086844439255565, 3.5785842584308827, 3.5527594390668367, 3.555200951224879, 3.535281017705014, 3.5220053682829207, 3.4951344630592747, 3.4935296249389647, 3.467118662783974, 3.4555736170316997, 3.442327208268015, 3.420152571828742, 3.406219805667275, 3.411763639952007, 3.38754685201143, 3.382415589784321, 3.3679726279409308, 3.3632140982778447, 3.338706965195505, 3.3389938730942577, 3.3114472780729596, 3.3121690945876274, 3.307169987527948, 3.3053086511712326, 3.2854208389081454, 3.281283910149022, 3.2668019033733167, 3.249109905142533, 3.2410871751684893, 3.2494976475364283, 3.2398473016839278, 3.213642339204487, 3.217357614918759, 3.195248992819535, 3.2003910576669794, 3.1954859136280263, 3.1826689559534977, 3.1666307158219187, 3.1670927343870465, 3.1584638028395804, 3.1540511823955337, 3.157048617915103, 3.1359904479980467, 3.1291060623369717, 3.11257026371203, 3.11153840265776, 3.1086255746138725, 3.102716307389109, 3.0893748057516, 3.091609472475554, 3.073008064470793, 3.0770080245168585, 3.0618172018151535, 3.058523766868993, 3.048341380169517, 3.0511294279600443, 3.037412441655209, 3.0377319888064735, 3.019855626758776, 3.0274468462090742, 3.0040023111042222, 3.0185526476408304, 3.0044297514463727, 3.0026271217747738, 2.989507712815937, 2.9950038714157907, 2.9816577906357615, 2.976326750203183, 2.9570474704943206, 2.959228152726826, 2.96718548222592, 2.9485872404198896, 2.935001378812288] validation loss is  [5.116441894980038, 4.616448562686183, 4.259403735649686, 4.081790423192897, 3.9497805122567824, 3.805279302997749, 3.7466963219041585, 3.690189355561713, 3.6318115166255405, 3.4739779123739036, 3.4651404228531013, 3.4402053055643034, 3.370185371206588, 3.2782671952447973, 3.251679406446569, 3.1773929816334188, 3.1803957734789168, 3.1638036315180673, 3.111699210495508, 3.1127334302213012, 3.0662024341711476, 3.0715251770340095, 3.0130942228461515, 3.0156242747266755, 2.9970067388871136, 2.979768787111555, 2.9289100630944516, 2.947327102933611, 2.962484333695484, 2.8866998287809995, 2.9133095500849877, 2.8684213281679556, 2.8502072686908626, 2.87497192671319, 2.913235005210428, 2.869264879146544, 2.7841437924809815, 2.8102668794263312, 2.8285801991695116, 2.773833769710124, 2.783383677987491, 2.776728604020191, 2.7752167557467935, 2.8217843340224578, 2.7443474000241577, 2.7976330188142153, 2.7486224475027132, 2.72404951207778, 2.7173463536911653, 2.782385333245542, 2.7008621552411247, 2.693345258215896, 2.7164484953679957, 2.7006895762531697, 2.7242389085913907, 2.6663397821057746, 2.6718100810251317, 2.685985557171477, 2.638286382210355, 2.6490352434270523, 2.6147487794651703, 2.675553942928795, 2.6143529845886873, 2.635861620181749, 2.6239514731559432, 2.615038347845318, 2.625133267971648, 2.6406976435364795, 2.601329511955005, 2.656723547382515, 2.6020157287100782, 2.6103098192134824, 2.621261200984987, 2.577987591759497, 2.6086646488734653, 2.587981355290453, 2.5924474081071485, 2.5502096825287124, 2.5791518507885334, 2.58772846931169, 2.5824458579055403, 2.5704643355698145, 2.5408217155632853, 2.5890711956665298, 2.601819709569466, 2.5647688132374227, 2.563815333262211, 2.557773137292942, 2.5020503617134415, 2.5631126085249316, 2.5490231123291145, 2.5649327300175897, 2.4959314069828067, 2.5421681163691674, 2.5563925865317594, 2.5211555597161044, 2.533853563941827, 2.5441449730336165, 2.54971110219715]
 Best training model found.
---------------------------------------------------------------------------------------------------
