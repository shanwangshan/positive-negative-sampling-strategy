/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
/home/wang9/anaconda3/envs/torch_1.11/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/lustre/wang9/Audio-video-ACL/negative_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is True
Directory  ./audio_model_lin/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 161.05 | loss  6.91 |
| epoch   1 |   200/  475 batches | ms/batch 145.85 | loss  7.13 |
| epoch   1 |   300/  475 batches | ms/batch 140.82 | loss  6.66 |
| epoch   1 |   400/  475 batches | ms/batch 138.93 | loss  6.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 66.01s | training loss  6.80 |
    | end of validation epoch   1 | time: 52.54s | validation loss  5.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [6.8013623458460755] validation loss is  [5.603403035332175]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 155.48 | loss  6.37 |
| epoch   2 |   200/  475 batches | ms/batch 141.73 | loss  6.29 |
| epoch   2 |   300/  475 batches | ms/batch 138.14 | loss  6.22 |
| epoch   2 |   400/  475 batches | ms/batch 135.84 | loss  6.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 64.07s | training loss  6.41 |
    | end of validation epoch   2 | time: 52.43s | validation loss  5.39 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [6.8013623458460755, 6.414185834181936] validation loss is  [5.603403035332175, 5.387486333606624]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 157.70 | loss  6.37 |
| epoch   3 |   200/  475 batches | ms/batch 144.79 | loss  6.13 |
| epoch   3 |   300/  475 batches | ms/batch 139.67 | loss  6.03 |
| epoch   3 |   400/  475 batches | ms/batch 137.32 | loss  6.11 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 65.36s | training loss  6.15 |
    | end of validation epoch   3 | time: 52.46s | validation loss  5.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 156.70 | loss  5.98 |
| epoch   4 |   200/  475 batches | ms/batch 143.70 | loss  5.93 |
| epoch   4 |   300/  475 batches | ms/batch 138.92 | loss  5.82 |
| epoch   4 |   400/  475 batches | ms/batch 137.05 | loss  6.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 65.13s | training loss  5.94 |
    | end of validation epoch   4 | time: 50.37s | validation loss  5.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 152.77 | loss  6.20 |
| epoch   5 |   200/  475 batches | ms/batch 140.85 | loss  5.77 |
| epoch   5 |   300/  475 batches | ms/batch 136.35 | loss  6.05 |
| epoch   5 |   400/  475 batches | ms/batch 134.09 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 63.96s | training loss  5.78 |
    | end of validation epoch   5 | time: 51.96s | validation loss  4.95 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 158.15 | loss  5.91 |
| epoch   6 |   200/  475 batches | ms/batch 144.76 | loss  5.50 |
| epoch   6 |   300/  475 batches | ms/batch 140.21 | loss  5.60 |
| epoch   6 |   400/  475 batches | ms/batch 137.93 | loss  5.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 65.51s | training loss  5.65 |
    | end of validation epoch   6 | time: 52.10s | validation loss  4.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 156.25 | loss  5.76 |
| epoch   7 |   200/  475 batches | ms/batch 143.10 | loss  5.58 |
| epoch   7 |   300/  475 batches | ms/batch 139.04 | loss  5.63 |
| epoch   7 |   400/  475 batches | ms/batch 136.66 | loss  5.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 64.84s | training loss  5.54 |
    | end of validation epoch   7 | time: 51.56s | validation loss  4.79 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 157.20 | loss  5.49 |
| epoch   8 |   200/  475 batches | ms/batch 143.06 | loss  5.56 |
| epoch   8 |   300/  475 batches | ms/batch 139.33 | loss  5.85 |
| epoch   8 |   400/  475 batches | ms/batch 136.73 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 65.16s | training loss  5.42 |
    | end of validation epoch   8 | time: 53.28s | validation loss  4.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 152.06 | loss  5.63 |
| epoch   9 |   200/  475 batches | ms/batch 143.39 | loss  5.29 |
| epoch   9 |   300/  475 batches | ms/batch 139.58 | loss  5.70 |
| epoch   9 |   400/  475 batches | ms/batch 137.59 | loss  5.02 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 65.40s | training loss  5.34 |
    | end of validation epoch   9 | time: 51.72s | validation loss  4.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 150.92 | loss  5.23 |
| epoch  10 |   200/  475 batches | ms/batch 140.73 | loss  5.25 |
| epoch  10 |   300/  475 batches | ms/batch 137.59 | loss  5.25 |
| epoch  10 |   400/  475 batches | ms/batch 135.56 | loss  4.83 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 64.18s | training loss  5.26 |
    | end of validation epoch  10 | time: 52.86s | validation loss  4.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 159.57 | loss  5.61 |
| epoch  11 |   200/  475 batches | ms/batch 145.96 | loss  5.24 |
| epoch  11 |   300/  475 batches | ms/batch 139.79 | loss  5.23 |
| epoch  11 |   400/  475 batches | ms/batch 137.79 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 65.11s | training loss  5.21 |
    | end of validation epoch  11 | time: 51.66s | validation loss  4.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 158.52 | loss  5.12 |
| epoch  12 |   200/  475 batches | ms/batch 141.67 | loss  5.42 |
| epoch  12 |   300/  475 batches | ms/batch 137.50 | loss  5.40 |
| epoch  12 |   400/  475 batches | ms/batch 135.90 | loss  5.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 64.51s | training loss  5.13 |
    | end of validation epoch  12 | time: 51.53s | validation loss  4.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 156.63 | loss  5.11 |
| epoch  13 |   200/  475 batches | ms/batch 144.44 | loss  5.45 |
| epoch  13 |   300/  475 batches | ms/batch 139.15 | loss  5.03 |
| epoch  13 |   400/  475 batches | ms/batch 136.93 | loss  5.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 64.62s | training loss  5.09 |
    | end of validation epoch  13 | time: 53.05s | validation loss  4.49 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 157.35 | loss  5.53 |
| epoch  14 |   200/  475 batches | ms/batch 142.65 | loss  4.90 |
| epoch  14 |   300/  475 batches | ms/batch 140.36 | loss  4.91 |
| epoch  14 |   400/  475 batches | ms/batch 138.16 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 65.58s | training loss  5.04 |
    | end of validation epoch  14 | time: 50.51s | validation loss  4.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 154.67 | loss  5.21 |
| epoch  15 |   200/  475 batches | ms/batch 143.56 | loss  5.40 |
| epoch  15 |   300/  475 batches | ms/batch 138.88 | loss  5.11 |
| epoch  15 |   400/  475 batches | ms/batch 136.46 | loss  5.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 64.78s | training loss  5.00 |
    | end of validation epoch  15 | time: 52.42s | validation loss  4.42 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 154.99 | loss  4.93 |
| epoch  16 |   200/  475 batches | ms/batch 141.92 | loss  5.02 |
| epoch  16 |   300/  475 batches | ms/batch 137.98 | loss  5.07 |
| epoch  16 |   400/  475 batches | ms/batch 136.12 | loss  5.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 64.62s | training loss  4.95 |
    | end of validation epoch  16 | time: 52.45s | validation loss  4.40 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 155.47 | loss  5.10 |
| epoch  17 |   200/  475 batches | ms/batch 142.40 | loss  4.97 |
| epoch  17 |   300/  475 batches | ms/batch 137.05 | loss  4.66 |
| epoch  17 |   400/  475 batches | ms/batch 135.21 | loss  4.92 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 64.01s | training loss  4.92 |
    | end of validation epoch  17 | time: 51.83s | validation loss  4.38 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 160.28 | loss  4.96 |
| epoch  18 |   200/  475 batches | ms/batch 144.40 | loss  4.89 |
| epoch  18 |   300/  475 batches | ms/batch 139.90 | loss  4.77 |
| epoch  18 |   400/  475 batches | ms/batch 137.30 | loss  5.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 64.77s | training loss  4.89 |
    | end of validation epoch  18 | time: 52.20s | validation loss  4.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 157.18 | loss  4.95 |
| epoch  19 |   200/  475 batches | ms/batch 142.29 | loss  4.82 |
| epoch  19 |   300/  475 batches | ms/batch 137.22 | loss  4.77 |
| epoch  19 |   400/  475 batches | ms/batch 134.73 | loss  5.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 63.87s | training loss  4.85 |
    | end of validation epoch  19 | time: 52.52s | validation loss  4.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 153.68 | loss  5.02 |
| epoch  20 |   200/  475 batches | ms/batch 141.26 | loss  4.75 |
| epoch  20 |   300/  475 batches | ms/batch 137.85 | loss  4.91 |
| epoch  20 |   400/  475 batches | ms/batch 135.99 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 64.30s | training loss  4.83 |
    | end of validation epoch  20 | time: 51.59s | validation loss  4.32 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 156.66 | loss  4.55 |
| epoch  21 |   200/  475 batches | ms/batch 141.41 | loss  5.02 |
| epoch  21 |   300/  475 batches | ms/batch 136.47 | loss  4.95 |
| epoch  21 |   400/  475 batches | ms/batch 135.09 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 64.30s | training loss  4.82 |
    | end of validation epoch  21 | time: 52.15s | validation loss  4.31 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 152.55 | loss  4.83 |
| epoch  22 |   200/  475 batches | ms/batch 139.74 | loss  4.94 |
| epoch  22 |   300/  475 batches | ms/batch 134.99 | loss  4.69 |
| epoch  22 |   400/  475 batches | ms/batch 133.09 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 63.37s | training loss  4.78 |
    | end of validation epoch  22 | time: 52.10s | validation loss  4.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 157.42 | loss  5.13 |
| epoch  23 |   200/  475 batches | ms/batch 143.10 | loss  4.72 |
| epoch  23 |   300/  475 batches | ms/batch 137.12 | loss  4.61 |
| epoch  23 |   400/  475 batches | ms/batch 135.31 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 64.21s | training loss  4.77 |
    | end of validation epoch  23 | time: 53.31s | validation loss  4.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 152.33 | loss  4.96 |
| epoch  24 |   200/  475 batches | ms/batch 140.53 | loss  4.89 |
| epoch  24 |   300/  475 batches | ms/batch 136.83 | loss  4.59 |
| epoch  24 |   400/  475 batches | ms/batch 134.59 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 63.52s | training loss  4.74 |
    | end of validation epoch  24 | time: 53.23s | validation loss  4.25 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 154.80 | loss  4.75 |
| epoch  25 |   200/  475 batches | ms/batch 140.46 | loss  4.53 |
| epoch  25 |   300/  475 batches | ms/batch 136.89 | loss  4.50 |
| epoch  25 |   400/  475 batches | ms/batch 134.37 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 64.08s | training loss  4.74 |
    | end of validation epoch  25 | time: 51.57s | validation loss  4.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 153.26 | loss  4.50 |
| epoch  26 |   200/  475 batches | ms/batch 141.35 | loss  4.97 |
| epoch  26 |   300/  475 batches | ms/batch 136.47 | loss  4.76 |
| epoch  26 |   400/  475 batches | ms/batch 134.34 | loss  4.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 63.90s | training loss  4.73 |
    | end of validation epoch  26 | time: 52.87s | validation loss  4.24 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 152.30 | loss  4.61 |
| epoch  27 |   200/  475 batches | ms/batch 139.01 | loss  4.75 |
| epoch  27 |   300/  475 batches | ms/batch 135.38 | loss  4.57 |
| epoch  27 |   400/  475 batches | ms/batch 133.93 | loss  4.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 63.88s | training loss  4.72 |
    | end of validation epoch  27 | time: 52.78s | validation loss  4.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 157.90 | loss  4.61 |
| epoch  28 |   200/  475 batches | ms/batch 141.80 | loss  5.17 |
| epoch  28 |   300/  475 batches | ms/batch 136.65 | loss  4.82 |
| epoch  28 |   400/  475 batches | ms/batch 135.35 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 64.23s | training loss  4.70 |
    | end of validation epoch  28 | time: 52.56s | validation loss  4.22 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 153.94 | loss  4.61 |
| epoch  29 |   200/  475 batches | ms/batch 140.01 | loss  4.78 |
| epoch  29 |   300/  475 batches | ms/batch 136.41 | loss  4.95 |
| epoch  29 |   400/  475 batches | ms/batch 135.44 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 64.55s | training loss  4.68 |
    | end of validation epoch  29 | time: 52.76s | validation loss  4.20 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 158.04 | loss  4.59 |
| epoch  30 |   200/  475 batches | ms/batch 144.54 | loss  4.41 |
| epoch  30 |   300/  475 batches | ms/batch 141.66 | loss  4.30 |
| epoch  30 |   400/  475 batches | ms/batch 139.51 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 66.26s | training loss  4.68 |
    | end of validation epoch  30 | time: 52.77s | validation loss  4.19 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 153.71 | loss  4.88 |
| epoch  31 |   200/  475 batches | ms/batch 141.55 | loss  4.46 |
| epoch  31 |   300/  475 batches | ms/batch 136.16 | loss  4.40 |
| epoch  31 |   400/  475 batches | ms/batch 134.74 | loss  4.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 63.97s | training loss  4.68 |
    | end of validation epoch  31 | time: 51.16s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 153.50 | loss  4.35 |
| epoch  32 |   200/  475 batches | ms/batch 142.51 | loss  4.66 |
| epoch  32 |   300/  475 batches | ms/batch 137.36 | loss  4.36 |
| epoch  32 |   400/  475 batches | ms/batch 134.10 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 64.04s | training loss  4.66 |
    | end of validation epoch  32 | time: 52.13s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 155.71 | loss  4.67 |
| epoch  33 |   200/  475 batches | ms/batch 141.98 | loss  4.43 |
| epoch  33 |   300/  475 batches | ms/batch 136.13 | loss  4.56 |
| epoch  33 |   400/  475 batches | ms/batch 133.51 | loss  4.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 63.83s | training loss  4.64 |
    | end of validation epoch  33 | time: 53.30s | validation loss  4.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 155.21 | loss  4.64 |
| epoch  34 |   200/  475 batches | ms/batch 141.11 | loss  4.69 |
| epoch  34 |   300/  475 batches | ms/batch 136.14 | loss  4.89 |
| epoch  34 |   400/  475 batches | ms/batch 134.48 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 64.33s | training loss  4.64 |
    | end of validation epoch  34 | time: 52.05s | validation loss  4.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 156.30 | loss  4.84 |
| epoch  35 |   200/  475 batches | ms/batch 140.47 | loss  4.35 |
| epoch  35 |   300/  475 batches | ms/batch 135.00 | loss  4.49 |
| epoch  35 |   400/  475 batches | ms/batch 133.34 | loss  4.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 63.82s | training loss  4.63 |
    | end of validation epoch  35 | time: 51.39s | validation loss  4.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 155.91 | loss  4.31 |
| epoch  36 |   200/  475 batches | ms/batch 143.16 | loss  4.56 |
| epoch  36 |   300/  475 batches | ms/batch 138.07 | loss  4.66 |
| epoch  36 |   400/  475 batches | ms/batch 135.41 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 64.35s | training loss  4.63 |
    | end of validation epoch  36 | time: 51.77s | validation loss  4.15 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057]
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 155.85 | loss  4.50 |
| epoch  37 |   200/  475 batches | ms/batch 143.09 | loss  4.52 |
| epoch  37 |   300/  475 batches | ms/batch 136.93 | loss  4.34 |
| epoch  37 |   400/  475 batches | ms/batch 135.48 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 64.34s | training loss  4.62 |
    | end of validation epoch  37 | time: 53.40s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 155.95 | loss  4.42 |
| epoch  38 |   200/  475 batches | ms/batch 142.49 | loss  4.42 |
| epoch  38 |   300/  475 batches | ms/batch 137.90 | loss  4.87 |
| epoch  38 |   400/  475 batches | ms/batch 135.62 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 64.20s | training loss  4.61 |
    | end of validation epoch  38 | time: 52.44s | validation loss  4.14 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 155.92 | loss  4.65 |
| epoch  39 |   200/  475 batches | ms/batch 143.11 | loss  4.59 |
| epoch  39 |   300/  475 batches | ms/batch 139.36 | loss  4.59 |
| epoch  39 |   400/  475 batches | ms/batch 137.13 | loss  4.79 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 65.16s | training loss  4.60 |
    | end of validation epoch  39 | time: 52.58s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 151.54 | loss  4.53 |
| epoch  40 |   200/  475 batches | ms/batch 140.70 | loss  4.31 |
| epoch  40 |   300/  475 batches | ms/batch 136.06 | loss  4.49 |
| epoch  40 |   400/  475 batches | ms/batch 133.70 | loss  4.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 63.87s | training loss  4.62 |
    | end of validation epoch  40 | time: 52.12s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 153.27 | loss  4.67 |
| epoch  41 |   200/  475 batches | ms/batch 141.32 | loss  4.96 |
| epoch  41 |   300/  475 batches | ms/batch 137.11 | loss  4.84 |
| epoch  41 |   400/  475 batches | ms/batch 136.27 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 64.68s | training loss  4.60 |
    | end of validation epoch  41 | time: 51.71s | validation loss  4.13 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127]
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 150.37 | loss  4.57 |
| epoch  42 |   200/  475 batches | ms/batch 139.87 | loss  4.50 |
| epoch  42 |   300/  475 batches | ms/batch 134.46 | loss  4.65 |
| epoch  42 |   400/  475 batches | ms/batch 133.11 | loss  3.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 63.52s | training loss  4.58 |
    | end of validation epoch  42 | time: 53.17s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 155.34 | loss  4.50 |
| epoch  43 |   200/  475 batches | ms/batch 141.31 | loss  4.74 |
| epoch  43 |   300/  475 batches | ms/batch 137.47 | loss  4.37 |
| epoch  43 |   400/  475 batches | ms/batch 135.49 | loss  4.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 64.53s | training loss  4.59 |
    | end of validation epoch  43 | time: 52.73s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 151.19 | loss  4.53 |
| epoch  44 |   200/  475 batches | ms/batch 138.82 | loss  4.22 |
| epoch  44 |   300/  475 batches | ms/batch 134.22 | loss  4.53 |
| epoch  44 |   400/  475 batches | ms/batch 132.96 | loss  4.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 63.34s | training loss  4.60 |
    | end of validation epoch  44 | time: 53.11s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263]
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 155.09 | loss  4.51 |
| epoch  45 |   200/  475 batches | ms/batch 142.77 | loss  4.49 |
| epoch  45 |   300/  475 batches | ms/batch 138.10 | loss  4.58 |
| epoch  45 |   400/  475 batches | ms/batch 135.84 | loss  4.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 64.58s | training loss  4.59 |
    | end of validation epoch  45 | time: 51.49s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611]
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 152.65 | loss  4.39 |
| epoch  46 |   200/  475 batches | ms/batch 139.72 | loss  4.61 |
| epoch  46 |   300/  475 batches | ms/batch 137.04 | loss  4.25 |
| epoch  46 |   400/  475 batches | ms/batch 134.90 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 63.73s | training loss  4.58 |
    | end of validation epoch  46 | time: 52.57s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 154.64 | loss  4.61 |
| epoch  47 |   200/  475 batches | ms/batch 141.25 | loss  4.63 |
| epoch  47 |   300/  475 batches | ms/batch 136.44 | loss  4.48 |
| epoch  47 |   400/  475 batches | ms/batch 134.89 | loss  4.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 64.48s | training loss  4.58 |
    | end of validation epoch  47 | time: 52.37s | validation loss  4.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 155.36 | loss  4.65 |
| epoch  48 |   200/  475 batches | ms/batch 141.71 | loss  4.90 |
| epoch  48 |   300/  475 batches | ms/batch 136.17 | loss  4.72 |
| epoch  48 |   400/  475 batches | ms/batch 135.35 | loss  4.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 64.34s | training loss  4.58 |
    | end of validation epoch  48 | time: 53.52s | validation loss  4.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329]
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 153.34 | loss  4.69 |
| epoch  49 |   200/  475 batches | ms/batch 140.95 | loss  4.71 |
| epoch  49 |   300/  475 batches | ms/batch 136.52 | loss  4.75 |
| epoch  49 |   400/  475 batches | ms/batch 134.09 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 63.83s | training loss  4.58 |
    | end of validation epoch  49 | time: 52.28s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 150.68 | loss  4.66 |
| epoch  50 |   200/  475 batches | ms/batch 138.98 | loss  4.53 |
| epoch  50 |   300/  475 batches | ms/batch 135.23 | loss  4.56 |
| epoch  50 |   400/  475 batches | ms/batch 134.09 | loss  4.77 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 63.86s | training loss  4.58 |
    | end of validation epoch  50 | time: 51.90s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 151.71 | loss  4.77 |
| epoch  51 |   200/  475 batches | ms/batch 139.92 | loss  4.45 |
| epoch  51 |   300/  475 batches | ms/batch 137.24 | loss  4.97 |
| epoch  51 |   400/  475 batches | ms/batch 134.79 | loss  4.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 63.74s | training loss  4.57 |
    | end of validation epoch  51 | time: 53.42s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 153.26 | loss  4.61 |
| epoch  52 |   200/  475 batches | ms/batch 141.25 | loss  4.67 |
| epoch  52 |   300/  475 batches | ms/batch 138.23 | loss  4.50 |
| epoch  52 |   400/  475 batches | ms/batch 135.77 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 64.46s | training loss  4.57 |
    | end of validation epoch  52 | time: 51.35s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488]
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 155.36 | loss  4.66 |
| epoch  53 |   200/  475 batches | ms/batch 142.05 | loss  4.59 |
| epoch  53 |   300/  475 batches | ms/batch 138.80 | loss  4.44 |
| epoch  53 |   400/  475 batches | ms/batch 136.32 | loss  4.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 64.56s | training loss  4.58 |
    | end of validation epoch  53 | time: 53.06s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 155.22 | loss  4.33 |
| epoch  54 |   200/  475 batches | ms/batch 141.73 | loss  4.39 |
| epoch  54 |   300/  475 batches | ms/batch 137.53 | loss  4.71 |
| epoch  54 |   400/  475 batches | ms/batch 136.29 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 64.87s | training loss  4.57 |
    | end of validation epoch  54 | time: 52.11s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875]
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 153.43 | loss  4.64 |
| epoch  55 |   200/  475 batches | ms/batch 141.00 | loss  4.86 |
| epoch  55 |   300/  475 batches | ms/batch 135.45 | loss  4.62 |
| epoch  55 |   400/  475 batches | ms/batch 134.45 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 63.91s | training loss  4.55 |
    | end of validation epoch  55 | time: 52.96s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 153.04 | loss  4.45 |
| epoch  56 |   200/  475 batches | ms/batch 140.46 | loss  4.41 |
| epoch  56 |   300/  475 batches | ms/batch 135.97 | loss  4.40 |
| epoch  56 |   400/  475 batches | ms/batch 134.43 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 64.08s | training loss  4.56 |
    | end of validation epoch  56 | time: 52.42s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 150.57 | loss  4.68 |
| epoch  57 |   200/  475 batches | ms/batch 140.39 | loss  5.13 |
| epoch  57 |   300/  475 batches | ms/batch 135.58 | loss  4.72 |
| epoch  57 |   400/  475 batches | ms/batch 133.92 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 63.80s | training loss  4.57 |
    | end of validation epoch  57 | time: 51.51s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821]
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 154.73 | loss  4.66 |
| epoch  58 |   200/  475 batches | ms/batch 140.20 | loss  4.58 |
| epoch  58 |   300/  475 batches | ms/batch 136.07 | loss  4.71 |
| epoch  58 |   400/  475 batches | ms/batch 134.31 | loss  4.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 63.73s | training loss  4.55 |
    | end of validation epoch  58 | time: 52.99s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 154.22 | loss  4.72 |
| epoch  59 |   200/  475 batches | ms/batch 141.40 | loss  4.88 |
| epoch  59 |   300/  475 batches | ms/batch 136.18 | loss  4.56 |
| epoch  59 |   400/  475 batches | ms/batch 134.52 | loss  4.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 64.05s | training loss  4.57 |
    | end of validation epoch  59 | time: 52.93s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713]
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 156.08 | loss  4.25 |
| epoch  60 |   200/  475 batches | ms/batch 141.91 | loss  4.88 |
| epoch  60 |   300/  475 batches | ms/batch 137.96 | loss  4.38 |
| epoch  60 |   400/  475 batches | ms/batch 135.06 | loss  4.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 64.41s | training loss  4.57 |
    | end of validation epoch  60 | time: 53.15s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382]
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 152.96 | loss  4.87 |
| epoch  61 |   200/  475 batches | ms/batch 138.54 | loss  4.53 |
| epoch  61 |   300/  475 batches | ms/batch 134.32 | loss  4.52 |
| epoch  61 |   400/  475 batches | ms/batch 132.45 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 62.96s | training loss  4.57 |
    | end of validation epoch  61 | time: 52.46s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 150.05 | loss  4.82 |
| epoch  62 |   200/  475 batches | ms/batch 138.88 | loss  4.60 |
| epoch  62 |   300/  475 batches | ms/batch 135.60 | loss  4.35 |
| epoch  62 |   400/  475 batches | ms/batch 134.37 | loss  4.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 63.95s | training loss  4.55 |
    | end of validation epoch  62 | time: 52.53s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209]
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 153.80 | loss  4.71 |
| epoch  63 |   200/  475 batches | ms/batch 140.74 | loss  4.73 |
| epoch  63 |   300/  475 batches | ms/batch 136.79 | loss  4.20 |
| epoch  63 |   400/  475 batches | ms/batch 134.64 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 63.79s | training loss  4.56 |
    | end of validation epoch  63 | time: 52.82s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755]
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 156.68 | loss  4.34 |
| epoch  64 |   200/  475 batches | ms/batch 140.90 | loss  4.62 |
| epoch  64 |   300/  475 batches | ms/batch 135.97 | loss  4.46 |
| epoch  64 |   400/  475 batches | ms/batch 134.33 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 63.88s | training loss  4.56 |
    | end of validation epoch  64 | time: 52.18s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 151.12 | loss  4.58 |
| epoch  65 |   200/  475 batches | ms/batch 141.09 | loss  4.73 |
| epoch  65 |   300/  475 batches | ms/batch 136.28 | loss  4.50 |
| epoch  65 |   400/  475 batches | ms/batch 133.93 | loss  4.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 63.72s | training loss  4.55 |
    | end of validation epoch  65 | time: 52.65s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285]
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 153.43 | loss  4.85 |
| epoch  66 |   200/  475 batches | ms/batch 141.60 | loss  4.65 |
| epoch  66 |   300/  475 batches | ms/batch 136.45 | loss  4.54 |
| epoch  66 |   400/  475 batches | ms/batch 134.34 | loss  4.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 63.98s | training loss  4.55 |
    | end of validation epoch  66 | time: 52.04s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472]
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 156.13 | loss  4.77 |
| epoch  67 |   200/  475 batches | ms/batch 141.78 | loss  4.26 |
| epoch  67 |   300/  475 batches | ms/batch 137.73 | loss  4.55 |
| epoch  67 |   400/  475 batches | ms/batch 135.77 | loss  4.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 64.21s | training loss  4.54 |
    | end of validation epoch  67 | time: 53.75s | validation loss  4.09 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 156.26 | loss  4.69 |
| epoch  68 |   200/  475 batches | ms/batch 142.45 | loss  4.55 |
| epoch  68 |   300/  475 batches | ms/batch 138.63 | loss  4.32 |
| epoch  68 |   400/  475 batches | ms/batch 136.75 | loss  4.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 64.93s | training loss  4.56 |
    | end of validation epoch  68 | time: 52.03s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423]
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 150.51 | loss  4.42 |
| epoch  69 |   200/  475 batches | ms/batch 142.82 | loss  4.50 |
| epoch  69 |   300/  475 batches | ms/batch 136.65 | loss  4.69 |
| epoch  69 |   400/  475 batches | ms/batch 134.84 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 64.24s | training loss  4.55 |
    | end of validation epoch  69 | time: 52.72s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448]
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 154.17 | loss  4.23 |
| epoch  70 |   200/  475 batches | ms/batch 140.76 | loss  4.65 |
| epoch  70 |   300/  475 batches | ms/batch 135.91 | loss  4.39 |
| epoch  70 |   400/  475 batches | ms/batch 134.09 | loss  4.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 63.60s | training loss  4.56 |
    | end of validation epoch  70 | time: 53.00s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457]
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 152.64 | loss  4.66 |
| epoch  71 |   200/  475 batches | ms/batch 140.32 | loss  4.98 |
| epoch  71 |   300/  475 batches | ms/batch 135.72 | loss  4.75 |
| epoch  71 |   400/  475 batches | ms/batch 134.34 | loss  4.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 63.76s | training loss  4.56 |
    | end of validation epoch  71 | time: 52.53s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 153.06 | loss  4.50 |
| epoch  72 |   200/  475 batches | ms/batch 139.63 | loss  4.56 |
| epoch  72 |   300/  475 batches | ms/batch 136.04 | loss  4.59 |
| epoch  72 |   400/  475 batches | ms/batch 133.89 | loss  4.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 63.94s | training loss  4.55 |
    | end of validation epoch  72 | time: 52.50s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615]
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 150.99 | loss  4.92 |
| epoch  73 |   200/  475 batches | ms/batch 139.72 | loss  4.75 |
| epoch  73 |   300/  475 batches | ms/batch 136.09 | loss  4.88 |
| epoch  73 |   400/  475 batches | ms/batch 134.41 | loss  4.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 63.91s | training loss  4.56 |
    | end of validation epoch  73 | time: 52.30s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955]
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 152.80 | loss  4.46 |
| epoch  74 |   200/  475 batches | ms/batch 140.50 | loss  4.59 |
| epoch  74 |   300/  475 batches | ms/batch 136.73 | loss  4.72 |
| epoch  74 |   400/  475 batches | ms/batch 134.54 | loss  4.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 63.75s | training loss  4.56 |
    | end of validation epoch  74 | time: 52.87s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 152.40 | loss  4.62 |
| epoch  75 |   200/  475 batches | ms/batch 140.57 | loss  4.12 |
| epoch  75 |   300/  475 batches | ms/batch 136.33 | loss  4.78 |
| epoch  75 |   400/  475 batches | ms/batch 133.92 | loss  4.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 63.70s | training loss  4.55 |
    | end of validation epoch  75 | time: 52.68s | validation loss  4.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135]
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 157.64 | loss  4.25 |
| epoch  76 |   200/  475 batches | ms/batch 142.76 | loss  4.26 |
| epoch  76 |   300/  475 batches | ms/batch 136.52 | loss  4.34 |
| epoch  76 |   400/  475 batches | ms/batch 134.66 | loss  4.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 63.78s | training loss  4.55 |
    | end of validation epoch  76 | time: 52.37s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622]
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 155.00 | loss  4.36 |
| epoch  77 |   200/  475 batches | ms/batch 141.41 | loss  4.63 |
| epoch  77 |   300/  475 batches | ms/batch 136.66 | loss  4.15 |
| epoch  77 |   400/  475 batches | ms/batch 134.61 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 64.09s | training loss  4.54 |
    | end of validation epoch  77 | time: 52.76s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061]
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 151.70 | loss  4.37 |
| epoch  78 |   200/  475 batches | ms/batch 139.71 | loss  4.59 |
| epoch  78 |   300/  475 batches | ms/batch 136.70 | loss  4.79 |
| epoch  78 |   400/  475 batches | ms/batch 134.16 | loss  4.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 63.63s | training loss  4.54 |
    | end of validation epoch  78 | time: 52.06s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704]
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 155.25 | loss  4.48 |
| epoch  79 |   200/  475 batches | ms/batch 143.12 | loss  4.79 |
| epoch  79 |   300/  475 batches | ms/batch 138.52 | loss  4.70 |
| epoch  79 |   400/  475 batches | ms/batch 135.51 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 64.28s | training loss  4.55 |
    | end of validation epoch  79 | time: 53.37s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703]
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 155.00 | loss  4.17 |
| epoch  80 |   200/  475 batches | ms/batch 141.30 | loss  4.60 |
| epoch  80 |   300/  475 batches | ms/batch 137.00 | loss  4.59 |
| epoch  80 |   400/  475 batches | ms/batch 135.66 | loss  4.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 64.34s | training loss  4.55 |
    | end of validation epoch  80 | time: 51.98s | validation loss  4.08 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875]
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 153.26 | loss  4.63 |
| epoch  81 |   200/  475 batches | ms/batch 139.91 | loss  4.58 |
| epoch  81 |   300/  475 batches | ms/batch 137.41 | loss  4.39 |
| epoch  81 |   400/  475 batches | ms/batch 135.00 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 64.24s | training loss  4.54 |
    | end of validation epoch  81 | time: 52.29s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 153.78 | loss  4.51 |
| epoch  82 |   200/  475 batches | ms/batch 139.70 | loss  4.86 |
| epoch  82 |   300/  475 batches | ms/batch 137.64 | loss  4.47 |
| epoch  82 |   400/  475 batches | ms/batch 135.87 | loss  4.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 64.36s | training loss  4.55 |
    | end of validation epoch  82 | time: 52.65s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156]
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 151.64 | loss  4.33 |
| epoch  83 |   200/  475 batches | ms/batch 139.14 | loss  4.46 |
| epoch  83 |   300/  475 batches | ms/batch 135.53 | loss  4.13 |
| epoch  83 |   400/  475 batches | ms/batch 134.23 | loss  4.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 63.90s | training loss  4.55 |
    | end of validation epoch  83 | time: 52.02s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966]
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 157.45 | loss  4.64 |
| epoch  84 |   200/  475 batches | ms/batch 143.95 | loss  4.71 |
| epoch  84 |   300/  475 batches | ms/batch 137.80 | loss  4.59 |
| epoch  84 |   400/  475 batches | ms/batch 134.55 | loss  4.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 63.92s | training loss  4.54 |
    | end of validation epoch  84 | time: 52.83s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874]
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 157.23 | loss  4.65 |
| epoch  85 |   200/  475 batches | ms/batch 141.57 | loss  4.29 |
| epoch  85 |   300/  475 batches | ms/batch 136.45 | loss  4.47 |
| epoch  85 |   400/  475 batches | ms/batch 135.18 | loss  4.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 64.30s | training loss  4.54 |
    | end of validation epoch  85 | time: 52.54s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 159.00 | loss  4.32 |
| epoch  86 |   200/  475 batches | ms/batch 143.06 | loss  4.52 |
| epoch  86 |   300/  475 batches | ms/batch 138.00 | loss  4.69 |
| epoch  86 |   400/  475 batches | ms/batch 135.58 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 64.55s | training loss  4.56 |
    | end of validation epoch  86 | time: 53.01s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156]
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 150.78 | loss  4.44 |
| epoch  87 |   200/  475 batches | ms/batch 138.90 | loss  4.30 |
| epoch  87 |   300/  475 batches | ms/batch 134.23 | loss  4.65 |
| epoch  87 |   400/  475 batches | ms/batch 133.42 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 63.59s | training loss  4.56 |
    | end of validation epoch  87 | time: 52.88s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 158.43 | loss  4.65 |
| epoch  88 |   200/  475 batches | ms/batch 142.07 | loss  4.75 |
| epoch  88 |   300/  475 batches | ms/batch 136.90 | loss  4.56 |
| epoch  88 |   400/  475 batches | ms/batch 134.55 | loss  4.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 63.85s | training loss  4.54 |
    | end of validation epoch  88 | time: 53.09s | validation loss  4.05 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 152.49 | loss  4.42 |
| epoch  89 |   200/  475 batches | ms/batch 140.43 | loss  4.66 |
| epoch  89 |   300/  475 batches | ms/batch 136.17 | loss  4.18 |
| epoch  89 |   400/  475 batches | ms/batch 134.02 | loss  4.78 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 63.63s | training loss  4.56 |
    | end of validation epoch  89 | time: 51.93s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136]
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 154.00 | loss  4.34 |
| epoch  90 |   200/  475 batches | ms/batch 141.43 | loss  4.59 |
| epoch  90 |   300/  475 batches | ms/batch 135.98 | loss  4.65 |
| epoch  90 |   400/  475 batches | ms/batch 133.44 | loss  4.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 63.58s | training loss  4.56 |
    | end of validation epoch  90 | time: 52.73s | validation loss  4.02 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 157.25 | loss  4.94 |
| epoch  91 |   200/  475 batches | ms/batch 141.37 | loss  4.58 |
| epoch  91 |   300/  475 batches | ms/batch 137.29 | loss  4.50 |
| epoch  91 |   400/  475 batches | ms/batch 136.47 | loss  4.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 65.21s | training loss  4.55 |
    | end of validation epoch  91 | time: 54.04s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817]
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 161.85 | loss  4.88 |
| epoch  92 |   200/  475 batches | ms/batch 147.78 | loss  4.79 |
| epoch  92 |   300/  475 batches | ms/batch 141.80 | loss  4.67 |
| epoch  92 |   400/  475 batches | ms/batch 139.82 | loss  4.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 66.04s | training loss  4.54 |
    | end of validation epoch  92 | time: 52.16s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515]
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 152.54 | loss  4.31 |
| epoch  93 |   200/  475 batches | ms/batch 140.58 | loss  4.61 |
| epoch  93 |   300/  475 batches | ms/batch 135.88 | loss  4.42 |
| epoch  93 |   400/  475 batches | ms/batch 134.87 | loss  4.74 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 64.03s | training loss  4.54 |
    | end of validation epoch  93 | time: 52.89s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557]
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 151.56 | loss  4.62 |
| epoch  94 |   200/  475 batches | ms/batch 141.05 | loss  4.74 |
| epoch  94 |   300/  475 batches | ms/batch 136.81 | loss  4.06 |
| epoch  94 |   400/  475 batches | ms/batch 134.49 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 64.14s | training loss  4.55 |
    | end of validation epoch  94 | time: 53.08s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783]
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 150.10 | loss  4.60 |
| epoch  95 |   200/  475 batches | ms/batch 140.38 | loss  4.57 |
| epoch  95 |   300/  475 batches | ms/batch 135.50 | loss  4.51 |
| epoch  95 |   400/  475 batches | ms/batch 133.17 | loss  4.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 63.28s | training loss  4.55 |
    | end of validation epoch  95 | time: 52.77s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742, 4.547422734812686] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783, 4.04340532246758]
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 155.36 | loss  4.63 |
| epoch  96 |   200/  475 batches | ms/batch 141.73 | loss  4.41 |
| epoch  96 |   300/  475 batches | ms/batch 136.90 | loss  4.62 |
| epoch  96 |   400/  475 batches | ms/batch 135.78 | loss  4.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 64.19s | training loss  4.54 |
    | end of validation epoch  96 | time: 53.16s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742, 4.547422734812686, 4.539048718904194] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783, 4.04340532246758, 4.030847246907339]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 154.51 | loss  4.41 |
| epoch  97 |   200/  475 batches | ms/batch 141.01 | loss  4.98 |
| epoch  97 |   300/  475 batches | ms/batch 135.27 | loss  4.30 |
| epoch  97 |   400/  475 batches | ms/batch 134.34 | loss  4.54 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 64.23s | training loss  4.55 |
    | end of validation epoch  97 | time: 52.43s | validation loss  4.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742, 4.547422734812686, 4.539048718904194, 4.549860977875559] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783, 4.04340532246758, 4.030847246907339, 4.056498932237385]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 157.02 | loss  4.31 |
| epoch  98 |   200/  475 batches | ms/batch 141.52 | loss  4.60 |
| epoch  98 |   300/  475 batches | ms/batch 136.15 | loss  4.22 |
| epoch  98 |   400/  475 batches | ms/batch 133.99 | loss  4.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 63.81s | training loss  4.54 |
    | end of validation epoch  98 | time: 52.11s | validation loss  4.04 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742, 4.547422734812686, 4.539048718904194, 4.549860977875559, 4.5418096979040845] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783, 4.04340532246758, 4.030847246907339, 4.056498932237385, 4.044072942573483]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 152.11 | loss  4.16 |
| epoch  99 |   200/  475 batches | ms/batch 140.31 | loss  4.76 |
| epoch  99 |   300/  475 batches | ms/batch 135.58 | loss  4.39 |
| epoch  99 |   400/  475 batches | ms/batch 134.70 | loss  4.23 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 64.04s | training loss  4.55 |
    | end of validation epoch  99 | time: 51.40s | validation loss  4.03 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [6.8013623458460755, 6.414185834181936, 6.154750056015818, 5.938396402660169, 5.782723027279502, 5.649631782330965, 5.537801969427812, 5.424957512303402, 5.34153469688014, 5.263055156908537, 5.2087875848067435, 5.1277250811928194, 5.089670680196662, 5.039120201311613, 4.995884090222811, 4.945939002789949, 4.923703232815392, 4.889732997291967, 4.854843382584421, 4.834004844866301, 4.816774735701712, 4.781313487605045, 4.77460067046316, 4.743028023368434, 4.735527048612896, 4.72617980655871, 4.717343505056281, 4.701098102770354, 4.680107768209357, 4.681703510284424, 4.675407766041003, 4.6561878836782356, 4.6429337320829696, 4.643580140565571, 4.630502474935431, 4.6331061875192745, 4.624586585195441, 4.612365231764944, 4.598852087322034, 4.620626727656314, 4.602055329774555, 4.584996982373689, 4.594064725574694, 4.598593009145636, 4.593175277709961, 4.581563025022808, 4.576933028572484, 4.577559582057752, 4.576874215979325, 4.577703586377596, 4.567594085492586, 4.570481764140882, 4.580481375142147, 4.574661471216302, 4.550737112948769, 4.562461432406777, 4.5708476558484525, 4.551470169268156, 4.567466805106715, 4.571834113472386, 4.566228626652768, 4.554973274030184, 4.561587043059499, 4.555905582026432, 4.554007635618511, 4.550794250588668, 4.540799134405035, 4.559949093868858, 4.546598391783865, 4.558807499534205, 4.556324220456575, 4.550403703388415, 4.561377670388473, 4.555334384315892, 4.554996448818006, 4.548350868225097, 4.542970694491737, 4.543763203871878, 4.552782460262901, 4.546077424099571, 4.540137707559686, 4.551505104366102, 4.55415207862854, 4.543507040425351, 4.539561074909411, 4.55775524741725, 4.560280501717015, 4.539657785014103, 4.556075824938322, 4.558308385547838, 4.546995320069162, 4.542751238973517, 4.539923183039615, 4.554293196828742, 4.547422734812686, 4.539048718904194, 4.549860977875559, 4.5418096979040845, 4.5505977078488] validation loss is  [5.603403035332175, 5.387486333606624, 5.20455723850667, 5.064931168275721, 4.950169407019095, 4.852110498091754, 4.789064166926536, 4.7006564741375065, 4.653634415955103, 4.596256059758804, 4.547135168764772, 4.512726419112262, 4.494315179456182, 4.450980166427228, 4.42045998773655, 4.398280179801107, 4.375888960702079, 4.349526048708363, 4.345984098290195, 4.316657066345215, 4.309106961017897, 4.279324986353642, 4.278894378357575, 4.254065012731472, 4.240467658563822, 4.237069809136271, 4.215846213973871, 4.2152815045428875, 4.202481586392186, 4.1880090276734165, 4.174703269445596, 4.1732891627720425, 4.1689123486270425, 4.159729977615741, 4.1507671480419255, 4.150992611877057, 4.123568260369181, 4.140563389834235, 4.124350273308634, 4.110956240101021, 4.127133808216127, 4.114717162957712, 4.090017438936634, 4.104768352348263, 4.110960780071611, 4.110332132387562, 4.1131000679080225, 4.12218484758329, 4.098248926531367, 4.086596278583302, 4.096380512253577, 4.095393561515488, 4.075410055513141, 4.0787630782407875, 4.103080172498687, 4.068446435848204, 4.085981284870821, 4.062121377271764, 4.066917261155713, 4.089750678599382, 4.059794281711097, 4.082677805123209, 4.060391291850755, 4.055917000570217, 4.0637727224526285, 4.070340935923472, 4.088744381896588, 4.056285757978423, 4.062222827382448, 4.058266810008457, 4.045324121202741, 4.0572855692951615, 4.0713334063521955, 4.0344882852890915, 4.066365384254135, 4.057096180795622, 4.05320093812061, 4.047659793821704, 4.048312631975703, 4.081197442126875, 4.044315648679974, 4.047346327485156, 4.059829471491966, 4.06334007086874, 4.040595228932485, 4.036833258236156, 4.031125234956501, 4.052980625328898, 4.0398642896604136, 4.0224950553990215, 4.025264405402817, 4.0428591736224515, 4.028539575448557, 4.055010224590783, 4.04340532246758, 4.030847246907339, 4.056498932237385, 4.044072942573483, 4.027853366707554]
