/run/nvme/job_2992392/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 309, 'ckp_path': './checkpoint/', 'vgg_path': '/vgg-sound/', 'unwanted_files_path': '../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64}
use_cude True
total number of training files is 38007
total number of training files is 38007
Let's use 4 GPUs!
Directory  ./checkpoint/  already exists
this is epoch 0
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   0 |   100/  123 batches | ms/batch 5536.17 | loss  5.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   0 | time: 649.97s | training loss  5.75 |
[5.747088998313842]
this is epoch 1
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   1 |   100/  123 batches | ms/batch 5691.71 | loss  5.71 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 648.09s | training loss  5.72 |
[5.747088998313842, 5.72084065181453]
this is epoch 2
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   2 |   100/  123 batches | ms/batch 5476.62 | loss  5.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 642.93s | training loss  5.69 |
[5.747088998313842, 5.72084065181453, 5.69125038240014]
this is epoch 3
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   3 |   100/  123 batches | ms/batch 5434.92 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 665.79s | training loss  5.68 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905]
this is epoch 4
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   4 |   100/  123 batches | ms/batch 5629.81 | loss  5.63 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 637.20s | training loss  5.66 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389]
this is epoch 5
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   5 |   100/  123 batches | ms/batch 5494.89 | loss  5.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 637.02s | training loss  5.66 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085]
this is epoch 6
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   6 |   100/  123 batches | ms/batch 5554.89 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 643.01s | training loss  5.65 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956]
this is epoch 7
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   7 |   100/  123 batches | ms/batch 5522.85 | loss  5.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 632.99s | training loss  5.64 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245]
this is epoch 8
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   8 |   100/  123 batches | ms/batch 5440.85 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 634.26s | training loss  5.64 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081]
this is epoch 9
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch   9 |   100/  123 batches | ms/batch 5464.30 | loss  5.62 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 630.62s | training loss  5.63 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755]
this is epoch 10
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  10 |   100/  123 batches | ms/batch 5578.48 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 634.85s | training loss  5.62 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629]
this is epoch 11
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  11 |   100/  123 batches | ms/batch 5555.95 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 639.62s | training loss  5.62 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818]
this is epoch 12
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  12 |   100/  123 batches | ms/batch 5483.02 | loss  5.61 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 642.86s | training loss  5.61 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654]
this is epoch 13
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  13 |   100/  123 batches | ms/batch 5682.75 | loss  5.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 651.56s | training loss  5.61 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436]
this is epoch 14
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  14 |   100/  123 batches | ms/batch 5764.96 | loss  5.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 650.93s | training loss  5.60 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255]
this is epoch 15
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  15 |   100/  123 batches | ms/batch 5391.07 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 634.32s | training loss  5.60 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097]
this is epoch 16
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  16 |   100/  123 batches | ms/batch 5573.35 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 632.17s | training loss  5.59 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116]
this is epoch 17
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  17 |   100/  123 batches | ms/batch 5571.66 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 643.19s | training loss  5.59 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195]
this is epoch 18
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  18 |   100/  123 batches | ms/batch 5548.00 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 645.04s | training loss  5.58 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557]
this is epoch 19
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  19 |   100/  123 batches | ms/batch 5574.91 | loss  5.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 640.52s | training loss  5.58 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698]
this is epoch 20
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  20 |   100/  123 batches | ms/batch 5671.66 | loss  5.59 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 645.14s | training loss  5.57 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365]
this is epoch 21
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  21 |   100/  123 batches | ms/batch 5661.70 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 642.04s | training loss  5.57 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608]
this is epoch 22
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  22 |   100/  123 batches | ms/batch 5585.31 | loss  5.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 647.17s | training loss  5.57 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078]
this is epoch 23
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  23 |   100/  123 batches | ms/batch 5574.64 | loss  5.58 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 633.43s | training loss  5.56 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867]
this is epoch 24
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  24 |   100/  123 batches | ms/batch 5606.88 | loss  5.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 640.28s | training loss  5.56 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403]
this is epoch 25
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  25 |   100/  123 batches | ms/batch 5488.68 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 635.91s | training loss  5.56 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005]
this is epoch 26
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  26 |   100/  123 batches | ms/batch 5550.77 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 641.87s | training loss  5.55 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326]
this is epoch 27
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  27 |   100/  123 batches | ms/batch 5727.31 | loss  5.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 657.41s | training loss  5.54 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705]
this is epoch 28
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  28 |   100/  123 batches | ms/batch 5511.80 | loss  5.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 644.34s | training loss  5.54 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397]
this is epoch 29
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  29 |   100/  123 batches | ms/batch 5508.39 | loss  5.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 636.16s | training loss  5.53 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843]
this is epoch 30
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  30 |   100/  123 batches | ms/batch 5527.15 | loss  5.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 630.35s | training loss  5.54 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456]
this is epoch 31
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  31 |   100/  123 batches | ms/batch 5579.53 | loss  5.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 641.41s | training loss  5.53 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715]
this is epoch 32
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  32 |   100/  123 batches | ms/batch 5545.10 | loss  5.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 641.34s | training loss  5.53 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734]
this is epoch 33
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  33 |   100/  123 batches | ms/batch 5605.26 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 644.20s | training loss  5.52 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229]
this is epoch 34
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  34 |   100/  123 batches | ms/batch 5511.01 | loss  5.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 635.17s | training loss  5.51 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954]
this is epoch 35
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  35 |   100/  123 batches | ms/batch 5677.91 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 648.81s | training loss  5.51 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844]
this is epoch 36
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  36 |   100/  123 batches | ms/batch 5546.02 | loss  5.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 634.94s | training loss  5.51 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547]
this is epoch 37
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  37 |   100/  123 batches | ms/batch 5528.69 | loss  5.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 633.71s | training loss  5.50 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673]
this is epoch 38
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  38 |   100/  123 batches | ms/batch 5626.05 | loss  5.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 641.21s | training loss  5.50 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765]
this is epoch 39
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  39 |   100/  123 batches | ms/batch 5587.14 | loss  5.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 638.34s | training loss  5.50 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831]
this is epoch 40
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  40 |   100/  123 batches | ms/batch 5563.22 | loss  5.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 634.78s | training loss  5.49 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043]
this is epoch 41
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  41 |   100/  123 batches | ms/batch 5630.89 | loss  5.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 644.06s | training loss  5.48 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233]
this is epoch 42
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  42 |   100/  123 batches | ms/batch 5615.68 | loss  5.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 647.41s | training loss  5.48 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084]
this is epoch 43
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  43 |   100/  123 batches | ms/batch 5582.54 | loss  5.51 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 637.38s | training loss  5.48 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117]
this is epoch 44
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  44 |   100/  123 batches | ms/batch 5622.56 | loss  5.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 646.38s | training loss  5.48 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911]
this is epoch 45
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  45 |   100/  123 batches | ms/batch 5625.77 | loss  5.48 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 635.79s | training loss  5.47 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035]
this is epoch 46
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  46 |   100/  123 batches | ms/batch 5537.33 | loss  5.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 633.98s | training loss  5.46 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955]
this is epoch 47
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  47 |   100/  123 batches | ms/batch 5559.47 | loss  5.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 642.07s | training loss  5.46 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907]
this is epoch 48
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  48 |   100/  123 batches | ms/batch 5654.96 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 639.81s | training loss  5.46 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172]
this is epoch 49
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  49 |   100/  123 batches | ms/batch 5626.18 | loss  5.53 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 647.74s | training loss  5.45 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129]
this is epoch 50
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  50 |   100/  123 batches | ms/batch 5391.59 | loss  5.50 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 637.21s | training loss  5.45 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324]
this is epoch 51
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  51 |   100/  123 batches | ms/batch 5489.82 | loss  5.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 636.89s | training loss  5.44 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659]
this is epoch 52
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  52 |   100/  123 batches | ms/batch 5501.34 | loss  5.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 636.57s | training loss  5.44 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465]
this is epoch 53
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  53 |   100/  123 batches | ms/batch 5496.06 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 632.47s | training loss  5.43 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981]
this is epoch 54
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  54 |   100/  123 batches | ms/batch 5600.64 | loss  5.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 645.42s | training loss  5.43 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129]
this is epoch 55
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  55 |   100/  123 batches | ms/batch 5476.17 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 630.16s | training loss  5.43 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388]
this is epoch 56
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  56 |   100/  123 batches | ms/batch 5713.02 | loss  5.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 643.82s | training loss  5.42 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662]
this is epoch 57
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  57 |   100/  123 batches | ms/batch 5544.60 | loss  5.43 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 641.50s | training loss  5.42 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499]
this is epoch 58
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  58 |   100/  123 batches | ms/batch 5492.36 | loss  5.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 641.71s | training loss  5.41 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287]
this is epoch 59
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  59 |   100/  123 batches | ms/batch 5590.29 | loss  5.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 638.76s | training loss  5.41 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517]
this is epoch 60
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  60 |   100/  123 batches | ms/batch 5493.87 | loss  5.39 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 633.91s | training loss  5.41 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904]
this is epoch 61
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  61 |   100/  123 batches | ms/batch 5784.75 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 669.56s | training loss  5.40 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827]
this is epoch 62
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  62 |   100/  123 batches | ms/batch 5520.53 | loss  5.38 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 631.94s | training loss  5.40 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179]
this is epoch 63
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  63 |   100/  123 batches | ms/batch 5550.83 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 635.05s | training loss  5.40 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066]
this is epoch 64
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  64 |   100/  123 batches | ms/batch 5561.04 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 638.93s | training loss  5.39 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957]
this is epoch 65
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  65 |   100/  123 batches | ms/batch 5521.66 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 650.96s | training loss  5.39 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153]
this is epoch 66
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  66 |   100/  123 batches | ms/batch 5521.00 | loss  5.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 640.89s | training loss  5.38 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688]
this is epoch 67
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  67 |   100/  123 batches | ms/batch 5534.78 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 643.86s | training loss  5.38 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295]
this is epoch 68
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  68 |   100/  123 batches | ms/batch 5570.73 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 640.18s | training loss  5.37 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737]
this is epoch 69
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  69 |   100/  123 batches | ms/batch 5598.19 | loss  5.40 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 636.90s | training loss  5.37 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208]
this is epoch 70
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  70 |   100/  123 batches | ms/batch 5574.52 | loss  5.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 641.93s | training loss  5.37 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868]
this is epoch 71
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  71 |   100/  123 batches | ms/batch 5474.34 | loss  5.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 629.54s | training loss  5.36 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975]
this is epoch 72
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  72 |   100/  123 batches | ms/batch 5606.03 | loss  5.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 638.11s | training loss  5.36 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553]
this is epoch 73
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  73 |   100/  123 batches | ms/batch 5773.27 | loss  5.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 660.09s | training loss  5.36 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046]
this is epoch 74
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  74 |   100/  123 batches | ms/batch 5572.29 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 636.85s | training loss  5.35 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771]
this is epoch 75
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  75 |   100/  123 batches | ms/batch 5532.23 | loss  5.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 637.25s | training loss  5.35 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189]
this is epoch 76
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  76 |   100/  123 batches | ms/batch 5771.40 | loss  5.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 659.51s | training loss  5.34 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776]
this is epoch 77
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  77 |   100/  123 batches | ms/batch 5660.99 | loss  5.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 643.26s | training loss  5.34 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123]
this is epoch 78
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  78 |   100/  123 batches | ms/batch 5569.32 | loss  5.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 632.84s | training loss  5.33 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359]
this is epoch 79
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  79 |   100/  123 batches | ms/batch 5561.98 | loss  5.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 644.51s | training loss  5.34 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425]
this is epoch 80
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  80 |   100/  123 batches | ms/batch 5601.95 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 636.78s | training loss  5.33 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119]
this is epoch 81
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  81 |   100/  123 batches | ms/batch 5677.53 | loss  5.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 644.86s | training loss  5.32 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675]
this is epoch 82
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  82 |   100/  123 batches | ms/batch 5470.67 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 631.03s | training loss  5.33 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538]
this is epoch 83
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  83 |   100/  123 batches | ms/batch 5750.92 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 654.36s | training loss  5.32 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551]
this is epoch 84
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  84 |   100/  123 batches | ms/batch 5547.11 | loss  5.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 633.34s | training loss  5.32 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885]
this is epoch 85
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  85 |   100/  123 batches | ms/batch 5658.17 | loss  5.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 638.34s | training loss  5.31 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927]
this is epoch 86
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  86 |   100/  123 batches | ms/batch 5524.72 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 635.29s | training loss  5.31 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419]
this is epoch 87
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  87 |   100/  123 batches | ms/batch 5783.95 | loss  5.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 660.91s | training loss  5.30 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115]
this is epoch 88
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  88 |   100/  123 batches | ms/batch 5413.42 | loss  5.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 631.75s | training loss  5.30 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014]
this is epoch 89
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  89 |   100/  123 batches | ms/batch 5654.00 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 638.02s | training loss  5.30 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582]
this is epoch 90
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  90 |   100/  123 batches | ms/batch 5559.29 | loss  5.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 638.28s | training loss  5.30 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063]
this is epoch 91
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  91 |   100/  123 batches | ms/batch 5584.71 | loss  5.29 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 632.60s | training loss  5.30 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641]
this is epoch 92
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  92 |   100/  123 batches | ms/batch 5470.48 | loss  5.20 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 635.37s | training loss  5.29 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595]
this is epoch 93
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  93 |   100/  123 batches | ms/batch 5511.36 | loss  5.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 635.29s | training loss  5.28 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445]
this is epoch 94
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  94 |   100/  123 batches | ms/batch 5660.11 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 642.28s | training loss  5.29 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879]
this is epoch 95
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  95 |   100/  123 batches | ms/batch 5579.43 | loss  5.21 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 638.57s | training loss  5.28 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879, 5.276223143910974]
this is epoch 96
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  96 |   100/  123 batches | ms/batch 5483.33 | loss  5.33 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 625.57s | training loss  5.28 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879, 5.276223143910974, 5.278748585925839]
this is epoch 97
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  97 |   100/  123 batches | ms/batch 5550.24 | loss  5.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 634.91s | training loss  5.27 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879, 5.276223143910974, 5.278748585925839, 5.27249314920689]
this is epoch 98
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  98 |   100/  123 batches | ms/batch 5571.79 | loss  5.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 630.33s | training loss  5.27 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879, 5.276223143910974, 5.278748585925839, 5.27249314920689, 5.270356666751024]
this is epoch 99
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
total number of training files is 38007
| epoch  99 |   100/  123 batches | ms/batch 5555.48 | loss  5.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 640.28s | training loss  5.27 |
[5.747088998313842, 5.72084065181453, 5.69125038240014, 5.676087712853905, 5.659713714103389, 5.6563597539576085, 5.646964681826956, 5.641384527935245, 5.637248737056081, 5.6261149538241755, 5.623276691126629, 5.620659591705818, 5.614853141753654, 5.6050890674436, 5.6013027245436255, 5.597999851878097, 5.5916822751363116, 5.589609681106195, 5.583743579988557, 5.580921328164698, 5.5723227911848365, 5.574238397241608, 5.569495550016078, 5.563750026671867, 5.562360825577403, 5.5555490865940005, 5.552223333498326, 5.541497358461705, 5.540345757957397, 5.534455970050843, 5.535217099073456, 5.5261728278989715, 5.529331385604734, 5.52226035963229, 5.51293975550954, 5.509672110642844, 5.506599693763547, 5.498491213573673, 5.496339584753765, 5.497620888841831, 5.490166431520043, 5.484097279184233, 5.483093017485084, 5.477332766463117, 5.475915649072911, 5.4699216354184035, 5.461619381012955, 5.461691720698907, 5.455879459536172, 5.453065255793129, 5.448547390418324, 5.44340933047659, 5.4355661384458465, 5.431136577109981, 5.42910937565129, 5.428719090252388, 5.421793042159662, 5.417378293789499, 5.410566101229287, 5.406539095126517, 5.40519920984904, 5.400661666218827, 5.404192436032179, 5.399593880506066, 5.392707797569957, 5.38586552550153, 5.38343335748688, 5.376389034395295, 5.372203412094737, 5.365860117160208, 5.367651857980868, 5.362782331016975, 5.355414030028553, 5.361793804944046, 5.353871306752771, 5.352463423721189, 5.341358250718776, 5.342269598953123, 5.332950770370359, 5.3364075296293425, 5.330942739316119, 5.324187530734675, 5.325599495957538, 5.323778001273551, 5.318659286188885, 5.314366499582927, 5.309823140865419, 5.3036644749525115, 5.296374262833014, 5.296774740141582, 5.295812909196063, 5.297554461936641, 5.2853646317148595, 5.284121629668445, 5.287664262259879, 5.276223143910974, 5.278748585925839, 5.27249314920689, 5.270356666751024, 5.266342453840302]
