/run/nvme/job_2995860/data
{'debug': False, 'num_workers': 16, 'seed': 0, 'n_epochs': 100, 'batch_size': 64, 'ckp_path': '../checkpoint/', 'vgg_path': '/vgg-sound/', 'filepath': '../selected_files.csv', 'unwanted_files_path': '../../unwanted.csv', 'video_clip_duration': 0.5, 'video_fps': 16.0, 'audio_fps': 16000, 'audio_dur': 1, 'spectrogram_fps': 100.0, 'n_fft': 512, 'n_mels': 64, 'num_classes': 309, 'feat_name': 'pool', 'pooling_op': None, 'feat_dim': 512, 'use_dropout': True, 'dropout': 0.5}
all the training files is 38007
training has  30406
all the training files is 38007
validation has  7601
/scratch/asignal/shanshan/Audio-video-ACL/real_ssl_classification_norm/test_indomain/../checkpoint/checkpoint.pt
model type is audio linear prob is False
Directory  ./audio_model_ft/  Created 
-----------start training
this is epoch 1
| epoch   1 |   100/  475 batches | ms/batch 1243.93 | loss  5.98 |
| epoch   1 |   200/  475 batches | ms/batch 659.44 | loss  5.52 |
| epoch   1 |   300/  475 batches | ms/batch 464.69 | loss  5.61 |
| epoch   1 |   400/  475 batches | ms/batch 367.70 | loss  5.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   1 | time: 153.25s | training loss  5.78 |
    | end of validation epoch   1 | time: 31.82s | validation loss  5.16 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 1 training loss is  [5.7798545506126] validation loss is  [5.160611825830796]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 2
| epoch   2 |   100/  475 batches | ms/batch 100.35 | loss  5.53 |
| epoch   2 |   200/  475 batches | ms/batch 85.55 | loss  5.40 |
| epoch   2 |   300/  475 batches | ms/batch 80.36 | loss  5.18 |
| epoch   2 |   400/  475 batches | ms/batch 77.98 | loss  4.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   2 | time: 37.02s | training loss  5.27 |
    | end of validation epoch   2 | time: 31.07s | validation loss  4.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 2 training loss is  [5.7798545506126, 5.265513761419999] validation loss is  [5.160611825830796, 4.659782958631756]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 3
| epoch   3 |   100/  475 batches | ms/batch 90.52 | loss  5.09 |
| epoch   3 |   200/  475 batches | ms/batch 79.99 | loss  4.70 |
| epoch   3 |   300/  475 batches | ms/batch 77.17 | loss  4.55 |
| epoch   3 |   400/  475 batches | ms/batch 75.34 | loss  4.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   3 | time: 36.00s | training loss  4.97 |
    | end of validation epoch   3 | time: 30.75s | validation loss  4.28 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 3 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 4
| epoch   4 |   100/  475 batches | ms/batch 89.02 | loss  4.92 |
| epoch   4 |   200/  475 batches | ms/batch 78.28 | loss  4.61 |
| epoch   4 |   300/  475 batches | ms/batch 75.58 | loss  4.85 |
| epoch   4 |   400/  475 batches | ms/batch 73.85 | loss  4.81 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   4 | time: 35.10s | training loss  4.75 |
    | end of validation epoch   4 | time: 30.86s | validation loss  4.10 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 4 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 5
| epoch   5 |   100/  475 batches | ms/batch 89.32 | loss  4.55 |
| epoch   5 |   200/  475 batches | ms/batch 95.29 | loss  4.61 |
| epoch   5 |   300/  475 batches | ms/batch 87.14 | loss  4.20 |
| epoch   5 |   400/  475 batches | ms/batch 82.84 | loss  4.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   5 | time: 38.95s | training loss  4.60 |
    | end of validation epoch   5 | time: 35.46s | validation loss  3.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 5 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 6
| epoch   6 |   100/  475 batches | ms/batch 98.20 | loss  4.48 |
| epoch   6 |   200/  475 batches | ms/batch 84.46 | loss  4.72 |
| epoch   6 |   300/  475 batches | ms/batch 80.97 | loss  4.27 |
| epoch   6 |   400/  475 batches | ms/batch 77.85 | loss  4.66 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   6 | time: 37.09s | training loss  4.49 |
    | end of validation epoch   6 | time: 42.05s | validation loss  3.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 6 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 7
| epoch   7 |   100/  475 batches | ms/batch 145.03 | loss  4.20 |
| epoch   7 |   200/  475 batches | ms/batch 106.84 | loss  4.13 |
| epoch   7 |   300/  475 batches | ms/batch 93.98 | loss  4.30 |
| epoch   7 |   400/  475 batches | ms/batch 87.81 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   7 | time: 41.02s | training loss  4.38 |
    | end of validation epoch   7 | time: 35.86s | validation loss  3.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 7 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 8
| epoch   8 |   100/  475 batches | ms/batch 102.74 | loss  4.11 |
| epoch   8 |   200/  475 batches | ms/batch 86.19 | loss  4.30 |
| epoch   8 |   300/  475 batches | ms/batch 80.82 | loss  4.52 |
| epoch   8 |   400/  475 batches | ms/batch 78.11 | loss  3.97 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   8 | time: 36.96s | training loss  4.28 |
    | end of validation epoch   8 | time: 33.09s | validation loss  3.61 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 8 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 9
| epoch   9 |   100/  475 batches | ms/batch 467.24 | loss  4.41 |
| epoch   9 |   200/  475 batches | ms/batch 279.31 | loss  4.25 |
| epoch   9 |   300/  475 batches | ms/batch 209.36 | loss  3.97 |
| epoch   9 |   400/  475 batches | ms/batch 175.56 | loss  4.32 |
---------------------------------------------------------------------------------------------------
    | end of training epoch   9 | time: 75.93s | training loss  4.22 |
    | end of validation epoch   9 | time: 38.59s | validation loss  3.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 9 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 10
| epoch  10 |   100/  475 batches | ms/batch 94.52 | loss  3.78 |
| epoch  10 |   200/  475 batches | ms/batch 81.77 | loss  4.46 |
| epoch  10 |   300/  475 batches | ms/batch 77.65 | loss  4.59 |
| epoch  10 |   400/  475 batches | ms/batch 75.79 | loss  3.93 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  10 | time: 36.16s | training loss  4.16 |
    | end of validation epoch  10 | time: 33.02s | validation loss  3.45 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 10 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 11
| epoch  11 |   100/  475 batches | ms/batch 113.12 | loss  3.69 |
| epoch  11 |   200/  475 batches | ms/batch 91.55 | loss  3.92 |
| epoch  11 |   300/  475 batches | ms/batch 340.50 | loss  4.04 |
| epoch  11 |   400/  475 batches | ms/batch 284.96 | loss  4.00 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  11 | time: 124.23s | training loss  4.09 |
    | end of validation epoch  11 | time: 40.52s | validation loss  3.44 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 11 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 12
| epoch  12 |   100/  475 batches | ms/batch 89.92 | loss  4.34 |
| epoch  12 |   200/  475 batches | ms/batch 79.69 | loss  4.21 |
| epoch  12 |   300/  475 batches | ms/batch 76.08 | loss  3.76 |
| epoch  12 |   400/  475 batches | ms/batch 74.56 | loss  4.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  12 | time: 35.53s | training loss  4.03 |
    | end of validation epoch  12 | time: 31.69s | validation loss  3.41 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 12 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 13
| epoch  13 |   100/  475 batches | ms/batch 98.13 | loss  4.12 |
| epoch  13 |   200/  475 batches | ms/batch 83.56 | loss  4.03 |
| epoch  13 |   300/  475 batches | ms/batch 79.01 | loss  3.91 |
| epoch  13 |   400/  475 batches | ms/batch 76.49 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  13 | time: 36.20s | training loss  4.00 |
    | end of validation epoch  13 | time: 32.05s | validation loss  3.35 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 13 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 14
| epoch  14 |   100/  475 batches | ms/batch 95.17 | loss  4.03 |
| epoch  14 |   200/  475 batches | ms/batch 82.93 | loss  3.72 |
| epoch  14 |   300/  475 batches | ms/batch 78.66 | loss  4.24 |
| epoch  14 |   400/  475 batches | ms/batch 77.97 | loss  3.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  14 | time: 37.14s | training loss  3.96 |
    | end of validation epoch  14 | time: 37.68s | validation loss  3.23 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 14 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 15
| epoch  15 |   100/  475 batches | ms/batch 91.78 | loss  3.68 |
| epoch  15 |   200/  475 batches | ms/batch 81.90 | loss  3.74 |
| epoch  15 |   300/  475 batches | ms/batch 77.90 | loss  3.51 |
| epoch  15 |   400/  475 batches | ms/batch 75.67 | loss  3.91 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  15 | time: 36.78s | training loss  3.92 |
    | end of validation epoch  15 | time: 39.37s | validation loss  3.26 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 15 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 16
| epoch  16 |   100/  475 batches | ms/batch 92.84 | loss  4.37 |
| epoch  16 |   200/  475 batches | ms/batch 81.67 | loss  3.82 |
| epoch  16 |   300/  475 batches | ms/batch 77.52 | loss  4.06 |
| epoch  16 |   400/  475 batches | ms/batch 75.39 | loss  4.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  16 | time: 35.83s | training loss  3.88 |
    | end of validation epoch  16 | time: 46.28s | validation loss  3.18 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 16 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 17
| epoch  17 |   100/  475 batches | ms/batch 300.74 | loss  3.56 |
| epoch  17 |   200/  475 batches | ms/batch 186.46 | loss  3.91 |
| epoch  17 |   300/  475 batches | ms/batch 147.50 | loss  4.09 |
| epoch  17 |   400/  475 batches | ms/batch 128.33 | loss  4.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  17 | time: 57.07s | training loss  3.84 |
    | end of validation epoch  17 | time: 47.39s | validation loss  3.17 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 17 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 18
| epoch  18 |   100/  475 batches | ms/batch 90.46 | loss  4.08 |
| epoch  18 |   200/  475 batches | ms/batch 80.99 | loss  3.68 |
| epoch  18 |   300/  475 batches | ms/batch 77.55 | loss  3.77 |
| epoch  18 |   400/  475 batches | ms/batch 76.15 | loss  3.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  18 | time: 36.33s | training loss  3.82 |
    | end of validation epoch  18 | time: 41.75s | validation loss  3.12 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 18 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 19
| epoch  19 |   100/  475 batches | ms/batch 90.42 | loss  4.11 |
| epoch  19 |   200/  475 batches | ms/batch 80.51 | loss  3.36 |
| epoch  19 |   300/  475 batches | ms/batch 76.92 | loss  3.54 |
| epoch  19 |   400/  475 batches | ms/batch 75.55 | loss  3.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  19 | time: 36.17s | training loss  3.81 |
    | end of validation epoch  19 | time: 42.02s | validation loss  3.07 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 19 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 20
| epoch  20 |   100/  475 batches | ms/batch 94.44 | loss  4.18 |
| epoch  20 |   200/  475 batches | ms/batch 83.15 | loss  3.60 |
| epoch  20 |   300/  475 batches | ms/batch 79.10 | loss  4.03 |
| epoch  20 |   400/  475 batches | ms/batch 77.55 | loss  3.69 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  20 | time: 36.72s | training loss  3.76 |
    | end of validation epoch  20 | time: 41.80s | validation loss  3.11 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 20 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 21
| epoch  21 |   100/  475 batches | ms/batch 91.00 | loss  3.94 |
| epoch  21 |   200/  475 batches | ms/batch 82.12 | loss  2.96 |
| epoch  21 |   300/  475 batches | ms/batch 78.44 | loss  3.60 |
| epoch  21 |   400/  475 batches | ms/batch 76.97 | loss  3.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  21 | time: 36.54s | training loss  3.72 |
    | end of validation epoch  21 | time: 53.20s | validation loss  3.06 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 21 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 22
| epoch  22 |   100/  475 batches | ms/batch 92.01 | loss  3.83 |
| epoch  22 |   200/  475 batches | ms/batch 102.33 | loss  3.80 |
| epoch  22 |   300/  475 batches | ms/batch 92.04 | loss  3.66 |
| epoch  22 |   400/  475 batches | ms/batch 86.60 | loss  3.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  22 | time: 48.54s | training loss  3.70 |
    | end of validation epoch  22 | time: 48.91s | validation loss  3.00 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 22 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 23
| epoch  23 |   100/  475 batches | ms/batch 134.71 | loss  3.88 |
| epoch  23 |   200/  475 batches | ms/batch 103.91 | loss  3.90 |
| epoch  23 |   300/  475 batches | ms/batch 92.77 | loss  3.84 |
| epoch  23 |   400/  475 batches | ms/batch 87.22 | loss  3.84 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  23 | time: 40.65s | training loss  3.67 |
    | end of validation epoch  23 | time: 41.91s | validation loss  2.98 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 23 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 24
| epoch  24 |   100/  475 batches | ms/batch 92.39 | loss  3.37 |
| epoch  24 |   200/  475 batches | ms/batch 80.40 | loss  3.64 |
| epoch  24 |   300/  475 batches | ms/batch 77.13 | loss  3.52 |
| epoch  24 |   400/  475 batches | ms/batch 75.27 | loss  3.24 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  24 | time: 35.88s | training loss  3.64 |
    | end of validation epoch  24 | time: 41.53s | validation loss  2.97 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 24 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 25
| epoch  25 |   100/  475 batches | ms/batch 91.98 | loss  3.69 |
| epoch  25 |   200/  475 batches | ms/batch 81.62 | loss  3.90 |
| epoch  25 |   300/  475 batches | ms/batch 77.88 | loss  3.50 |
| epoch  25 |   400/  475 batches | ms/batch 75.81 | loss  3.72 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  25 | time: 36.11s | training loss  3.64 |
    | end of validation epoch  25 | time: 44.45s | validation loss  2.91 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 25 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 26
| epoch  26 |   100/  475 batches | ms/batch 466.14 | loss  4.48 |
| epoch  26 |   200/  475 batches | ms/batch 268.49 | loss  3.54 |
| epoch  26 |   300/  475 batches | ms/batch 202.23 | loss  3.36 |
| epoch  26 |   400/  475 batches | ms/batch 169.29 | loss  3.52 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  26 | time: 73.58s | training loss  3.61 |
    | end of validation epoch  26 | time: 41.49s | validation loss  2.99 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 26 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 27
| epoch  27 |   100/  475 batches | ms/batch 91.68 | loss  3.21 |
| epoch  27 |   200/  475 batches | ms/batch 80.86 | loss  3.96 |
| epoch  27 |   300/  475 batches | ms/batch 77.43 | loss  3.68 |
| epoch  27 |   400/  475 batches | ms/batch 75.91 | loss  3.65 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  27 | time: 36.20s | training loss  3.59 |
    | end of validation epoch  27 | time: 41.96s | validation loss  2.88 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 27 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 28
| epoch  28 |   100/  475 batches | ms/batch 93.11 | loss  3.38 |
| epoch  28 |   200/  475 batches | ms/batch 82.25 | loss  3.23 |
| epoch  28 |   300/  475 batches | ms/batch 77.80 | loss  3.08 |
| epoch  28 |   400/  475 batches | ms/batch 81.59 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  28 | time: 37.42s | training loss  3.57 |
    | end of validation epoch  28 | time: 41.91s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 28 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 29
| epoch  29 |   100/  475 batches | ms/batch 91.37 | loss  3.47 |
| epoch  29 |   200/  475 batches | ms/batch 81.67 | loss  3.74 |
| epoch  29 |   300/  475 batches | ms/batch 77.63 | loss  4.00 |
| epoch  29 |   400/  475 batches | ms/batch 76.34 | loss  3.75 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  29 | time: 36.48s | training loss  3.56 |
    | end of validation epoch  29 | time: 42.14s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 29 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 30
| epoch  30 |   100/  475 batches | ms/batch 96.15 | loss  3.53 |
| epoch  30 |   200/  475 batches | ms/batch 83.23 | loss  3.20 |
| epoch  30 |   300/  475 batches | ms/batch 78.56 | loss  3.23 |
| epoch  30 |   400/  475 batches | ms/batch 76.61 | loss  3.85 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  30 | time: 37.38s | training loss  3.53 |
    | end of validation epoch  30 | time: 62.80s | validation loss  2.89 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 30 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 31
| epoch  31 |   100/  475 batches | ms/batch 312.61 | loss  3.59 |
| epoch  31 |   200/  475 batches | ms/batch 191.87 | loss  3.79 |
| epoch  31 |   300/  475 batches | ms/batch 151.33 | loss  3.69 |
| epoch  31 |   400/  475 batches | ms/batch 130.96 | loss  3.64 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  31 | time: 58.26s | training loss  3.51 |
    | end of validation epoch  31 | time: 41.75s | validation loss  2.85 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 31 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 32
| epoch  32 |   100/  475 batches | ms/batch 90.61 | loss  3.36 |
| epoch  32 |   200/  475 batches | ms/batch 80.83 | loss  3.38 |
| epoch  32 |   300/  475 batches | ms/batch 77.75 | loss  3.52 |
| epoch  32 |   400/  475 batches | ms/batch 75.81 | loss  3.35 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  32 | time: 36.12s | training loss  3.51 |
    | end of validation epoch  32 | time: 41.89s | validation loss  2.87 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 32 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 33
| epoch  33 |   100/  475 batches | ms/batch 92.87 | loss  3.31 |
| epoch  33 |   200/  475 batches | ms/batch 80.99 | loss  3.65 |
| epoch  33 |   300/  475 batches | ms/batch 77.64 | loss  3.49 |
| epoch  33 |   400/  475 batches | ms/batch 75.75 | loss  3.70 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  33 | time: 36.24s | training loss  3.50 |
    | end of validation epoch  33 | time: 42.19s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 33 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 34
| epoch  34 |   100/  475 batches | ms/batch 90.85 | loss  3.21 |
| epoch  34 |   200/  475 batches | ms/batch 82.21 | loss  3.51 |
| epoch  34 |   300/  475 batches | ms/batch 79.06 | loss  3.41 |
| epoch  34 |   400/  475 batches | ms/batch 76.97 | loss  3.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  34 | time: 36.80s | training loss  3.48 |
    | end of validation epoch  34 | time: 41.76s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 34 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 35
| epoch  35 |   100/  475 batches | ms/batch 92.56 | loss  3.18 |
| epoch  35 |   200/  475 batches | ms/batch 81.12 | loss  3.79 |
| epoch  35 |   300/  475 batches | ms/batch 77.70 | loss  3.29 |
| epoch  35 |   400/  475 batches | ms/batch 76.10 | loss  2.95 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  35 | time: 36.27s | training loss  3.44 |
    | end of validation epoch  35 | time: 41.23s | validation loss  2.81 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 35 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 36
| epoch  36 |   100/  475 batches | ms/batch 92.97 | loss  3.48 |
| epoch  36 |   200/  475 batches | ms/batch 83.41 | loss  3.42 |
| epoch  36 |   300/  475 batches | ms/batch 78.96 | loss  3.26 |
| epoch  36 |   400/  475 batches | ms/batch 76.72 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  36 | time: 36.50s | training loss  3.44 |
    | end of validation epoch  36 | time: 41.74s | validation loss  2.83 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 36 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804]
this is epoch 37
| epoch  37 |   100/  475 batches | ms/batch 93.01 | loss  2.96 |
| epoch  37 |   200/  475 batches | ms/batch 81.78 | loss  3.46 |
| epoch  37 |   300/  475 batches | ms/batch 78.15 | loss  3.70 |
| epoch  37 |   400/  475 batches | ms/batch 76.15 | loss  3.57 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  37 | time: 36.44s | training loss  3.43 |
    | end of validation epoch  37 | time: 42.18s | validation loss  2.75 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 37 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 38
| epoch  38 |   100/  475 batches | ms/batch 91.81 | loss  3.15 |
| epoch  38 |   200/  475 batches | ms/batch 81.69 | loss  3.31 |
| epoch  38 |   300/  475 batches | ms/batch 77.82 | loss  3.10 |
| epoch  38 |   400/  475 batches | ms/batch 76.30 | loss  3.34 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  38 | time: 36.27s | training loss  3.42 |
    | end of validation epoch  38 | time: 42.27s | validation loss  2.77 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 38 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 39
| epoch  39 |   100/  475 batches | ms/batch 90.23 | loss  3.23 |
| epoch  39 |   200/  475 batches | ms/batch 81.95 | loss  3.16 |
| epoch  39 |   300/  475 batches | ms/batch 78.28 | loss  3.12 |
| epoch  39 |   400/  475 batches | ms/batch 76.85 | loss  3.49 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  39 | time: 36.44s | training loss  3.40 |
    | end of validation epoch  39 | time: 42.58s | validation loss  2.76 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 39 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 40
| epoch  40 |   100/  475 batches | ms/batch 93.14 | loss  3.65 |
| epoch  40 |   200/  475 batches | ms/batch 82.07 | loss  3.27 |
| epoch  40 |   300/  475 batches | ms/batch 78.11 | loss  3.43 |
| epoch  40 |   400/  475 batches | ms/batch 76.33 | loss  3.56 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  40 | time: 36.37s | training loss  3.41 |
    | end of validation epoch  40 | time: 42.58s | validation loss  2.82 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 40 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229]
this is epoch 41
| epoch  41 |   100/  475 batches | ms/batch 294.94 | loss  3.06 |
| epoch  41 |   200/  475 batches | ms/batch 182.77 | loss  3.51 |
| epoch  41 |   300/  475 batches | ms/batch 145.09 | loss  3.06 |
| epoch  41 |   400/  475 batches | ms/batch 125.99 | loss  3.41 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  41 | time: 56.26s | training loss  3.38 |
    | end of validation epoch  41 | time: 41.37s | validation loss  2.74 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 41 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 42
| epoch  42 |   100/  475 batches | ms/batch 93.65 | loss  2.94 |
| epoch  42 |   200/  475 batches | ms/batch 83.29 | loss  3.26 |
| epoch  42 |   300/  475 batches | ms/batch 78.79 | loss  3.28 |
| epoch  42 |   400/  475 batches | ms/batch 76.78 | loss  3.47 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  42 | time: 36.64s | training loss  3.37 |
    | end of validation epoch  42 | time: 41.81s | validation loss  2.78 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 42 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 43
| epoch  43 |   100/  475 batches | ms/batch 91.24 | loss  3.68 |
| epoch  43 |   200/  475 batches | ms/batch 81.78 | loss  3.63 |
| epoch  43 |   300/  475 batches | ms/batch 84.88 | loss  3.76 |
| epoch  43 |   400/  475 batches | ms/batch 81.01 | loss  3.60 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  43 | time: 38.17s | training loss  3.37 |
    | end of validation epoch  43 | time: 41.87s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 43 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 44
| epoch  44 |   100/  475 batches | ms/batch 92.09 | loss  3.42 |
| epoch  44 |   200/  475 batches | ms/batch 81.09 | loss  3.05 |
| epoch  44 |   300/  475 batches | ms/batch 77.69 | loss  3.34 |
| epoch  44 |   400/  475 batches | ms/batch 75.97 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  44 | time: 36.22s | training loss  3.34 |
    | end of validation epoch  44 | time: 41.98s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 44 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 45
| epoch  45 |   100/  475 batches | ms/batch 92.42 | loss  3.73 |
| epoch  45 |   200/  475 batches | ms/batch 80.96 | loss  3.45 |
| epoch  45 |   300/  475 batches | ms/batch 77.21 | loss  3.33 |
| epoch  45 |   400/  475 batches | ms/batch 76.43 | loss  3.37 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  45 | time: 35.97s | training loss  3.32 |
    | end of validation epoch  45 | time: 41.28s | validation loss  2.72 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 45 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 46
| epoch  46 |   100/  475 batches | ms/batch 91.26 | loss  3.16 |
| epoch  46 |   200/  475 batches | ms/batch 80.17 | loss  3.91 |
| epoch  46 |   300/  475 batches | ms/batch 76.32 | loss  3.05 |
| epoch  46 |   400/  475 batches | ms/batch 74.43 | loss  3.12 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  46 | time: 35.58s | training loss  3.32 |
    | end of validation epoch  46 | time: 36.08s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 46 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 47
| epoch  47 |   100/  475 batches | ms/batch 97.06 | loss  2.96 |
| epoch  47 |   200/  475 batches | ms/batch 85.01 | loss  3.27 |
| epoch  47 |   300/  475 batches | ms/batch 80.20 | loss  2.97 |
| epoch  47 |   400/  475 batches | ms/batch 77.96 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  47 | time: 37.01s | training loss  3.32 |
    | end of validation epoch  47 | time: 35.62s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 47 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408]
this is epoch 48
| epoch  48 |   100/  475 batches | ms/batch 95.20 | loss  3.32 |
| epoch  48 |   200/  475 batches | ms/batch 84.58 | loss  3.81 |
| epoch  48 |   300/  475 batches | ms/batch 80.55 | loss  3.24 |
| epoch  48 |   400/  475 batches | ms/batch 77.95 | loss  2.86 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  48 | time: 36.95s | training loss  3.29 |
    | end of validation epoch  48 | time: 35.97s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 48 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 49
| epoch  49 |   100/  475 batches | ms/batch 94.80 | loss  3.50 |
| epoch  49 |   200/  475 batches | ms/batch 84.51 | loss  3.02 |
| epoch  49 |   300/  475 batches | ms/batch 80.62 | loss  3.31 |
| epoch  49 |   400/  475 batches | ms/batch 89.06 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  49 | time: 41.47s | training loss  3.29 |
    | end of validation epoch  49 | time: 36.01s | validation loss  2.70 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 49 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165]
this is epoch 50
| epoch  50 |   100/  475 batches | ms/batch 95.59 | loss  2.97 |
| epoch  50 |   200/  475 batches | ms/batch 85.32 | loss  3.35 |
| epoch  50 |   300/  475 batches | ms/batch 80.13 | loss  3.11 |
| epoch  50 |   400/  475 batches | ms/batch 77.83 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  50 | time: 37.19s | training loss  3.29 |
    | end of validation epoch  50 | time: 35.95s | validation loss  2.64 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 50 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 51
| epoch  51 |   100/  475 batches | ms/batch 98.19 | loss  3.18 |
| epoch  51 |   200/  475 batches | ms/batch 85.45 | loss  3.41 |
| epoch  51 |   300/  475 batches | ms/batch 80.47 | loss  3.19 |
| epoch  51 |   400/  475 batches | ms/batch 78.34 | loss  2.98 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  51 | time: 37.31s | training loss  3.28 |
    | end of validation epoch  51 | time: 35.00s | validation loss  2.71 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 51 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 52
| epoch  52 |   100/  475 batches | ms/batch 94.05 | loss  2.94 |
| epoch  52 |   200/  475 batches | ms/batch 83.40 | loss  3.43 |
| epoch  52 |   300/  475 batches | ms/batch 79.33 | loss  3.24 |
| epoch  52 |   400/  475 batches | ms/batch 77.20 | loss  2.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  52 | time: 39.20s | training loss  3.26 |
    | end of validation epoch  52 | time: 35.87s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 52 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 53
| epoch  53 |   100/  475 batches | ms/batch 96.66 | loss  3.26 |
| epoch  53 |   200/  475 batches | ms/batch 85.70 | loss  3.36 |
| epoch  53 |   300/  475 batches | ms/batch 81.03 | loss  3.63 |
| epoch  53 |   400/  475 batches | ms/batch 78.36 | loss  3.25 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  53 | time: 37.19s | training loss  3.25 |
    | end of validation epoch  53 | time: 34.89s | validation loss  2.69 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 53 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 54
| epoch  54 |   100/  475 batches | ms/batch 95.34 | loss  2.99 |
| epoch  54 |   200/  475 batches | ms/batch 84.01 | loss  3.14 |
| epoch  54 |   300/  475 batches | ms/batch 80.15 | loss  3.34 |
| epoch  54 |   400/  475 batches | ms/batch 77.59 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  54 | time: 36.83s | training loss  3.24 |
    | end of validation epoch  54 | time: 35.21s | validation loss  2.66 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 54 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 55
| epoch  55 |   100/  475 batches | ms/batch 96.41 | loss  3.23 |
| epoch  55 |   200/  475 batches | ms/batch 84.14 | loss  3.11 |
| epoch  55 |   300/  475 batches | ms/batch 80.99 | loss  2.92 |
| epoch  55 |   400/  475 batches | ms/batch 78.12 | loss  3.13 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  55 | time: 37.03s | training loss  3.24 |
    | end of validation epoch  55 | time: 34.58s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 55 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 56
| epoch  56 |   100/  475 batches | ms/batch 95.64 | loss  2.90 |
| epoch  56 |   200/  475 batches | ms/batch 84.58 | loss  3.40 |
| epoch  56 |   300/  475 batches | ms/batch 80.67 | loss  3.03 |
| epoch  56 |   400/  475 batches | ms/batch 78.16 | loss  3.45 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  56 | time: 37.17s | training loss  3.24 |
    | end of validation epoch  56 | time: 34.49s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 56 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 57
| epoch  57 |   100/  475 batches | ms/batch 97.65 | loss  3.17 |
| epoch  57 |   200/  475 batches | ms/batch 85.22 | loss  3.10 |
| epoch  57 |   300/  475 batches | ms/batch 96.92 | loss  3.26 |
| epoch  57 |   400/  475 batches | ms/batch 90.16 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  57 | time: 41.86s | training loss  3.22 |
    | end of validation epoch  57 | time: 34.89s | validation loss  2.68 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 57 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 58
| epoch  58 |   100/  475 batches | ms/batch 93.75 | loss  2.91 |
| epoch  58 |   200/  475 batches | ms/batch 84.01 | loss  3.19 |
| epoch  58 |   300/  475 batches | ms/batch 79.66 | loss  3.31 |
| epoch  58 |   400/  475 batches | ms/batch 76.78 | loss  3.14 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  58 | time: 36.61s | training loss  3.23 |
    | end of validation epoch  58 | time: 34.13s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 58 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 59
| epoch  59 |   100/  475 batches | ms/batch 94.03 | loss  3.42 |
| epoch  59 |   200/  475 batches | ms/batch 83.48 | loss  3.25 |
| epoch  59 |   300/  475 batches | ms/batch 80.02 | loss  3.45 |
| epoch  59 |   400/  475 batches | ms/batch 77.59 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  59 | time: 36.90s | training loss  3.22 |
    | end of validation epoch  59 | time: 34.66s | validation loss  2.65 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 59 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 60
| epoch  60 |   100/  475 batches | ms/batch 95.72 | loss  3.18 |
| epoch  60 |   200/  475 batches | ms/batch 83.65 | loss  2.84 |
| epoch  60 |   300/  475 batches | ms/batch 79.70 | loss  3.05 |
| epoch  60 |   400/  475 batches | ms/batch 77.66 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  60 | time: 36.82s | training loss  3.19 |
    | end of validation epoch  60 | time: 34.10s | validation loss  2.62 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 60 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 61
| epoch  61 |   100/  475 batches | ms/batch 96.15 | loss  3.42 |
| epoch  61 |   200/  475 batches | ms/batch 85.24 | loss  3.41 |
| epoch  61 |   300/  475 batches | ms/batch 81.27 | loss  3.45 |
| epoch  61 |   400/  475 batches | ms/batch 78.55 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  61 | time: 37.16s | training loss  3.20 |
    | end of validation epoch  61 | time: 33.60s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 61 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 62
| epoch  62 |   100/  475 batches | ms/batch 93.72 | loss  2.89 |
| epoch  62 |   200/  475 batches | ms/batch 83.43 | loss  3.27 |
| epoch  62 |   300/  475 batches | ms/batch 85.67 | loss  3.58 |
| epoch  62 |   400/  475 batches | ms/batch 82.03 | loss  3.28 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  62 | time: 38.69s | training loss  3.18 |
    | end of validation epoch  62 | time: 34.11s | validation loss  2.63 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 62 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 63
| epoch  63 |   100/  475 batches | ms/batch 92.39 | loss  3.26 |
| epoch  63 |   200/  475 batches | ms/batch 82.77 | loss  2.91 |
| epoch  63 |   300/  475 batches | ms/batch 79.26 | loss  3.03 |
| epoch  63 |   400/  475 batches | ms/batch 77.22 | loss  3.03 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  63 | time: 36.64s | training loss  3.16 |
    | end of validation epoch  63 | time: 33.98s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 63 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 64
| epoch  64 |   100/  475 batches | ms/batch 94.20 | loss  3.00 |
| epoch  64 |   200/  475 batches | ms/batch 83.14 | loss  3.55 |
| epoch  64 |   300/  475 batches | ms/batch 79.56 | loss  3.33 |
| epoch  64 |   400/  475 batches | ms/batch 77.14 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  64 | time: 36.66s | training loss  3.17 |
    | end of validation epoch  64 | time: 33.75s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 64 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701]
this is epoch 65
| epoch  65 |   100/  475 batches | ms/batch 95.76 | loss  3.02 |
| epoch  65 |   200/  475 batches | ms/batch 84.17 | loss  3.37 |
| epoch  65 |   300/  475 batches | ms/batch 80.18 | loss  3.39 |
| epoch  65 |   400/  475 batches | ms/batch 77.73 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  65 | time: 37.00s | training loss  3.18 |
    | end of validation epoch  65 | time: 33.09s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 65 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 66
| epoch  66 |   100/  475 batches | ms/batch 95.40 | loss  2.88 |
| epoch  66 |   200/  475 batches | ms/batch 84.35 | loss  3.35 |
| epoch  66 |   300/  475 batches | ms/batch 80.34 | loss  3.65 |
| epoch  66 |   400/  475 batches | ms/batch 77.87 | loss  3.18 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  66 | time: 36.94s | training loss  3.17 |
    | end of validation epoch  66 | time: 33.23s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 66 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 67
| epoch  67 |   100/  475 batches | ms/batch 94.55 | loss  3.24 |
| epoch  67 |   200/  475 batches | ms/batch 83.58 | loss  3.21 |
| epoch  67 |   300/  475 batches | ms/batch 81.47 | loss  3.72 |
| epoch  67 |   400/  475 batches | ms/batch 79.05 | loss  3.19 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  67 | time: 37.52s | training loss  3.16 |
    | end of validation epoch  67 | time: 33.25s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 67 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914]
this is epoch 68
| epoch  68 |   100/  475 batches | ms/batch 98.01 | loss  3.04 |
| epoch  68 |   200/  475 batches | ms/batch 88.70 | loss  2.86 |
| epoch  68 |   300/  475 batches | ms/batch 84.38 | loss  3.02 |
| epoch  68 |   400/  475 batches | ms/batch 80.79 | loss  3.42 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  68 | time: 37.78s | training loss  3.14 |
    | end of validation epoch  68 | time: 32.72s | validation loss  2.67 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 68 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 69
| epoch  69 |   100/  475 batches | ms/batch 94.83 | loss  3.21 |
| epoch  69 |   200/  475 batches | ms/batch 84.44 | loss  3.10 |
| epoch  69 |   300/  475 batches | ms/batch 80.63 | loss  3.11 |
| epoch  69 |   400/  475 batches | ms/batch 78.09 | loss  2.96 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  69 | time: 37.05s | training loss  3.13 |
    | end of validation epoch  69 | time: 33.14s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 69 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 70
| epoch  70 |   100/  475 batches | ms/batch 94.26 | loss  2.82 |
| epoch  70 |   200/  475 batches | ms/batch 83.48 | loss  3.25 |
| epoch  70 |   300/  475 batches | ms/batch 80.01 | loss  3.43 |
| epoch  70 |   400/  475 batches | ms/batch 77.41 | loss  2.67 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  70 | time: 36.74s | training loss  3.12 |
    | end of validation epoch  70 | time: 32.85s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 70 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 71
| epoch  71 |   100/  475 batches | ms/batch 94.39 | loss  3.06 |
| epoch  71 |   200/  475 batches | ms/batch 83.77 | loss  3.45 |
| epoch  71 |   300/  475 batches | ms/batch 80.08 | loss  3.04 |
| epoch  71 |   400/  475 batches | ms/batch 77.94 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  71 | time: 44.41s | training loss  3.12 |
    | end of validation epoch  71 | time: 32.88s | validation loss  2.58 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 71 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 72
| epoch  72 |   100/  475 batches | ms/batch 92.72 | loss  3.14 |
| epoch  72 |   200/  475 batches | ms/batch 82.52 | loss  2.75 |
| epoch  72 |   300/  475 batches | ms/batch 79.20 | loss  3.08 |
| epoch  72 |   400/  475 batches | ms/batch 77.27 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  72 | time: 36.62s | training loss  3.11 |
    | end of validation epoch  72 | time: 32.65s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 72 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 73
| epoch  73 |   100/  475 batches | ms/batch 92.28 | loss  2.98 |
| epoch  73 |   200/  475 batches | ms/batch 82.84 | loss  2.70 |
| epoch  73 |   300/  475 batches | ms/batch 79.48 | loss  3.15 |
| epoch  73 |   400/  475 batches | ms/batch 77.76 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  73 | time: 37.07s | training loss  3.11 |
    | end of validation epoch  73 | time: 32.69s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 73 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 74
| epoch  74 |   100/  475 batches | ms/batch 89.53 | loss  3.06 |
| epoch  74 |   200/  475 batches | ms/batch 81.17 | loss  2.73 |
| epoch  74 |   300/  475 batches | ms/batch 78.29 | loss  3.18 |
| epoch  74 |   400/  475 batches | ms/batch 77.63 | loss  3.46 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  74 | time: 36.80s | training loss  3.11 |
    | end of validation epoch  74 | time: 32.32s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 74 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376]
this is epoch 75
| epoch  75 |   100/  475 batches | ms/batch 92.11 | loss  3.12 |
| epoch  75 |   200/  475 batches | ms/batch 81.95 | loss  3.04 |
| epoch  75 |   300/  475 batches | ms/batch 79.23 | loss  2.99 |
| epoch  75 |   400/  475 batches | ms/batch 78.02 | loss  2.80 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  75 | time: 37.21s | training loss  3.09 |
    | end of validation epoch  75 | time: 32.96s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 75 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 76
| epoch  76 |   100/  475 batches | ms/batch 93.96 | loss  3.67 |
| epoch  76 |   200/  475 batches | ms/batch 82.82 | loss  2.69 |
| epoch  76 |   300/  475 batches | ms/batch 79.51 | loss  3.35 |
| epoch  76 |   400/  475 batches | ms/batch 77.97 | loss  3.30 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  76 | time: 37.11s | training loss  3.08 |
    | end of validation epoch  76 | time: 32.98s | validation loss  2.60 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 76 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 77
| epoch  77 |   100/  475 batches | ms/batch 91.44 | loss  2.73 |
| epoch  77 |   200/  475 batches | ms/batch 81.57 | loss  3.60 |
| epoch  77 |   300/  475 batches | ms/batch 78.39 | loss  3.28 |
| epoch  77 |   400/  475 batches | ms/batch 77.18 | loss  2.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  77 | time: 37.05s | training loss  3.07 |
    | end of validation epoch  77 | time: 33.79s | validation loss  2.55 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 77 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 78
| epoch  78 |   100/  475 batches | ms/batch 93.32 | loss  2.80 |
| epoch  78 |   200/  475 batches | ms/batch 82.00 | loss  2.70 |
| epoch  78 |   300/  475 batches | ms/batch 78.53 | loss  2.90 |
| epoch  78 |   400/  475 batches | ms/batch 77.33 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  78 | time: 37.02s | training loss  3.07 |
    | end of validation epoch  78 | time: 35.93s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 78 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 79
| epoch  79 |   100/  475 batches | ms/batch 90.58 | loss  2.96 |
| epoch  79 |   200/  475 batches | ms/batch 80.61 | loss  3.35 |
| epoch  79 |   300/  475 batches | ms/batch 77.39 | loss  2.75 |
| epoch  79 |   400/  475 batches | ms/batch 76.05 | loss  3.10 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  79 | time: 36.58s | training loss  3.05 |
    | end of validation epoch  79 | time: 35.98s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 79 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 80
| epoch  80 |   100/  475 batches | ms/batch 90.57 | loss  3.31 |
| epoch  80 |   200/  475 batches | ms/batch 81.13 | loss  3.00 |
| epoch  80 |   300/  475 batches | ms/batch 77.59 | loss  2.60 |
| epoch  80 |   400/  475 batches | ms/batch 76.61 | loss  3.17 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  80 | time: 36.56s | training loss  3.06 |
    | end of validation epoch  80 | time: 36.10s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 80 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 81
| epoch  81 |   100/  475 batches | ms/batch 92.45 | loss  2.89 |
| epoch  81 |   200/  475 batches | ms/batch 80.22 | loss  3.23 |
| epoch  81 |   300/  475 batches | ms/batch 76.81 | loss  2.86 |
| epoch  81 |   400/  475 batches | ms/batch 75.91 | loss  3.04 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  81 | time: 36.42s | training loss  3.04 |
    | end of validation epoch  81 | time: 39.18s | validation loss  2.57 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 81 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 82
| epoch  82 |   100/  475 batches | ms/batch 92.14 | loss  3.64 |
| epoch  82 |   200/  475 batches | ms/batch 81.59 | loss  2.85 |
| epoch  82 |   300/  475 batches | ms/batch 78.20 | loss  3.02 |
| epoch  82 |   400/  475 batches | ms/batch 75.85 | loss  2.87 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  82 | time: 36.48s | training loss  3.04 |
    | end of validation epoch  82 | time: 39.50s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 82 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 83
| epoch  83 |   100/  475 batches | ms/batch 91.84 | loss  3.50 |
| epoch  83 |   200/  475 batches | ms/batch 82.07 | loss  2.91 |
| epoch  83 |   300/  475 batches | ms/batch 78.49 | loss  3.07 |
| epoch  83 |   400/  475 batches | ms/batch 76.46 | loss  3.27 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  83 | time: 36.46s | training loss  3.04 |
    | end of validation epoch  83 | time: 39.30s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 83 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038]
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 84
| epoch  84 |   100/  475 batches | ms/batch 91.64 | loss  2.71 |
| epoch  84 |   200/  475 batches | ms/batch 81.10 | loss  2.67 |
| epoch  84 |   300/  475 batches | ms/batch 78.04 | loss  2.66 |
| epoch  84 |   400/  475 batches | ms/batch 76.33 | loss  3.15 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  84 | time: 36.49s | training loss  3.03 |
    | end of validation epoch  84 | time: 39.97s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 84 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607]
 Best training model found.
---------------------------------------------------------------------------------------------------
 Best validation model found and saved.
---------------------------------------------------------------------------------------------------
this is epoch 85
| epoch  85 |   100/  475 batches | ms/batch 93.68 | loss  2.87 |
| epoch  85 |   200/  475 batches | ms/batch 81.85 | loss  3.30 |
| epoch  85 |   300/  475 batches | ms/batch 78.06 | loss  3.15 |
| epoch  85 |   400/  475 batches | ms/batch 76.25 | loss  3.07 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  85 | time: 36.36s | training loss  3.04 |
    | end of validation epoch  85 | time: 39.33s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 85 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744]
this is epoch 86
| epoch  86 |   100/  475 batches | ms/batch 92.53 | loss  2.73 |
| epoch  86 |   200/  475 batches | ms/batch 81.66 | loss  2.75 |
| epoch  86 |   300/  475 batches | ms/batch 78.18 | loss  2.78 |
| epoch  86 |   400/  475 batches | ms/batch 76.09 | loss  3.31 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  86 | time: 36.28s | training loss  3.03 |
    | end of validation epoch  86 | time: 40.47s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 86 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 87
| epoch  87 |   100/  475 batches | ms/batch 91.55 | loss  3.23 |
| epoch  87 |   200/  475 batches | ms/batch 81.42 | loss  3.00 |
| epoch  87 |   300/  475 batches | ms/batch 77.68 | loss  2.98 |
| epoch  87 |   400/  475 batches | ms/batch 75.62 | loss  2.88 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  87 | time: 36.13s | training loss  3.01 |
    | end of validation epoch  87 | time: 41.40s | validation loss  2.59 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 87 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 88
| epoch  88 |   100/  475 batches | ms/batch 111.31 | loss  2.93 |
| epoch  88 |   200/  475 batches | ms/batch 90.86 | loss  3.14 |
| epoch  88 |   300/  475 batches | ms/batch 83.74 | loss  2.97 |
| epoch  88 |   400/  475 batches | ms/batch 80.68 | loss  2.94 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  88 | time: 38.13s | training loss  3.01 |
    | end of validation epoch  88 | time: 41.23s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 88 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153]
this is epoch 89
| epoch  89 |   100/  475 batches | ms/batch 89.75 | loss  2.80 |
| epoch  89 |   200/  475 batches | ms/batch 80.43 | loss  2.81 |
| epoch  89 |   300/  475 batches | ms/batch 76.93 | loss  3.05 |
| epoch  89 |   400/  475 batches | ms/batch 75.12 | loss  3.01 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  89 | time: 35.90s | training loss  2.99 |
    | end of validation epoch  89 | time: 41.60s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 89 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 90
| epoch  90 |   100/  475 batches | ms/batch 91.35 | loss  3.33 |
| epoch  90 |   200/  475 batches | ms/batch 80.55 | loss  2.88 |
| epoch  90 |   300/  475 batches | ms/batch 77.47 | loss  3.38 |
| epoch  90 |   400/  475 batches | ms/batch 75.63 | loss  3.36 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  90 | time: 36.22s | training loss  3.00 |
    | end of validation epoch  90 | time: 41.36s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 90 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133]
this is epoch 91
| epoch  91 |   100/  475 batches | ms/batch 90.44 | loss  2.88 |
| epoch  91 |   200/  475 batches | ms/batch 81.17 | loss  2.87 |
| epoch  91 |   300/  475 batches | ms/batch 77.48 | loss  2.48 |
| epoch  91 |   400/  475 batches | ms/batch 75.86 | loss  3.06 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  91 | time: 36.16s | training loss  2.98 |
    | end of validation epoch  91 | time: 41.60s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 91 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 92
| epoch  92 |   100/  475 batches | ms/batch 90.59 | loss  3.11 |
| epoch  92 |   200/  475 batches | ms/batch 82.35 | loss  2.87 |
| epoch  92 |   300/  475 batches | ms/batch 78.66 | loss  2.75 |
| epoch  92 |   400/  475 batches | ms/batch 76.17 | loss  3.09 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  92 | time: 36.24s | training loss  2.98 |
    | end of validation epoch  92 | time: 41.15s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 92 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 93
| epoch  93 |   100/  475 batches | ms/batch 91.39 | loss  3.23 |
| epoch  93 |   200/  475 batches | ms/batch 80.78 | loss  3.07 |
| epoch  93 |   300/  475 batches | ms/batch 77.66 | loss  2.78 |
| epoch  93 |   400/  475 batches | ms/batch 75.72 | loss  2.89 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  93 | time: 36.20s | training loss  2.97 |
    | end of validation epoch  93 | time: 41.36s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 93 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 94
| epoch  94 |   100/  475 batches | ms/batch 90.08 | loss  2.41 |
| epoch  94 |   200/  475 batches | ms/batch 79.56 | loss  3.15 |
| epoch  94 |   300/  475 batches | ms/batch 77.32 | loss  3.14 |
| epoch  94 |   400/  475 batches | ms/batch 75.54 | loss  3.26 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  94 | time: 35.82s | training loss  2.96 |
    | end of validation epoch  94 | time: 41.66s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 94 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 95
| epoch  95 |   100/  475 batches | ms/batch 92.14 | loss  3.03 |
| epoch  95 |   200/  475 batches | ms/batch 80.33 | loss  3.01 |
| epoch  95 |   300/  475 batches | ms/batch 77.00 | loss  3.04 |
| epoch  95 |   400/  475 batches | ms/batch 75.11 | loss  3.55 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  95 | time: 35.85s | training loss  2.96 |
    | end of validation epoch  95 | time: 41.31s | validation loss  2.52 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 95 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757, 2.95748467897114] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977, 2.5239000310416984]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 96
| epoch  96 |   100/  475 batches | ms/batch 90.76 | loss  2.92 |
| epoch  96 |   200/  475 batches | ms/batch 80.38 | loss  2.81 |
| epoch  96 |   300/  475 batches | ms/batch 77.55 | loss  2.96 |
| epoch  96 |   400/  475 batches | ms/batch 75.86 | loss  2.76 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  96 | time: 36.11s | training loss  2.94 |
    | end of validation epoch  96 | time: 40.67s | validation loss  2.53 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 96 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757, 2.95748467897114, 2.9443927669525145] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977, 2.5239000310416984, 2.5345924181096695]
 Best training model found.
---------------------------------------------------------------------------------------------------
this is epoch 97
| epoch  97 |   100/  475 batches | ms/batch 91.19 | loss  3.05 |
| epoch  97 |   200/  475 batches | ms/batch 81.39 | loss  2.84 |
| epoch  97 |   300/  475 batches | ms/batch 77.79 | loss  3.16 |
| epoch  97 |   400/  475 batches | ms/batch 76.02 | loss  2.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  97 | time: 36.01s | training loss  2.95 |
    | end of validation epoch  97 | time: 41.47s | validation loss  2.50 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 97 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757, 2.95748467897114, 2.9443927669525145, 2.945424154683163] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977, 2.5239000310416984, 2.5345924181096695, 2.495927994992553]
this is epoch 98
| epoch  98 |   100/  475 batches | ms/batch 90.44 | loss  2.76 |
| epoch  98 |   200/  475 batches | ms/batch 80.91 | loss  3.34 |
| epoch  98 |   300/  475 batches | ms/batch 77.30 | loss  3.14 |
| epoch  98 |   400/  475 batches | ms/batch 75.78 | loss  2.73 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  98 | time: 36.14s | training loss  2.96 |
    | end of validation epoch  98 | time: 41.10s | validation loss  2.51 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 98 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757, 2.95748467897114, 2.9443927669525145, 2.945424154683163, 2.9589861689115824] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977, 2.5239000310416984, 2.5345924181096695, 2.495927994992553, 2.511775285256009]
this is epoch 99
| epoch  99 |   100/  475 batches | ms/batch 91.52 | loss  3.12 |
| epoch  99 |   200/  475 batches | ms/batch 80.50 | loss  3.02 |
| epoch  99 |   300/  475 batches | ms/batch 77.08 | loss  2.80 |
| epoch  99 |   400/  475 batches | ms/batch 75.45 | loss  2.68 |
---------------------------------------------------------------------------------------------------
    | end of training epoch  99 | time: 35.98s | training loss  2.94 |
    | end of validation epoch  99 | time: 40.58s | validation loss  2.56 |
---------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------
after epoch 99 training loss is  [5.7798545506126, 5.265513761419999, 4.972181115401418, 4.748796599538703, 4.598103019814742, 4.487449127498426, 4.380273402866564, 4.283334647228843, 4.215033960844341, 4.158426942825318, 4.089793019043772, 4.034138949042872, 4.00089347387615, 3.9565735656336734, 3.9236027160443756, 3.8761472601639597, 3.8435454980950605, 3.815223502108925, 3.8091937296014082, 3.761412686297768, 3.724040295450311, 3.6957773464604426, 3.674555115448801, 3.644706112711053, 3.63926091997247, 3.6066982655776174, 3.586033319172106, 3.567992606915926, 3.5593877310501902, 3.526434699108726, 3.512487325668335, 3.5051714826885023, 3.5026726115377325, 3.483052197506553, 3.442588182248567, 3.4439699358689158, 3.434960816533942, 3.42200555199071, 3.398710072667975, 3.4065182766161466, 3.3845988750457763, 3.373205338528282, 3.368003226330406, 3.3365689709312036, 3.3207496512563606, 3.321327016228124, 3.3248107157255475, 3.2874976107948704, 3.294028128573769, 3.2889725168127764, 3.2813402045400517, 3.2610844095129714, 3.2481268175024733, 3.23858198617634, 3.2377647118819386, 3.2401816107097425, 3.2175955019499125, 3.2293857925816587, 3.2157578101911044, 3.1936426293222526, 3.1975198635302093, 3.1820788258000423, 3.1615675760570325, 3.174260711669922, 3.178488275628341, 3.170578676524915, 3.1640332814266805, 3.138168245114778, 3.13286083020662, 3.122896694886057, 3.1191395272706686, 3.1128707865664835, 3.107773911325555, 3.1081225164313064, 3.0878274214895147, 3.0849985890639458, 3.0741838174117238, 3.0718758899287173, 3.0510121872550564, 3.06128120121203, 3.0424110206804778, 3.0357878223218417, 3.0374362734744422, 3.0325598405536853, 3.036051012340345, 3.0251766134563245, 3.0050180395025956, 3.0115614986419676, 2.9935460592571057, 3.0045644895653973, 2.981439621574, 2.9781702799546093, 2.9736249050341153, 2.9605635693198757, 2.95748467897114, 2.9443927669525145, 2.945424154683163, 2.9589861689115824, 2.937857982735885] validation loss is  [5.160611825830796, 4.659782958631756, 4.277703850209212, 4.096570145182249, 3.911329620024737, 3.7512819206013397, 3.683885778699602, 3.6074217828381965, 3.5170134736710237, 3.445604947434754, 3.437764756819781, 3.407625016044168, 3.349305215002108, 3.2322948980732122, 3.255579778126308, 3.176494866860013, 3.1679905382525018, 3.12089097800375, 3.0702704822315887, 3.1057756528133105, 3.062627668140315, 2.997943996381359, 2.982112942623491, 2.969088476245143, 2.90957833738888, 2.9891327709710898, 2.8774917806897844, 2.872224477158875, 2.848886205368683, 2.892976881075306, 2.852361320447521, 2.8665201143056405, 2.8216650245570336, 2.8172438805844604, 2.811871496569209, 2.8327483950542804, 2.750932785643249, 2.768123851102941, 2.7616791745193865, 2.823847175646229, 2.7441366820776163, 2.781253630373658, 2.721644125064882, 2.7007572951437044, 2.7229429152833315, 2.6710977003353986, 2.686673535018408, 2.6884082125014617, 2.7009509691671165, 2.6406768710673356, 2.7063905070809757, 2.6683577769944646, 2.686697865734581, 2.6603521920051896, 2.6279251735751368, 2.6263836982871305, 2.6758304794295493, 2.623156459391618, 2.649096108284317, 2.6229828525992, 2.6003391722671125, 2.629395661233854, 2.599338556538109, 2.60062266297701, 2.5933362966825984, 2.582437683554257, 2.5897509721146914, 2.672583792389942, 2.583909963359352, 2.5956585868066098, 2.5795536231593927, 2.5578766890934537, 2.5854698589869907, 2.5584613174951376, 2.555708149901959, 2.5958705589550886, 2.553846225017259, 2.557376416791387, 2.5327723216609797, 2.5257042355898047, 2.5680629656094465, 2.5162244784731826, 2.5111856660923038, 2.4953571437787607, 2.5565367676630744, 2.504154767308916, 2.594668271160927, 2.5059622105430153, 2.5194219791588663, 2.5263203963512133, 2.5139937941767587, 2.5222837774693465, 2.506005078804593, 2.5324130849677977, 2.5239000310416984, 2.5345924181096695, 2.495927994992553, 2.511775285256009, 2.557725619869072]
 Best training model found.
---------------------------------------------------------------------------------------------------
